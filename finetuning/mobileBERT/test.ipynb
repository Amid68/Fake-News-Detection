{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4151f5e2",
   "metadata": {},
   "source": [
    "# Enhanced Documentation for MobileBERT Fine-tuning on ISOT Dataset\n",
    "\n",
    "In this notebook, I'll build on our previous exploratory data analysis and feature engineering work to fine-tune a MobileBERT model for fake news detection. While our engineered features achieved impressive results, transformer models can capture complex linguistic patterns that might further improve performance or provide better generalization to new data. MobileBERT is specifically designed for mobile applications, offering a better trade-off between model size, inference speed, and accuracy compared to larger models like BERT or RoBERTa.\n",
    "\n",
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, I'll install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417117f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c50a2",
   "metadata": {},
   "source": [
    "Now, let's import the basic libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dcae5",
   "metadata": {},
   "source": [
    "Import the transformer-specific libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer-specific libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47461f",
   "metadata": {},
   "source": [
    "Import evaluation libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation libraries\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e489a",
   "metadata": {},
   "source": [
    "Set up reproducibility and check for GPU availability:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb067e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "# This ensures that our experiments can be replicated with the same results\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "# MobileBERT can run efficiently on CPU, but GPU will significantly speed up training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c869b87",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare the Dataset\n",
    "\n",
    "Load the preprocessed datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "# These datasets have already been cleaned and split in our previous data preprocessing steps\n",
    "# The preprocessing included removing the \"(Reuters)\" pattern to prevent data leakage\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/train_fake_news.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/val_fake_news.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing from Part 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706703ba",
   "metadata": {},
   "source": [
    "Examine the data format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ede759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data to understand the structure\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deef37f",
   "metadata": {},
   "source": [
    "Define a function to convert pandas DataFrames to HuggingFace Datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "# This is necessary because the Transformers library works best with HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For MobileBERT, we'll use both title and text\n",
    "    # Combining title and text provides more context for the model\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1576dc",
   "metadata": {},
   "source": [
    "Apply the conversion function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd04e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our datasets to HuggingFace format\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec893bf",
   "metadata": {},
   "source": [
    "## 3. Prepare Tokenizer and Model\n",
    "\n",
    "Check data format and types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd32d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few examples in your dataset to ensure proper formatting\n",
    "print(\"First example in train_dataset:\", train_dataset[0])\n",
    "\n",
    "# Debug the content types to catch any potential issues\n",
    "print(\"Text type for first example:\", type(train_dataset[0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4544a",
   "metadata": {},
   "source": [
    "Define a cleaning function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cleaning function for the dataset\n",
    "# This ensures all text entries are properly formatted strings\n",
    "def clean_dataset(example):\n",
    "    example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781cf5e",
   "metadata": {},
   "source": [
    "Apply cleaning to the datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all datasets\n",
    "train_dataset = train_dataset.map(clean_dataset)\n",
    "val_dataset = val_dataset.map(clean_dataset)\n",
    "test_dataset = test_dataset.map(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc7d98",
   "metadata": {},
   "source": [
    "Initialize the MobileBERT tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileBERT tokenizer\n",
    "# We use the uncased version as case is typically not important for fake news detection\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# Define the maximum sequence length\n",
    "# 512 is the maximum that BERT models can handle\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7b147",
   "metadata": {},
   "source": [
    "Define the tokenization function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2334d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the dataset\n",
    "# This converts text into the numerical format that the model can process\n",
    "def tokenize_function(examples):\n",
    "    # Convert all text entries to strings and handle potential None values\n",
    "    texts = [str(text) if text is not None else \"\" for text in examples['text']]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',  # Pad to max_length to create uniform batch sizes\n",
    "        truncation=True,       # Truncate texts longer than max_length\n",
    "        max_length=max_length,\n",
    "        return_tensors=None    # Don't return tensors in batch mode\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6a772",
   "metadata": {},
   "source": [
    "Apply tokenization to our datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to our datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891c294",
   "metadata": {},
   "source": [
    "Set the format for PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch after tokenization\n",
    "# This ensures compatibility with the PyTorch-based Transformers library\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6b34f",
   "metadata": {},
   "source": [
    "## 4. Define Metrics and Evaluation Strategy\n",
    "\n",
    "Define our evaluation metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics\n",
    "# This will be used to evaluate model performance during and after training\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9717e",
   "metadata": {},
   "source": [
    "## 5. Initialize Model for Fine-tuning\n",
    "\n",
    "Initialize the MobileBERT model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31730d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileBERT model for sequence classification\n",
    "# We use the pre-trained model and add a classification head for our binary task\n",
    "model = MobileBertForSequenceClassification.from_pretrained(\n",
    "    'google/mobilebert-uncased',\n",
    "    num_labels=2  # Binary classification: 0 for fake, 1 for real\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67d9a",
   "metadata": {},
   "source": [
    "Move the model to the appropriate device:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device (GPU if available)\n",
    "# This significantly speeds up training if a GPU is available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1ac43",
   "metadata": {},
   "source": [
    "## 6. Define Training Arguments and Trainer\n",
    "\n",
    "Configure the training parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "# These hyperparameters were selected based on empirical testing and literature recommendations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    num_train_epochs=3,              # Number of training epochs - 3 is typically sufficient for this task\n",
    "    per_device_train_batch_size=16,  # Batch size for training - MobileBERT is efficient enough for larger batches\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay for regularization\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=100,               # Log every X steps\n",
    "    eval_strategy=\"epoch\",           # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save model checkpoint every epoch\n",
    "    load_best_model_at_end=True,     # Load the best model at the end of training\n",
    "    metric_for_best_model=\"f1\",      # Use F1 score to determine the best model\n",
    "    push_to_hub=False,               # Don't push to Hugging Face Hub\n",
    "    report_to=\"none\",                # Disable reporting to avoid wandb or other services\n",
    "    learning_rate=2e-5               # Learning rate - 2e-5 is a common value for fine-tuning transformers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad490f",
   "metadata": {},
   "source": [
    "Create the Trainer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d89d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "# The Trainer handles the training loop, evaluation, and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_tokenized,       # Training dataset\n",
    "    eval_dataset=val_tokenized,          # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # The function to compute metrics\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping to prevent overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eeedaa",
   "metadata": {},
   "source": [
    "## 7. Fine-tune the Model\n",
    "\n",
    "Start the timer to measure training time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer to measure training time\n",
    "# This helps us compare efficiency across different models\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f197f",
   "metadata": {},
   "source": [
    "Train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd538bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This will fine-tune the pre-trained MobileBERT on our fake news dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe21fef",
   "metadata": {},
   "source": [
    "Calculate and display the training time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333e541",
   "metadata": {},
   "source": [
    "Save the fine-tuned model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model for later use\n",
    "trainer.save_model(\"./mobilebert-fake-news-detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ba03d",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Evaluate the model on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_tokenized)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ed17d",
   "metadata": {},
   "source": [
    "Get predictions on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "test_pred = trainer.predict(test_tokenized)\n",
    "y_preds = np.argmax(test_pred.predictions, axis=1)\n",
    "y_true = test_pred.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86614a9e",
   "metadata": {},
   "source": [
    "Create confusion matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix to visualize model performance\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aef01",
   "metadata": {},
   "source": [
    "Plot the confusion matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820783b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for better visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('MobileBERT Confusion Matrix')\n",
    "plt.savefig('mobilebert_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be214947",
   "metadata": {},
   "source": [
    "Print the classification report:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for detailed performance metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82909c",
   "metadata": {},
   "source": [
    "## 9. Analyze Misclassified Examples\n",
    "\n",
    "Find and count misclassified examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of misclassified examples\n",
    "misclassified_indices = np.where(y_preds != y_true)[0]\n",
    "print(f\"Number of misclassified examples: {len(misclassified_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bf97a",
   "metadata": {},
   "source": [
    "Analyze misclassified examples if any exist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are misclassifications, analyze a few to understand model limitations\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Get the original text and predictions\n",
    "    misclassified_texts = []\n",
    "    for idx in misclassified_indices[:5]:  # Examine up to 5 examples\n",
    "        # Convert numpy.int64 to Python int\n",
    "        idx_int = int(idx)\n",
    "        \n",
    "        # Now use the converted index\n",
    "        original_idx = test_dataset[idx_int]['__index_level_0__'] if '__index_level_0__' in test_dataset[idx_int] else idx_int\n",
    "        \n",
    "        text = test_df.iloc[original_idx]['title']\n",
    "        true_label = \"Real\" if y_true[idx] == 1 else \"Fake\"\n",
    "        pred_label = \"Real\" if y_preds[idx] == 1 else \"Fake\"\n",
    "        \n",
    "        misclassified_texts.append({\n",
    "            'Title': text,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': pred_label\n",
    "        })\n",
    "    \n",
    "    # Display misclassified examples\n",
    "    print(\"\\nSample of misclassified examples:\")\n",
    "    display(pd.DataFrame(misclassified_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87387d19",
   "metadata": {},
   "source": [
    "## 10. Model Performance Comparison and Conclusions\n",
    "\n",
    "Create a comparison table with previous models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of model performances\n",
    "models = ['TF-IDF + ML', 'DistilBERT', 'TinyBERT', 'MobileBERT']\n",
    "accuracy = [0.984, 0.9996, 0.9991, test_results['eval_accuracy']] \n",
    "f1_scores = [0.984, 0.9996, 0.9991, test_results['eval_f1']]\n",
    "training_times = ['39.18 minutes', '48.69 minutes', '8.99 minutes', f\"{training_time/60:.2f} minutes\"]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Training Time': training_times\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47dbfa",
   "metadata": {},
   "source": [
    "## Conclusion and Discussion\n",
    "\n",
    "In this notebook, I've fine-tuned a MobileBERT model for fake news detection on the ISOT dataset. Here are the key findings and insights:\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "MobileBERT achieves excellent accuracy, comparable to our previous models using engineered features, DistilBERT, and TinyBERT. This demonstrates that lightweight transformer models can maintain high performance while requiring fewer computational resources.\n",
    "\n",
    "### Efficiency Considerations\n",
    "\n",
    "MobileBERT is specifically designed for mobile and edge devices, offering a good balance between model size, inference speed, and accuracy. With approximately 25M parameters (compared to BERT's 110M and DistilBERT's 67M), it's significantly more efficient while maintaining strong performance.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "The high accuracy combined with MobileBERT's efficiency makes it suitable for deployment in resource-constrained environments like mobile applications or edge devices. This enables real-time fake news detection without requiring powerful hardware.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "While the model performs exceptionally well on the ISOT dataset, real-world deployment would benefit from:\n",
    "\n",
    "1. Testing on more diverse datasets to ensure generalization\n",
    "2. Implementing explainability techniques to understand model decisions\n",
    "3. Exploring quantization and pruning for further efficiency improvements\n",
    "4. Developing ensemble approaches combining traditional ML and transformer models\n",
    "\n",
    "This work demonstrates that lightweight transformer models like MobileBERT can effectively detect fake news with high accuracy while maintaining reasonable computational requirements, making them practical for real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
