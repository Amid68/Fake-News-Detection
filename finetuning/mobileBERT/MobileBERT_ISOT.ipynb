{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033cd636",
   "metadata": {},
   "source": [
    "# Fine-tuning MobileBERT for Fake News Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook guides you through the process of fine-tuning a MobileBERT model for fake news detection using the ISOT dataset. MobileBERT is particularly well-suited for deployment on mobile and edge devices due to its compact architecture and efficiency optimizations. Unlike larger models like BERT or RoBERTa, MobileBERT was specifically designed to balance performance with computational constraints, making it ideal for resource-limited environments.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Set up the necessary environment and libraries\n",
    "2. Load and prepare the ISOT dataset for training\n",
    "3. Configure MobileBERT for sequence classification\n",
    "4. Fine-tune the model with carefully selected hyperparameters\n",
    "5. Evaluate performance and analyze results\n",
    "6. Save the model for future use or deployment\n",
    "\n",
    "## 1. Setup and Environment Preparation\n",
    "\n",
    "First, let's install and import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34671ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb39c7",
   "metadata": {},
   "source": [
    "Now let's import all the libraries we'll need for our fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20350c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff84aa",
   "metadata": {},
   "source": [
    "Let's set up reproducibility by setting random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011eb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afea40",
   "metadata": {},
   "source": [
    "Check for GPU availability to accelerate training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa89b7",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "Now we'll load the preprocessed ISOT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adcaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/isot-fake-news-robust/train_fake_news_robust.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/isot-fake-news-robust/val_fake_news_robust.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/isot-fake-news-robust/test_fake_news_robust.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d8600",
   "metadata": {},
   "source": [
    "Let's examine the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c78cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c2bf9",
   "metadata": {},
   "source": [
    "### Converting to HuggingFace Dataset Format\n",
    "\n",
    "Now we'll convert our pandas DataFrames to the HuggingFace Dataset format, which is optimized for working with transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94dbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For MobileBERT, we'll use both title and text\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d76c95",
   "metadata": {},
   "source": [
    "Apply the conversion function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b16574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our datasets\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e20d4",
   "metadata": {},
   "source": [
    "We combine the title and body text into a single text field because:\n",
    "1. News headlines often contain important contextual information that can help identify fake news\n",
    "2. MobileBERT can process sequences up to 512 tokens, which is sufficient for most news articles\n",
    "3. This approach provides the model with the maximum available information for classification\n",
    "\n",
    "### Data Cleaning and Preparation\n",
    "\n",
    "Before tokenization, we ensure the dataset is clean and properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be800b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few examples in your dataset\n",
    "print(\"First example in train_dataset:\", train_dataset[0])\n",
    "\n",
    "# Debug the content types\n",
    "print(\"Text type for first example:\", type(train_dataset[0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e1d20",
   "metadata": {},
   "source": [
    "Define a cleaning function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cleaning function for the dataset\n",
    "def clean_dataset(example):\n",
    "    example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697cc9a",
   "metadata": {},
   "source": [
    "Apply cleaning to the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all datasets\n",
    "train_dataset = train_dataset.map(clean_dataset)\n",
    "val_dataset = val_dataset.map(clean_dataset)\n",
    "test_dataset = test_dataset.map(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd354ee1",
   "metadata": {},
   "source": [
    "This cleaning step ensures that all text entries are properly formatted as strings, preventing potential errors during tokenization.\n",
    "\n",
    "## 3. Model Architecture and Configuration\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Let's prepare the tokenizer for MobileBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileBERT tokenizer\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_length = 512  # This is the maximum that BERT models can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c3ebd",
   "metadata": {},
   "source": [
    "Define the tokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4accf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Convert all text entries to strings and handle potential None values\n",
    "    texts = [str(text) if text is not None else \"\" for text in examples['text']]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=None  # Don't return tensors in batch mode\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ded48",
   "metadata": {},
   "source": [
    "Apply tokenization to our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to our datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459205aa",
   "metadata": {},
   "source": [
    "Set the format for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch after tokenization\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade44e3d",
   "metadata": {},
   "source": [
    "Key tokenization decisions:\n",
    "- We use the uncased version of MobileBERT because case information is less critical for fake news detection\n",
    "- We set `max_length=512` to use the full context window of MobileBERT\n",
    "- We apply padding to ensure all sequences have the same length, which is necessary for batch processing\n",
    "- We use truncation to handle any articles that exceed the maximum length\n",
    "- We use batched processing for efficiency\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "Now we initialize the MobileBERT model for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileBERT model for sequence classification\n",
    "model = MobileBertForSequenceClassification.from_pretrained(\n",
    "    'google/mobilebert-uncased',\n",
    "    num_labels=2  # Binary classification: 0 for fake, 1 for real\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8ada0",
   "metadata": {},
   "source": [
    "We use the pretrained MobileBERT model and adapt it for our binary classification task. The pretrained weights provide a strong starting point that captures general language understanding, which we'll fine-tune for our specific task of fake news detection.\n",
    "\n",
    "## 4. Training Process\n",
    "\n",
    "### Defining Metrics\n",
    "\n",
    "Let's define a function to compute evaluation metrics during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee14eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f591c",
   "metadata": {},
   "source": [
    "We track multiple metrics because accuracy alone can be misleading, especially if the dataset is imbalanced:\n",
    "- Accuracy: Overall correctness of predictions\n",
    "- Precision: Proportion of positive identifications that were actually correct\n",
    "- Recall: Proportion of actual positives that were identified correctly\n",
    "- F1 Score: Harmonic mean of precision and recall, providing a balance between the two\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "Let's set up the training arguments with carefully chosen hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    num_train_epochs=5,              # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training - MobileBERT is efficient\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=100,               # Log every X steps\n",
    "    eval_strategy=\"epoch\",           # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save model checkpoint every epoch\n",
    "    load_best_model_at_end=True,     # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",      # Use F1 score to determine the best model\n",
    "    push_to_hub=False,               # Don't push to Hugging Face Hub\n",
    "    report_to=\"none\",                # Disable reporting to avoid wandb or other services\n",
    "    learning_rate=2e-5               # Learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a09b0",
   "metadata": {},
   "source": [
    "Key hyperparameter choices and their rationale:\n",
    "- `num_train_epochs=5`: Provides sufficient training iterations while avoiding overfitting\n",
    "- `per_device_train_batch_size=16`: Balances memory constraints with training efficiency\n",
    "- `warmup_steps=500`: Gradually increases the learning rate to stabilize early training\n",
    "- `weight_decay=0.01`: Adds L2 regularization to prevent overfitting\n",
    "- `evaluation_strategy=\"epoch\"`: Evaluates after each epoch to track progress\n",
    "- `metric_for_best_model=\"f1\"`: Uses F1 score as the primary metric for model selection because it balances precision and recall\n",
    "\n",
    "### Training Execution\n",
    "\n",
    "Let's initialize the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_tokenized,       # Training dataset\n",
    "    eval_dataset=val_tokenized,          # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # The function to compute metrics\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5835aca",
   "metadata": {},
   "source": [
    "Now, let's train the model with the option to resume from a checkpoint if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer to measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model (with option to resume from checkpoint)\n",
    "# To resume from a checkpoint, use: trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37294b",
   "metadata": {},
   "source": [
    "We include an early stopping callback with a patience of 2 epochs to prevent overfitting. This means training will stop if the F1 score on the validation set doesn't improve for 2 consecutive epochs. This is particularly important for MobileBERT, which might be more prone to overfitting due to its more limited capacity.\n",
    "\n",
    "## 5. Evaluation\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "Now let's evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_tokenized)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c56e5f",
   "metadata": {},
   "source": [
    "### Detailed Performance Analysis\n",
    "\n",
    "Let's perform a more detailed analysis of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "test_pred = trainer.predict(test_tokenized)\n",
    "y_preds = np.argmax(test_pred.predictions, axis=1)\n",
    "y_true = test_pred.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9f5e0",
   "metadata": {},
   "source": [
    "Create and visualize the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e083c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('MobileBERT Confusion Matrix')\n",
    "plt.savefig('mobilebert_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f79c02",
   "metadata": {},
   "source": [
    "Print the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe80aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f581",
   "metadata": {},
   "source": [
    "## 6. Error Analysis and Results Summary\n",
    "\n",
    "Let's analyze the misclassified examples to understand where the model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf529359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of misclassified examples\n",
    "misclassified_indices = np.where(y_preds != y_true)[0]\n",
    "print(f\"Number of misclassified examples: {len(misclassified_indices)}\")\n",
    "\n",
    "# If there are misclassifications, analyze a few\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Get the original text and predictions\n",
    "    misclassified_texts = []\n",
    "    for idx in misclassified_indices[:5]:  # Examine up to 5 examples\n",
    "        # Convert numpy.int64 to Python int\n",
    "        idx_int = int(idx)\n",
    "        \n",
    "        # Now use the converted index\n",
    "        original_idx = test_dataset[idx_int]['__index_level_0__'] if '__index_level_0__' in test_dataset[idx_int] else idx_int\n",
    "        \n",
    "        text = test_df.iloc[original_idx]['title']\n",
    "        true_label = \"Real\" if y_true[idx] == 1 else \"Fake\"\n",
    "        pred_label = \"Real\" if y_preds[idx] == 1 else \"Fake\"\n",
    "        \n",
    "        misclassified_texts.append({\n",
    "            'Title': text,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': pred_label\n",
    "        })\n",
    "    \n",
    "    # Display misclassified examples\n",
    "    print(\"\\nSample of misclassified examples:\")\n",
    "    display(pd.DataFrame(misclassified_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73ef8a",
   "metadata": {},
   "source": [
    "Let's also create a comparison with other models if you have those results available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table with previous models\n",
    "models = ['TF-IDF + ML', 'DistilBERT', 'TinyBERT', 'MobileBERT']\n",
    "accuracy = [0.984, 0.9996, 0.9991, test_results['eval_accuracy']] \n",
    "f1_scores = [0.984, 0.9996, 0.9991, test_results['eval_f1']]\n",
    "training_times = ['0.13 minutes', '48.69 minutes', '8.99 minutes', f\"{training_time/60:.2f} minutes\"]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Training Time': training_times\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707f2a5",
   "metadata": {},
   "source": [
    "## 7. Saving the Model\n",
    "\n",
    "Finally, let's save the fine-tuned model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80948be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./mobilebert-fake-news-detector\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6022d",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "MobileBERT offers several advantages for fake news detection:\n",
    "\n",
    "1. **Efficiency:** MobileBERT's bottleneck architecture significantly reduces model size and improves inference speed, making it suitable for resource-constrained environments.\n",
    "\n",
    "2. **Performance:** Despite its optimizations for efficiency, MobileBERT maintains competitive performance compared to larger models like DistilBERT and RoBERTa.\n",
    "\n",
    "3. **Mobile Deployment:** The model's architecture was specifically designed for mobile applications, making it ideal for on-device fake news detection.\n",
    "\n",
    "4. **Memory Footprint:** With approximately 25M parameters (compared to BERT's 110M or RoBERTa's 125M), MobileBERT requires significantly less memory while preserving most of the capability.\n",
    "\n",
    "This notebook demonstrated how to fine-tune MobileBERT for fake news detection, achieving excellent performance with reasonable computational requirements. The model can now be deployed in various scenarios, particularly those with resource constraints where larger models would be impractical.\n",
    "\n",
    "In future work, you might consider:\n",
    "1. Exploring additional optimization techniques like quantization for even greater efficiency\n",
    "2. Testing the model on more diverse fake news datasets to evaluate generalization\n",
    "3. Implementing the model in a mobile application to demonstrate real-world deployment\n",
    "4. Comparing inference latency across different mobile devices"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
