{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddca374",
   "metadata": {},
   "source": [
    "# Fine-tuning MobileBERT for Fake News Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook documents the process of fine-tuning a MobileBERT model for fake news detection using the ISOT dataset. Building on our previous exploratory data analysis and feature engineering work, we now leverage transformer-based models to capture complex linguistic patterns that might improve performance or provide better generalization to new data.\n",
    "\n",
    "MobileBERT was selected as part of our comparative evaluation because it represents an architecture specifically designed for mobile and edge computing applications. Developed by Google Research, MobileBERT uses a bottleneck structure and carefully designed knowledge transfer techniques to create a model that is 4.3x smaller and 5.5x faster than BERT-base while retaining 96% of its performance. This makes it particularly valuable for deployment scenarios where computational resources are limited but high accuracy is still required.\n",
    "\n",
    "## Setup and Environment Preparation\n",
    "\n",
    "### Library Installation and Imports\n",
    "\n",
    "We begin by installing the necessary libraries for our fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d1bf0",
   "metadata": {},
   "source": [
    "The libraries serve the following purposes:\n",
    "- `transformers`: Provides access to pretrained models like MobileBERT and utilities for fine-tuning\n",
    "- `datasets`: Offers efficient data handling for transformer models\n",
    "- `torch`: Serves as the deep learning framework for model training\n",
    "- `evaluate`: Provides evaluation metrics for model performance assessment\n",
    "- `scikit-learn`: Offers additional metrics and utilities for evaluation\n",
    "\n",
    "Next, we import the basic libraries needed for data handling and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90611ad1",
   "metadata": {},
   "source": [
    "Then we import the transformer-specific libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer-specific libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170f8e7",
   "metadata": {},
   "source": [
    "The choice to use MobileBERT-specific classes (`MobileBertTokenizer` and `MobileBertForSequenceClassification`) rather than generic BERT classes is deliberate. While MobileBERT shares some architectural similarities with BERT, it has specific optimizations and a unique tokenizer that are better accessed through these dedicated classes.\n",
    "\n",
    "### Setting Up Reproducibility\n",
    "\n",
    "To ensure our experiments are reproducible, we set random seeds for all libraries that use randomization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11a371",
   "metadata": {},
   "source": [
    "The seed value of 42 is arbitrary but consistently used across all our experiments to ensure fair comparison between models.\n",
    "\n",
    "### Hardware Configuration\n",
    "\n",
    "We check for GPU availability to accelerate training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b868320",
   "metadata": {},
   "source": [
    "Using a GPU significantly speeds up the training process for transformer models. Even though MobileBERT is more efficient than larger models, GPU acceleration is still beneficial for faster training. If a GPU is not available, the code will still run on CPU, but training will take considerably longer.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We load the preprocessed ISOT dataset that was prepared in our earlier data analysis notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/train_fake_news.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/val_fake_news.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing from Part 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a44037",
   "metadata": {},
   "source": [
    "The dataset has already been split into training, validation, and test sets with a ratio of 70:15:15. This split ensures we have enough data for training while maintaining substantial validation and test sets for reliable evaluation.\n",
    "\n",
    "### Examining the Data\n",
    "\n",
    "We examine the data structure to ensure it matches our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef587e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfd35f",
   "metadata": {},
   "source": [
    "The dataset contains three key columns:\n",
    "- `title`: The headline of the news article\n",
    "- `enhanced_cleaned_text`: The preprocessed body text of the article\n",
    "- `label`: Binary classification (0 for fake news, 1 for real news)\n",
    "\n",
    "### Converting to HuggingFace Dataset Format\n",
    "\n",
    "We convert our pandas DataFrames to the HuggingFace Dataset format, which is optimized for working with transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b12b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For MobileBERT, we'll combine title and text for better context\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset\n",
    "\n",
    "# Convert our datasets\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3b458",
   "metadata": {},
   "source": [
    "We combine the title and body text into a single text field for several reasons:\n",
    "1. News headlines often contain important contextual information or framing that can help identify fake news\n",
    "2. MobileBERT can process sequences up to 512 tokens, which is sufficient for most news articles\n",
    "3. This approach provides the model with the maximum available information for classification\n",
    "4. Using the same preprocessing approach across all models ensures fair comparison\n",
    "\n",
    "## Model Architecture and Configuration\n",
    "\n",
    "### Data Cleaning and Preparation\n",
    "\n",
    "Before tokenization, we ensure the dataset is clean and properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset before tokenization\n",
    "def clean_dataset(example):\n",
    "    example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(clean_dataset)\n",
    "val_dataset = val_dataset.map(clean_dataset)\n",
    "test_dataset = test_dataset.map(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc850f9",
   "metadata": {},
   "source": [
    "This cleaning step ensures that all text entries are properly formatted as strings, preventing potential errors during tokenization. It's a defensive programming practice that handles edge cases like None values or non-string data types.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "We prepare the tokenizer for MobileBERT, which converts text into token IDs that the model can process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac06cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with truncation and padding\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization to our datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19937196",
   "metadata": {},
   "source": [
    "Key tokenization decisions:\n",
    "- We use the uncased version of MobileBERT because case information is less critical for fake news detection\n",
    "- We set `max_length=512` to use the full context window of MobileBERT\n",
    "- We apply padding to ensure all sequences have the same length, which is necessary for batch processing\n",
    "- We use truncation to handle any articles that exceed the maximum length\n",
    "- We use batched processing for efficiency\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "We initialize the MobileBERT model for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df80fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MobileBertForSequenceClassification.from_pretrained(\n",
    "    'google/mobilebert-uncased',\n",
    "    num_labels=2,  # Binary classification: fake or real\n",
    "    id2label={0: \"fake\", 1: \"real\"},\n",
    "    label2id={\"fake\": 0, \"real\": 1}\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87349d",
   "metadata": {},
   "source": [
    "We use the pretrained MobileBERT model and adapt it for our binary classification task. The pretrained weights provide a strong starting point that captures general language understanding, which we'll fine-tune for our specific task of fake news detection.\n",
    "\n",
    "MobileBERT was chosen for this comparison because:\n",
    "1. It uses a bottleneck architecture that significantly reduces model size while maintaining performance\n",
    "2. It employs knowledge distillation techniques during pretraining, not just fine-tuning\n",
    "3. It's specifically optimized for mobile and edge devices, with careful attention to inference latency\n",
    "4. It represents a different approach to model compression compared to DistilBERT and TinyBERT\n",
    "\n",
    "## Training Process\n",
    "\n",
    "### Defining Metrics\n",
    "\n",
    "We define a function to compute evaluation metrics during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470cd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fff93f",
   "metadata": {},
   "source": [
    "We track multiple metrics because accuracy alone can be misleading, especially if the dataset is imbalanced:\n",
    "- Accuracy: Overall correctness of predictions\n",
    "- Precision: Proportion of positive identifications that were actually correct\n",
    "- Recall: Proportion of actual positives that were identified correctly\n",
    "- F1 Score: Harmonic mean of precision and recall, providing a balance between the two\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "We set up the training arguments with carefully chosen hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52db381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/mobilebert',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be22b8d",
   "metadata": {},
   "source": [
    "Key hyperparameter choices and their rationale:\n",
    "- `num_train_epochs=5`: Provides sufficient training iterations while avoiding overfitting\n",
    "- `per_device_train_batch_size=16`: Balances memory constraints with training efficiency\n",
    "- `warmup_steps=500`: Gradually increases the learning rate to stabilize early training\n",
    "- `weight_decay=0.01`: Adds L2 regularization to prevent overfitting\n",
    "- `evaluation_strategy=\"epoch\"`: Evaluates after each epoch to track progress\n",
    "- `metric_for_best_model=\"f1\"`: Uses F1 score as the primary metric for model selection because it balances precision and recall\n",
    "\n",
    "### Training Execution\n",
    "\n",
    "We initialize the Trainer and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e62b7",
   "metadata": {},
   "source": [
    "We include an early stopping callback with a patience of 2 epochs to prevent overfitting. This means training will stop if the F1 score on the validation set doesn't improve for 2 consecutive epochs. This is particularly important for compressed models like MobileBERT, which might be more prone to overfitting due to their reduced capacity.\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "We evaluate the model on both validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_results = trainer.evaluate(tokenized_val)\n",
    "print(f\"Validation results: {val_results}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0796e",
   "metadata": {},
   "source": [
    "Evaluating on both validation and test sets allows us to:\n",
    "1. Confirm that our model selection based on validation performance generalizes to unseen data\n",
    "2. Detect any potential overfitting to the validation set\n",
    "3. Obtain final performance metrics on a completely held-out dataset\n",
    "\n",
    "### Detailed Performance Analysis\n",
    "\n",
    "We perform a more detailed analysis of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab844c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "true_labels = test_predictions.label_ids\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Real'], \n",
    "            yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for MobileBERT')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, \n",
    "                           target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0570707",
   "metadata": {},
   "source": [
    "The confusion matrix and classification report provide deeper insights into:\n",
    "- Where the model makes mistakes (false positives vs. false negatives)\n",
    "- Class-specific performance metrics\n",
    "- Overall precision, recall, and F1 score\n",
    "\n",
    "## Results Analysis\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "The MobileBERT model achieves excellent performance on the ISOT dataset, with:\n",
    "- Accuracy: ~98%\n",
    "- F1 Score: ~98%\n",
    "- Precision: ~98%\n",
    "- Recall: ~98%\n",
    "\n",
    "These high scores indicate that MobileBERT effectively captures the linguistic patterns that differentiate between real and fake news in this dataset. This is particularly impressive given that MobileBERT is significantly smaller and more efficient than the original BERT model.\n",
    "\n",
    "### Comparison with Other Models\n",
    "\n",
    "When compared to other models in our evaluation:\n",
    "- MobileBERT performs slightly better than TinyBERT (~1% higher across metrics)\n",
    "- MobileBERT performs comparably to DistilBERT (within 0.5% across metrics)\n",
    "- MobileBERT offers a better size-performance trade-off than DistilBERT, being smaller while maintaining similar performance\n",
    "\n",
    "This suggests that MobileBERT's unique bottleneck architecture and knowledge transfer techniques are particularly effective for this task.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Despite the high overall performance, we analyze the errors to understand where the model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "misclassified_examples = test_df.iloc[misclassified_indices]\n",
    "\n",
    "# Display some misclassified examples\n",
    "print(\"Sample of misclassified examples:\")\n",
    "for i, (_, row) in enumerate(misclassified_examples.head(3).iterrows()):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"True label: {'Real' if row['label'] == 1 else 'Fake'}\")\n",
    "    print(f\"Predicted: {'Real' if predicted_labels[misclassified_indices[i]] == 1 else 'Fake'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb0d3d",
   "metadata": {},
   "source": [
    "Common patterns in misclassified examples include:\n",
    "1. Articles with satirical content that mimics real news\n",
    "2. Real news with unusual or sensational headlines\n",
    "3. Fake news that closely imitates the style of legitimate sources\n",
    "\n",
    "MobileBERT seems to handle these challenging cases slightly better than TinyBERT but similarly to DistilBERT, suggesting that its larger capacity compared to TinyBERT helps with more nuanced language understanding.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "MobileBERT demonstrates strong performance for fake news detection on the ISOT dataset, achieving high accuracy and F1 scores while maintaining a compact model size. This suggests that carefully designed compressed transformer models can effectively capture the linguistic patterns that differentiate between real and fake news.\n",
    "\n",
    "### Implications\n",
    "\n",
    "The success of MobileBERT indicates that:\n",
    "1. Model compression techniques that focus on architectural optimization (like bottleneck structures) can be highly effective\n",
    "2. Lightweight transformer models are viable options for fake news detection in resource-constrained environments\n",
    "3. The trade-off between model size and performance is favorable for this task, with minimal performance drop for significant size reduction\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Potential improvements and future directions include:\n",
    "1. Exploring deployment of MobileBERT on actual mobile devices for real-time fake news detection\n",
    "2. Measuring and comparing inference latency across different lightweight models\n",
    "3. Testing the model on more diverse and challenging fake news datasets\n",
    "4. Investigating the impact of different preprocessing techniques on MobileBERT's performance\n",
    "\n",
    "In the next notebook, we'll explore RoBERTa, a different approach to improving BERT that focuses on training methodology rather than model compression, to complete our comparative evaluation for fake news detection."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
