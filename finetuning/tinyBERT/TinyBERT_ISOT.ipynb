{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9b08f2",
   "metadata": {},
   "source": [
    "# Fine-tuning TinyBERT for Fake News Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook documents the process of fine-tuning a TinyBERT model for fake news detection using the ISOT dataset. Building on our previous exploratory data analysis and feature engineering work, we now leverage transformer-based models to capture complex linguistic patterns that might improve performance or provide better generalization to new data.\n",
    "\n",
    "TinyBERT was selected as part of our comparative evaluation because it represents an extreme in model compression while maintaining reasonable performance. As a highly compressed version of BERT (using both knowledge distillation and architectural modifications), TinyBERT is approximately 7.5x smaller and 9.4x faster than the original BERT-base model. This makes it particularly valuable for resource-constrained environments or applications requiring real-time inference.\n",
    "\n",
    "## Setup and Environment Preparation\n",
    "\n",
    "### Library Installation and Imports\n",
    "\n",
    "We begin by installing the necessary libraries for our fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00bb2b",
   "metadata": {},
   "source": [
    "The libraries serve the following purposes:\n",
    "- `transformers`: Provides access to pretrained models like TinyBERT and utilities for fine-tuning\n",
    "- `datasets`: Offers efficient data handling for transformer models\n",
    "- `torch`: Serves as the deep learning framework for model training\n",
    "- `evaluate`: Provides evaluation metrics for model performance assessment\n",
    "- `scikit-learn`: Offers additional metrics and utilities for evaluation\n",
    "\n",
    "Next, we import the specific modules needed for our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37765630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831055cd",
   "metadata": {},
   "source": [
    "Note that we're using the standard BERT tokenizer and model classes from the transformers library, but loading the TinyBERT weights. This is because TinyBERT uses the same architecture as BERT but with fewer layers and smaller hidden dimensions. The transformers library allows us to use the TinyBERT weights with the BERT classes.\n",
    "\n",
    "### Setting Up Reproducibility\n",
    "\n",
    "To ensure our experiments are reproducible, we set random seeds for all libraries that use randomization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd03ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d07bc0",
   "metadata": {},
   "source": [
    "The seed value of 42 is arbitrary but consistently used across all our experiments to ensure fair comparison between models.\n",
    "\n",
    "### Hardware Configuration\n",
    "\n",
    "We check for GPU availability to accelerate training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7ce84",
   "metadata": {},
   "source": [
    "Using a GPU significantly speeds up the training process for transformer models. Even though TinyBERT is more efficient than larger models, GPU acceleration is still beneficial for faster training. If a GPU is not available, the code will still run on CPU, but training will take considerably longer.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We load the preprocessed ISOT dataset that was prepared in our earlier data analysis notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/train_fake_news.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/val_fake_news.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing from Part 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661067e",
   "metadata": {},
   "source": [
    "The dataset has already been split into training, validation, and test sets with a ratio of 70:15:15. This split ensures we have enough data for training while maintaining substantial validation and test sets for reliable evaluation.\n",
    "\n",
    "### Examining the Data\n",
    "\n",
    "We examine the data structure to ensure it matches our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299dec6",
   "metadata": {},
   "source": [
    "The dataset contains three key columns:\n",
    "- `title`: The headline of the news article\n",
    "- `enhanced_cleaned_text`: The preprocessed body text of the article\n",
    "- `label`: Binary classification (0 for fake news, 1 for real news)\n",
    "\n",
    "### Converting to HuggingFace Dataset Format\n",
    "\n",
    "We convert our pandas DataFrames to the HuggingFace Dataset format, which is optimized for working with transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73220f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For TinyBERT, we'll use both title and text\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset\n",
    "\n",
    "# Convert our datasets\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3175a",
   "metadata": {},
   "source": [
    "We combine the title and body text into a single text field for several reasons:\n",
    "1. News headlines often contain important contextual information or framing that can help identify fake news\n",
    "2. TinyBERT, like other BERT variants, can process sequences up to 512 tokens, which is sufficient for most news articles\n",
    "3. This approach provides the model with the maximum available information for classification\n",
    "4. Using the same preprocessing approach across all models ensures fair comparison\n",
    "\n",
    "## Model Architecture and Configuration\n",
    "\n",
    "### Data Cleaning and Preparation\n",
    "\n",
    "Before tokenization, we ensure the dataset is clean and properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd20c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few examples in your dataset\n",
    "print(\"First example in train_dataset:\", train_dataset[0])\n",
    "\n",
    "# Debug the content types\n",
    "print(\"Text type for first example:\", type(train_dataset[0]['text']))\n",
    "\n",
    "# If needed, clean the dataset before tokenization\n",
    "def clean_dataset(example):\n",
    "    example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(clean_dataset)\n",
    "val_dataset = val_dataset.map(clean_dataset)\n",
    "test_dataset = test_dataset.map(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a256c",
   "metadata": {},
   "source": [
    "This cleaning step ensures that all text entries are properly formatted as strings, preventing potential errors during tokenization. It's a defensive programming practice that handles edge cases like None values or non-string data types.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "We prepare the tokenizer for TinyBERT, which converts text into token IDs that the model can process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8518f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with truncation and padding\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization to our datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d0e9d",
   "metadata": {},
   "source": [
    "Key tokenization decisions:\n",
    "- We use the TinyBERT tokenizer which is compatible with the BERT tokenizer\n",
    "- We set `max_length=512` to use the full context window of TinyBERT\n",
    "- We apply padding to ensure all sequences have the same length, which is necessary for batch processing\n",
    "- We use truncation to handle any articles that exceed the maximum length\n",
    "- We use batched processing for efficiency\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "We initialize the TinyBERT model for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'huawei-noah/TinyBERT_General_4L_312D',\n",
    "    num_labels=2,  # Binary classification: fake or real\n",
    "    id2label={0: \"fake\", 1: \"real\"},\n",
    "    label2id={\"fake\": 0, \"real\": 1}\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c94e3b",
   "metadata": {},
   "source": [
    "We use the pretrained TinyBERT model with 4 layers and 312-dimensional embeddings. This configuration was chosen because:\n",
    "1. It offers a good balance between model size and performance\n",
    "2. The 4-layer version is significantly smaller than the original BERT (28MB vs 340MB)\n",
    "3. It has been pretrained on general domain data, making it adaptable to our news classification task\n",
    "\n",
    "## Training Process\n",
    "\n",
    "### Defining Metrics\n",
    "\n",
    "We define a function to compute evaluation metrics during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b78a5",
   "metadata": {},
   "source": [
    "We track multiple metrics because accuracy alone can be misleading, especially if the dataset is imbalanced:\n",
    "- Accuracy: Overall correctness of predictions\n",
    "- Precision: Proportion of positive identifications that were actually correct\n",
    "- Recall: Proportion of actual positives that were identified correctly\n",
    "- F1 Score: Harmonic mean of precision and recall, providing a balance between the two\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "We set up the training arguments with carefully chosen hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/tinybert',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,  # Larger batch size due to smaller model\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b826323",
   "metadata": {},
   "source": [
    "Key hyperparameter choices and their rationale:\n",
    "- `num_train_epochs=5`: Provides sufficient training iterations while avoiding overfitting\n",
    "- `per_device_train_batch_size=32`: Larger than for other models because TinyBERT requires less memory, allowing for more efficient training\n",
    "- `warmup_steps=500`: Gradually increases the learning rate to stabilize early training\n",
    "- `weight_decay=0.01`: Adds L2 regularization to prevent overfitting\n",
    "- `evaluation_strategy=\"epoch\"`: Evaluates after each epoch to track progress\n",
    "- `metric_for_best_model=\"f1\"`: Uses F1 score as the primary metric for model selection because it balances precision and recall\n",
    "\n",
    "### Training Execution\n",
    "\n",
    "We initialize the Trainer and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03660d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e186953",
   "metadata": {},
   "source": [
    "We include an early stopping callback with a patience of 2 epochs to prevent overfitting. This means training will stop if the F1 score on the validation set doesn't improve for 2 consecutive epochs. This is particularly important for smaller models like TinyBERT, which might be more prone to overfitting due to their limited capacity.\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "We evaluate the model on both validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bf452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_results = trainer.evaluate(tokenized_val)\n",
    "print(f\"Validation results: {val_results}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187a2c3",
   "metadata": {},
   "source": [
    "Evaluating on both validation and test sets allows us to:\n",
    "1. Confirm that our model selection based on validation performance generalizes to unseen data\n",
    "2. Detect any potential overfitting to the validation set\n",
    "3. Obtain final performance metrics on a completely held-out dataset\n",
    "\n",
    "### Detailed Performance Analysis\n",
    "\n",
    "We perform a more detailed analysis of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b741d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "true_labels = test_predictions.label_ids\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Real'], \n",
    "            yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for TinyBERT')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, \n",
    "                           target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e9eeb",
   "metadata": {},
   "source": [
    "The confusion matrix and classification report provide deeper insights into:\n",
    "- Where the model makes mistakes (false positives vs. false negatives)\n",
    "- Class-specific performance metrics\n",
    "- Overall precision, recall, and F1 score\n",
    "\n",
    "## Results Analysis\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "The TinyBERT model achieves excellent performance on the ISOT dataset, with:\n",
    "- Accuracy: ~97%\n",
    "- F1 Score: ~97%\n",
    "- Precision: ~97%\n",
    "- Recall: ~97%\n",
    "\n",
    "These high scores indicate that even a highly compressed model like TinyBERT can effectively capture the linguistic patterns that differentiate between real and fake news in this dataset. This is particularly impressive given that TinyBERT has only 4 layers compared to BERT's 12 layers.\n",
    "\n",
    "### Comparison with Other Models\n",
    "\n",
    "When compared to larger models like DistilBERT, TinyBERT shows only a slight decrease in performance (approximately 1-2% lower across metrics). This small performance gap is a reasonable trade-off considering:\n",
    "1. TinyBERT is significantly smaller (28MB vs 66MB for DistilBERT)\n",
    "2. Inference is faster, which is important for real-time applications\n",
    "3. The model requires less memory, making it suitable for edge devices\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Despite the high overall performance, we analyze the errors to understand where the model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "misclassified_examples = test_df.iloc[misclassified_indices]\n",
    "\n",
    "# Display some misclassified examples\n",
    "print(\"Sample of misclassified examples:\")\n",
    "for i, (_, row) in enumerate(misclassified_examples.head(3).iterrows()):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"True label: {'Real' if row['label'] == 1 else 'Fake'}\")\n",
    "    print(f\"Predicted: {'Real' if predicted_labels[misclassified_indices[i]] == 1 else 'Fake'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fad919",
   "metadata": {},
   "source": [
    "Common patterns in misclassified examples include:\n",
    "1. Articles with complex or nuanced language that might require deeper semantic understanding\n",
    "2. Articles with ambiguous phrasing or context-dependent meaning\n",
    "3. Fake news that closely mimics the style and structure of legitimate sources\n",
    "\n",
    "TinyBERT seems to struggle slightly more than larger models with these complex cases, which is expected given its reduced capacity.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "TinyBERT demonstrates strong performance for fake news detection on the ISOT dataset, achieving high accuracy and F1 scores despite its compact size. This suggests that even highly compressed transformer models can effectively capture the linguistic patterns that differentiate between real and fake news.\n",
    "\n",
    "### Implications\n",
    "\n",
    "The success of TinyBERT indicates that:\n",
    "1. Model compression techniques like knowledge distillation and architectural modifications can preserve most of the performance while drastically reducing model size\n",
    "2. Lightweight transformer models are viable options for fake news detection in resource-constrained environments\n",
    "3. The trade-off between model size and performance is favorable for this task, with only a small performance drop for significant size reduction\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Potential improvements and future directions include:\n",
    "1. Exploring task-specific distillation for TinyBERT to further improve performance\n",
    "2. Investigating the minimum model size that can achieve acceptable performance for fake news detection\n",
    "3. Testing the model on more diverse and challenging fake news datasets\n",
    "4. Comparing inference speed and memory usage across different lightweight models in real-world deployment scenarios\n",
    "\n",
    "In the next notebooks, we'll explore other lightweight transformer models (MobileBERT and RoBERTa) to complete our comparative evaluation for fake news detection."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
