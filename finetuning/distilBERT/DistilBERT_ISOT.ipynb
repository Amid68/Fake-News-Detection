{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f213c6ef",
   "metadata": {},
   "source": [
    "# Fine-tuning DistilBERT for Fake News Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook documents the process of fine-tuning a DistilBERT model for fake news detection using the ISOT dataset. Building on our previous exploratory data analysis and feature engineering work, we now leverage transformer-based models to capture more complex linguistic patterns that might improve performance or provide better generalization to new data.\n",
    "\n",
    "DistilBERT was selected as our first transformer model because it offers a good balance between computational efficiency and performance. As a knowledge-distilled version of BERT, it retains about 97% of BERT's language understanding capabilities while being 40% smaller and 60% faster. This makes it an excellent starting point for our comparative evaluation of lightweight pretrained models.\n",
    "\n",
    "## Setup and Environment Preparation\n",
    "\n",
    "### Library Installation and Imports\n",
    "\n",
    "We begin by installing the necessary libraries for our fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecae0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488c2c9",
   "metadata": {},
   "source": [
    "The libraries serve the following purposes:\n",
    "- `transformers`: Provides access to pretrained models like DistilBERT and utilities for fine-tuning\n",
    "- `datasets`: Offers efficient data handling for transformer models\n",
    "- `torch`: Serves as the deep learning framework for model training\n",
    "- `evaluate`: Provides evaluation metrics for model performance assessment\n",
    "- `scikit-learn`: Offers additional metrics and utilities for evaluation\n",
    "\n",
    "Next, we import the specific modules needed for our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac63e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44fb40",
   "metadata": {},
   "source": [
    "### Setting Up Reproducibility\n",
    "\n",
    "To ensure our experiments are reproducible, we set random seeds for all libraries that use randomization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e73294",
   "metadata": {},
   "source": [
    "The seed value of 42 is arbitrary but consistently used across all our experiments to ensure fair comparison between models.\n",
    "\n",
    "### Hardware Configuration\n",
    "\n",
    "We check for GPU availability to accelerate training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0217427",
   "metadata": {},
   "source": [
    "Using a GPU significantly speeds up the training process for transformer models. If a GPU is not available, the code will still run on CPU, but training will take considerably longer.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We load the preprocessed ISOT dataset that was prepared in our earlier data analysis notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/train_fake_news.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/val_fake_news.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing from Part 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8993d1a",
   "metadata": {},
   "source": [
    "The dataset has already been split into training, validation, and test sets with a ratio of 70:15:15. This split ensures we have enough data for training while maintaining substantial validation and test sets for reliable evaluation.\n",
    "\n",
    "### Examining the Data\n",
    "\n",
    "We examine the data structure to ensure it matches our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0dc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811de5a8",
   "metadata": {},
   "source": [
    "The dataset contains three key columns:\n",
    "- `title`: The headline of the news article\n",
    "- `enhanced_cleaned_text`: The preprocessed body text of the article\n",
    "- `label`: Binary classification (0 for fake news, 1 for real news)\n",
    "\n",
    "### Converting to HuggingFace Dataset Format\n",
    "\n",
    "We convert our pandas DataFrames to the HuggingFace Dataset format, which is optimized for working with transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f59708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For DistilBERT, we'll combine title and text for better context\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset\n",
    "\n",
    "# Convert our datasets\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45895a",
   "metadata": {},
   "source": [
    "We combine the title and body text into a single text field because:\n",
    "1. News headlines often contain important contextual information\n",
    "2. DistilBERT can process sequences up to 512 tokens, which is sufficient for most news articles\n",
    "3. This approach provides the model with the maximum available information for classification\n",
    "\n",
    "## Model Architecture and Configuration\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "We prepare the tokenizer for DistilBERT, which converts text into token IDs that the model can process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034db6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with truncation and padding\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization to our datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99836c83",
   "metadata": {},
   "source": [
    "Key tokenization decisions:\n",
    "- We use the uncased version of DistilBERT because case information is less critical for fake news detection\n",
    "- We set `max_length=512` to use the full context window of DistilBERT\n",
    "- We apply padding to ensure all sequences have the same length, which is necessary for batch processing\n",
    "- We use truncation to handle any articles that exceed the maximum length\n",
    "\n",
    "### Model Initialization\n",
    "\n",
    "We initialize the DistilBERT model for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201abe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2,  # Binary classification: fake or real\n",
    "    id2label={0: \"fake\", 1: \"real\"},\n",
    "    label2id={\"fake\": 0, \"real\": 1}\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50c043",
   "metadata": {},
   "source": [
    "We use the pretrained DistilBERT model and adapt it for our binary classification task. The pretrained weights provide a strong starting point that captures general language understanding, which we'll fine-tune for our specific task of fake news detection.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "### Defining Metrics\n",
    "\n",
    "We define a function to compute evaluation metrics during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b887a",
   "metadata": {},
   "source": [
    "We track multiple metrics because accuracy alone can be misleading, especially if the dataset is imbalanced:\n",
    "- Accuracy: Overall correctness of predictions\n",
    "- Precision: Proportion of positive identifications that were actually correct\n",
    "- Recall: Proportion of actual positives that were identified correctly\n",
    "- F1 Score: Harmonic mean of precision and recall, providing a balance between the two\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "We set up the training arguments with carefully chosen hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/distilbert',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c8a99",
   "metadata": {},
   "source": [
    "Key hyperparameter choices and their rationale:\n",
    "- `num_train_epochs=5`: Provides sufficient training iterations while avoiding overfitting\n",
    "- `per_device_train_batch_size=16`: Balances memory constraints with training efficiency\n",
    "- `warmup_steps=500`: Gradually increases the learning rate to stabilize early training\n",
    "- `weight_decay=0.01`: Adds L2 regularization to prevent overfitting\n",
    "- `evaluation_strategy=\"epoch\"`: Evaluates after each epoch to track progress\n",
    "- `metric_for_best_model=\"f1\"`: Uses F1 score as the primary metric for model selection because it balances precision and recall\n",
    "\n",
    "### Training Execution\n",
    "\n",
    "We initialize the Trainer and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c75a6",
   "metadata": {},
   "source": [
    "We include an early stopping callback with a patience of 2 epochs to prevent overfitting. This means training will stop if the F1 score on the validation set doesn't improve for 2 consecutive epochs.\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "We evaluate the model on both validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_results = trainer.evaluate(tokenized_val)\n",
    "print(f\"Validation results: {val_results}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9680388",
   "metadata": {},
   "source": [
    "Evaluating on both validation and test sets allows us to:\n",
    "1. Confirm that our model selection based on validation performance generalizes to unseen data\n",
    "2. Detect any potential overfitting to the validation set\n",
    "3. Obtain final performance metrics on a completely held-out dataset\n",
    "\n",
    "### Detailed Performance Analysis\n",
    "\n",
    "We perform a more detailed analysis of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "true_labels = test_predictions.label_ids\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Real'], \n",
    "            yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for DistilBERT')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, \n",
    "                           target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610467f",
   "metadata": {},
   "source": [
    "The confusion matrix and classification report provide deeper insights into:\n",
    "- Where the model makes mistakes (false positives vs. false negatives)\n",
    "- Class-specific performance metrics\n",
    "- Overall precision, recall, and F1 score\n",
    "\n",
    "## Results Analysis\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "The DistilBERT model achieves excellent performance on the ISOT dataset, with:\n",
    "- Accuracy: ~98%\n",
    "- F1 Score: ~98%\n",
    "- Precision: ~98%\n",
    "- Recall: ~98%\n",
    "\n",
    "These high scores indicate that DistilBERT effectively captures the linguistic patterns that differentiate between real and fake news in this dataset.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Despite the high overall performance, we analyze the errors to understand where the model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "misclassified_examples = test_df.iloc[misclassified_indices]\n",
    "\n",
    "# Display some misclassified examples\n",
    "print(\"Sample of misclassified examples:\")\n",
    "for i, (_, row) in enumerate(misclassified_examples.head(3).iterrows()):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"True label: {'Real' if row['label'] == 1 else 'Fake'}\")\n",
    "    print(f\"Predicted: {'Real' if predicted_labels[misclassified_indices[i]] == 1 else 'Fake'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7e69a",
   "metadata": {},
   "source": [
    "Common patterns in misclassified examples include:\n",
    "1. Articles with satirical content that mimics real news\n",
    "2. Real news with unusual or sensational headlines\n",
    "3. Fake news that closely imitates the style of legitimate sources\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "While DistilBERT performs well on the ISOT dataset, it has several limitations:\n",
    "1. It may not generalize well to news from different domains or time periods\n",
    "2. The model doesn't explicitly consider external knowledge or fact verification\n",
    "3. As a smaller model, it may miss some nuanced linguistic patterns that larger models could capture\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "DistilBERT demonstrates strong performance for fake news detection on the ISOT dataset, achieving high accuracy and F1 scores. This suggests that transformer-based models can effectively capture the linguistic patterns that differentiate between real and fake news.\n",
    "\n",
    "### Implications\n",
    "\n",
    "The success of DistilBERT, a lightweight transformer model, indicates that:\n",
    "1. Complex linguistic patterns are important for fake news detection\n",
    "2. Pretrained language models can be effectively fine-tuned for this task\n",
    "3. Smaller, distilled models can achieve excellent performance while being more computationally efficient\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Potential improvements and future directions include:\n",
    "1. Experimenting with other lightweight transformer models for comparison\n",
    "2. Incorporating external knowledge or fact-checking mechanisms\n",
    "3. Testing the model on more diverse and challenging fake news datasets\n",
    "4. Exploring model interpretability to understand what features are most important for classification\n",
    "\n",
    "In the next notebooks, we'll explore other lightweight transformer models (TinyBERT, MobileBERT, and RoBERTa) to compare their performance and efficiency for fake news detection."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
