{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b09e50",
   "metadata": {},
   "source": [
    "# Part 3: Fine-tuning DistilBERT for Fake News Detection\n",
    "\n",
    "In this notebook, I'll build on our previous exploratory data analysis and feature engineering work to fine-tune a DistilBERT model for fake news detection. While our engineered features achieved impressive results, transformer models like DistilBERT can capture more complex linguistic patterns that might further improve performance or provide better generalization to new data.\n",
    "\n",
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, I'll import the necessary libraries and install any missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset as HFDataset\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f0f8e",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare the Dataset\n",
    "\n",
    "I'll load the preprocessed datasets from our previous work. If you're running this notebook independently, make sure you have the processed files from Part 2, or run the data preprocessing steps from the previous notebooks first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('train_fake_news.csv')\n",
    "    val_df = pd.read_csv('val_fake_news.csv') \n",
    "    test_df = pd.read_csv('test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Please run the data preprocessing from Part 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b73f2",
   "metadata": {},
   "source": [
    "Let's examine the data format to ensure it's what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65949b",
   "metadata": {},
   "source": [
    "Next, I'll convert our pandas DataFrames to the Hugging Face Dataset format, which is optimized for working with the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pandas DataFrames to HuggingFace Datasets\n",
    "def convert_to_hf_dataset(df):\n",
    "    # For DistilBERT, we'll use both title and text\n",
    "    df['text'] = df['title'] + \" \" + df['enhanced_cleaned_text']\n",
    "    \n",
    "    # Convert to HuggingFace Dataset format\n",
    "    dataset = HFDataset.from_pandas(df[['text', 'label']])\n",
    "    return dataset\n",
    "\n",
    "# Convert our datasets\n",
    "train_dataset = convert_to_hf_dataset(train_df)\n",
    "val_dataset = convert_to_hf_dataset(val_df)\n",
    "test_dataset = convert_to_hf_dataset(test_df)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cf48b",
   "metadata": {},
   "source": [
    "## 3. Prepare Tokenizer and Model\n",
    "\n",
    "Now I'll set up the DistilBERT tokenizer and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the maximum sequence length\n",
    "# Most news articles are quite long, but we need to balance information retention with computational efficiency\n",
    "max_length = 512  # This is the maximum that BERT models can handle\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to our datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4670169",
   "metadata": {},
   "source": [
    "## 4. Define Metrics and Evaluation Strategy\n",
    "\n",
    "I'll define our evaluation metrics to track model performance during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ddf09",
   "metadata": {},
   "source": [
    "## 5. Initialize Model for Fine-tuning\n",
    "\n",
    "Now I'll initialize the DistilBERT model for sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # Binary classification: 0 for fake, 1 for real\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba4d6a",
   "metadata": {},
   "source": [
    "## 6. Define Training Arguments and Trainer\n",
    "\n",
    "Next, I'll configure the training parameters and create a Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=100,               # Log every X steps\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save model checkpoint every epoch\n",
    "    load_best_model_at_end=True,     # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",      # Use F1 score to determine the best model\n",
    "    push_to_hub=False,               # Don't push to Hugging Face Hub\n",
    "    report_to=\"none\"                 # Disable reporting to avoid wandb or other services\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_tokenized,       # Training dataset\n",
    "    eval_dataset=val_tokenized,          # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # The function to compute metrics\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0a10a",
   "metadata": {},
   "source": [
    "## 7. Fine-tune the Model\n",
    "\n",
    "Now I'll fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a143610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer to measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time/60:.2f} minutes\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./distilbert-fake-news-detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fc168",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "I'll evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_tokenized)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438610d",
   "metadata": {},
   "source": [
    "Let's also look at the confusion matrix to get a better understanding of the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "test_pred = trainer.predict(test_tokenized)\n",
    "y_preds = np.argmax(test_pred.predictions, axis=1)\n",
    "y_true = test_pred.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('DistilBERT Confusion Matrix')\n",
    "plt.savefig('distilbert_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079be815",
   "metadata": {},
   "source": [
    "## 9. Compare Results with Previous Models\n",
    "\n",
    "Let's compare the DistilBERT results with our previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results from our previous models\n",
    "previous_results = {\n",
    "    'Engineered Features': {\n",
    "        'accuracy': 0.9998,\n",
    "        'precision': 1.0,\n",
    "        'recall': 1.0,\n",
    "        'f1': 1.0\n",
    "    },\n",
    "    'TF-IDF': {\n",
    "        'accuracy': 0.984,\n",
    "        'precision': 0.985,\n",
    "        'recall': 0.984,\n",
    "        'f1': 0.984\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add DistilBERT results\n",
    "model_results = {\n",
    "    'Engineered Features': previous_results['Engineered Features'],\n",
    "    'TF-IDF': previous_results['TF-IDF'],\n",
    "    'DistilBERT': {\n",
    "        'accuracy': test_results['eval_accuracy'],\n",
    "        'precision': test_results['eval_precision'],\n",
    "        'recall': test_results['eval_recall'],\n",
    "        'f1': test_results['eval_f1']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "# Create comparative bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "results_df.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.90, 1.01)  # Set y-axis to focus on the high performance range\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2793a40",
   "metadata": {},
   "source": [
    "## 10. Analyze Misclassified Examples\n",
    "\n",
    "Let's analyze some misclassified examples to understand where the model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of misclassified examples\n",
    "misclassified_indices = np.where(y_preds != y_true)[0]\n",
    "print(f\"Number of misclassified examples: {len(misclassified_indices)}\")\n",
    "\n",
    "# If there are misclassifications, analyze a few\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Get the original text and predictions\n",
    "    misclassified_texts = []\n",
    "    for idx in misclassified_indices[:5]:  # Examine up to 5 examples\n",
    "        original_idx = test_dataset[idx]['__index_level_0__'] if '__index_level_0__' in test_dataset[idx] else idx\n",
    "        \n",
    "        text = test_df.iloc[original_idx]['title']\n",
    "        true_label = \"Real\" if y_true[idx] == 1 else \"Fake\"\n",
    "        pred_label = \"Real\" if y_preds[idx] == 1 else \"Fake\"\n",
    "        \n",
    "        misclassified_texts.append({\n",
    "            'Title': text,\n",
    "            'True Label': true_label,\n",
    "            'Predicted Label': pred_label\n",
    "        })\n",
    "    \n",
    "    # Display misclassified examples\n",
    "    print(\"\\nSample of misclassified examples:\")\n",
    "    pd.DataFrame(misclassified_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb1de3",
   "metadata": {},
   "source": [
    "## 11. Use the Model for Predictions\n",
    "\n",
    "Now I'll show how to use our fine-tuned model to make predictions on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505dd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new texts\n",
    "def predict_fake_news(texts, model, tokenizer, device):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get predicted class (0 for fake, 1 for real)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_classes = predictions.argmax(dim=1).cpu().numpy()\n",
    "    confidence = predictions.max(dim=1).values.cpu().numpy()\n",
    "    \n",
    "    # Convert to human-readable labels\n",
    "    results = []\n",
    "    for i, pred_class in enumerate(pred_classes):\n",
    "        label = \"Real News\" if pred_class == 1 else \"Fake News\"\n",
    "        results.append({\n",
    "            'text': texts[i],\n",
    "            'prediction': label,\n",
    "            'confidence': confidence[i],\n",
    "            'class': pred_class\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example texts to test\n",
    "sample_texts = [\n",
    "    # Example of real news (from a reputable source)\n",
    "    \"Senate Passes Bipartisan Infrastructure Bill. The bill would provide funding for roads, bridges and other physical infrastructure.\",\n",
    "    \n",
    "    # Example of fake news (made-up sensationalist headline)\n",
    "    \"BOMBSHELL: Government Admits Mind Control Program Targeting Citizens! Secret documents reveal shocking truth.\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "sample_predictions = predict_fake_news(sample_texts, model, tokenizer, device)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions on sample texts:\")\n",
    "print(sample_predictions[['text', 'prediction', 'confidence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c1c5d",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary and recommendations\n",
    "conclusions = \"\"\"\n",
    "# Conclusions from DistilBERT Fine-tuning\n",
    "\n",
    "In this notebook, I've fine-tuned a DistilBERT model for fake news detection on the ISOT dataset. Here are the key findings:\n",
    "\n",
    "1. **Performance Comparison**: The DistilBERT model achieved [insert accuracy here] accuracy, which is [better/worse/comparable] to our previous models using engineered features (99.98%) and TF-IDF (98.4%).\n",
    "\n",
    "2. **Training Efficiency**: Despite being more complex, DistilBERT is quite efficient for fine-tuning, with the process completing in approximately [insert time] minutes.\n",
    "\n",
    "3. **Error Analysis**: Analysis of misclassified examples shows that DistilBERT struggles with [insert observations about errors].\n",
    "\n",
    "4. **Generalization Potential**: Transformer models like DistilBERT likely have better generalization capabilities to new and unseen fake news, as they understand context and semantic meaning more deeply.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Experiment with Other Pretrained Models**: Try fine-tuning larger models like BERT-base or RoBERTa to see if they offer improvements.\n",
    "\n",
    "2. **Combined Approach**: Develop an ensemble model that combines our engineered features with transformer-based features.\n",
    "\n",
    "3. **External Validation**: Test the model on different fake news datasets to evaluate cross-dataset generalization.\n",
    "\n",
    "4. **Model Explainability**: Implement techniques like LIME or SHAP to understand which parts of text the model relies on for classification.\n",
    "\n",
    "5. **Deployment Considerations**: Optimize the model for inference time if it's to be used in a real-time application.\n",
    "\n",
    "The transformer-based approach offers a powerful complement to our feature engineering work, potentially providing better generalization to evolving fake news tactics and new domains.\n",
    "\"\"\"\n",
    "\n",
    "print(conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f344ff",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive approach to fine-tuning DistilBERT for fake news detection, building on our previous work of exploratory data analysis and feature engineering. The transformer-based approach captures complex linguistic patterns that may complement our engineered features and improve model robustness."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
