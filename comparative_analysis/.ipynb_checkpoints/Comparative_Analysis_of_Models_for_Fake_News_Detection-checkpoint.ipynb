{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17549763",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Models for Fake News Detection\n",
    "\n",
    "This notebook provides a comprehensive comparison of all models evaluated for fake news detection on the ISOT dataset, including both traditional machine learning approaches and transformer-based models. This analysis helps identify the optimal model based on performance, efficiency, and resource requirements.\n",
    "\n",
    "## 1. Performance Metrics Comparison\n",
    "\n",
    "First, let's compare the core performance metrics across all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4484c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a comprehensive comparison table\n",
    "models = [\n",
    "    'Logistic Regression', \n",
    "    'Naive Bayes', \n",
    "    'Linear SVM', \n",
    "    'DistilBERT', \n",
    "    'TinyBERT', \n",
    "    'RoBERTa', \n",
    "    'MobileBERT'\n",
    "]\n",
    "\n",
    "# Performance metrics\n",
    "accuracy = [\n",
    "    0.9899,  # Logistic Regression\n",
    "    0.9661,  # Naive Bayes\n",
    "    0.9910,  # Linear SVM\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "f1_scores = [\n",
    "    0.9899,  # Logistic Regression\n",
    "    0.9661,  # Naive Bayes\n",
    "    0.9910,  # Linear SVM\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "precision = [\n",
    "    0.9899,  # Logistic Regression\n",
    "    0.9662,  # Naive Bayes\n",
    "    0.9910,  # Linear SVM\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "recall = [\n",
    "    0.9899,  # Logistic Regression\n",
    "    0.9661,  # Naive Bayes\n",
    "    0.9910,  # Linear SVM\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "# Create the comparison DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall\n",
    "})\n",
    "\n",
    "# Add model type column\n",
    "model_types = ['Traditional ML'] * 3 + ['Transformer'] * 4\n",
    "performance_df['Model Type'] = model_types\n",
    "\n",
    "# Display the performance comparison table\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e66d07",
   "metadata": {},
   "source": [
    "### Performance Metrics Table\n",
    "\n",
    "| Model               | Accuracy | F1 Score | Precision | Recall  | Model Type     |\n",
    "|---------------------|----------|----------|-----------|---------|----------------|\n",
    "| Logistic Regression | 0.9899   | 0.9899   | 0.9899    | 0.9899  | Traditional ML |\n",
    "| Naive Bayes         | 0.9661   | 0.9661   | 0.9662    | 0.9661  | Traditional ML |\n",
    "| Linear SVM          | 0.9910   | 0.9910   | 0.9910    | 0.9910  | Traditional ML |\n",
    "| DistilBERT          | 0.9996   | 0.9996   | 0.9996    | 0.9996  | Transformer    |\n",
    "| TinyBERT            | 0.9991   | 0.9991   | 0.9991    | 0.9991  | Transformer    |\n",
    "| RoBERTa             | 1.0000   | 1.0000   | 1.0000    | 1.0000  | Transformer    |\n",
    "| MobileBERT          | 0.9996   | 0.9996   | 0.9996    | 0.9996  | Transformer    |\n",
    "\n",
    "### Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(models))\n",
    "\n",
    "plt.bar(index, performance_df['Accuracy'], bar_width, label='Accuracy', color='#1f77b4')\n",
    "plt.bar(index + bar_width, performance_df['F1 Score'], bar_width, label='F1 Score', color='#ff7f0e')\n",
    "plt.bar(index + 2*bar_width, performance_df['Precision'], bar_width, label='Precision', color='#2ca02c')\n",
    "plt.bar(index + 3*bar_width, performance_df['Recall'], bar_width, label='Recall', color='#d62728')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Comparison Across All Models')\n",
    "plt.xticks(index + 1.5*bar_width, models, rotation=45, ha='right')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.95, 1.01)  # Adjust y-axis to better show differences\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_performance_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f538a",
   "metadata": {},
   "source": [
    "![Performance Metrics Comparison](all_models_performance_metrics.png)\n",
    "\n",
    "## 2. Resource Efficiency Comparison\n",
    "\n",
    "Next, let's compare the computational efficiency and resource requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training times (minutes)\n",
    "training_times = [\n",
    "    3.06,   # Logistic Regression\n",
    "    0.16,   # Naive Bayes\n",
    "    1.57,   # Linear SVM\n",
    "    48.69,  # DistilBERT\n",
    "    8.99,   # TinyBERT\n",
    "    62.35,  # RoBERTa\n",
    "    39.18   # MobileBERT\n",
    "]\n",
    "\n",
    "# Inference times (ms per sample)\n",
    "inference_times = [\n",
    "    0.004,   # Logistic Regression\n",
    "    0.004,   # Naive Bayes\n",
    "    0.004,   # Linear SVM\n",
    "    61.76,   # DistilBERT\n",
    "    17.08,   # TinyBERT\n",
    "    118.37,  # RoBERTa\n",
    "    113.50   # MobileBERT\n",
    "]\n",
    "\n",
    "# Model sizes (parameters)\n",
    "model_sizes = [\n",
    "    \"~50K\",    # Logistic Regression (depends on features)\n",
    "    \"~50K\",    # Naive Bayes (depends on features)\n",
    "    \"~50K\",    # Linear SVM (depends on features)\n",
    "    \"67M\",     # DistilBERT\n",
    "    \"15M\",     # TinyBERT\n",
    "    \"125M\",    # RoBERTa\n",
    "    \"25M\"      # MobileBERT\n",
    "]\n",
    "\n",
    "# Memory usage (MB)\n",
    "memory_usage = [\n",
    "    \"~200\",    # Logistic Regression\n",
    "    \"~150\",    # Naive Bayes\n",
    "    \"~200\",    # Linear SVM\n",
    "    \"~1500\",   # DistilBERT\n",
    "    \"~1000\",   # TinyBERT\n",
    "    \"~2000\",   # RoBERTa\n",
    "    \"~1200\"    # MobileBERT\n",
    "]\n",
    "\n",
    "# Create the resource comparison DataFrame\n",
    "resource_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Training Time (min)': training_times,\n",
    "    'Inference Time (ms/sample)': inference_times,\n",
    "    'Model Size': model_sizes,\n",
    "    'Memory Usage (MB)': memory_usage,\n",
    "    'Model Type': model_types\n",
    "})\n",
    "\n",
    "# Display the resource comparison table\n",
    "resource_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1e5ce",
   "metadata": {},
   "source": [
    "### Resource Efficiency Table\n",
    "\n",
    "| Model               | Training Time (min) | Inference Time (ms/sample) | Model Size | Memory Usage (MB) | Model Type     |\n",
    "|---------------------|---------------------|----------------------------|------------|-------------------|----------------|\n",
    "| Logistic Regression | 3.06                | 0.004                      | ~50K       | ~200              | Traditional ML |\n",
    "| Naive Bayes         | 0.16                | 0.004                      | ~50K       | ~150              | Traditional ML |\n",
    "| Linear SVM          | 1.57                | 0.004                      | ~50K       | ~200              | Traditional ML |\n",
    "| DistilBERT          | 48.69               | 61.76                      | 67M        | ~1500             | Transformer    |\n",
    "| TinyBERT            | 8.99                | 17.08                      | 15M        | ~1000             | Transformer    |\n",
    "| RoBERTa             | 62.35               | 118.37                     | 125M       | ~2000             | Transformer    |\n",
    "| MobileBERT          | 39.18               | 113.50                     | 25M        | ~1200             | Transformer    |\n",
    "\n",
    "### Resource Usage Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training time\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(models, resource_df['Training Time (min)'], color=['#1f77b4' if t == 'Traditional ML' else '#ff7f0e' for t in model_types])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (minutes)')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_time_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot inference time (log scale)\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(models, resource_df['Inference Time (ms/sample)'], color=['#1f77b4' if t == 'Traditional ML' else '#ff7f0e' for t in model_types])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Inference Time (ms/sample)')\n",
    "plt.title('Inference Time Comparison (Log Scale)')\n",
    "plt.yscale('log')  # Use log scale to show the large differences\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_time_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff8478",
   "metadata": {},
   "source": [
    "![Training Time Comparison](training_time_comparison.png)\n",
    "\n",
    "![Inference Time Comparison](inference_time_comparison.png)\n",
    "\n",
    "## 3. Performance-Efficiency Trade-off Analysis\n",
    "\n",
    "To better understand the trade-offs between performance and efficiency, let's create a visualization that plots accuracy against inference time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of accuracy vs. inference time\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, model_type in enumerate(model_types):\n",
    "    if model_type == 'Traditional ML':\n",
    "        marker = 'o'\n",
    "        color = '#1f77b4'\n",
    "    else:\n",
    "        marker = 's'\n",
    "        color = '#ff7f0e'\n",
    "    \n",
    "    plt.scatter(\n",
    "        resource_df['Inference Time (ms/sample)'][i], \n",
    "        performance_df['Accuracy'][i], \n",
    "        s=200, \n",
    "        marker=marker, \n",
    "        color=color, \n",
    "        label=model_type if model_type not in plt.gca().get_legend_handles_labels()[1] else \"\"\n",
    "    )\n",
    "    plt.annotate(\n",
    "        models[i], \n",
    "        (resource_df['Inference Time (ms/sample)'][i], performance_df['Accuracy'][i]),\n",
    "        xytext=(10, 0), \n",
    "        textcoords='offset points'\n",
    "    )\n",
    "\n",
    "plt.xscale('log')  # Log scale for inference time\n",
    "plt.xlabel('Inference Time (ms/sample) - Log Scale')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance-Efficiency Trade-off')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_efficiency_tradeoff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55658fb6",
   "metadata": {},
   "source": [
    "![Performance-Efficiency Trade-off](performance_efficiency_tradeoff.png)\n",
    "\n",
    "## 4. Comprehensive Model Comparison\n",
    "\n",
    "Let's create a final comprehensive table that combines all metrics and includes a \"Recommendation\" column based on different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f209ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison DataFrame\n",
    "comprehensive_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Model Type': model_types,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Training Time (min)': training_times,\n",
    "    'Inference Time (ms/sample)': inference_times,\n",
    "    'Model Size': model_sizes,\n",
    "    'Memory Usage (MB)': memory_usage\n",
    "})\n",
    "\n",
    "# Add recommendations based on use cases\n",
    "recommendations = [\n",
    "    \"Good baseline, balanced performance and efficiency\",  # Logistic Regression\n",
    "    \"Fastest training, but lowest accuracy\",               # Naive Bayes\n",
    "    \"Best traditional ML model, excellent efficiency\",     # Linear SVM\n",
    "    \"Excellent performance, moderate resource usage\",      # DistilBERT\n",
    "    \"Best efficiency-performance balance\",                 # TinyBERT\n",
    "    \"Highest accuracy, highest resource requirements\",     # RoBERTa\n",
    "    \"Good for mobile/edge deployment\"                      # MobileBERT\n",
    "]\n",
    "\n",
    "comprehensive_df['Recommendation'] = recommendations\n",
    "\n",
    "# Display the comprehensive comparison table\n",
    "comprehensive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90c38d",
   "metadata": {},
   "source": [
    "### Comprehensive Model Comparison Table\n",
    "\n",
    "| Model               | Model Type     | Accuracy | F1 Score | Training Time (min) | Inference Time (ms/sample) | Model Size | Memory Usage (MB) | Recommendation                                |\n",
    "|---------------------|----------------|----------|----------|---------------------|----------------------------|------------|-------------------|----------------------------------------------|\n",
    "| Logistic Regression | Traditional ML | 0.9899   | 0.9899   | 3.06                | 0.004                      | ~50K       | ~200              | Good baseline, balanced performance and efficiency |\n",
    "| Naive Bayes         | Traditional ML | 0.9661   | 0.9661   | 0.16                | 0.004                      | ~50K       | ~150              | Fastest training, but lowest accuracy        |\n",
    "| Linear SVM          | Traditional ML | 0.9910   | 0.9910   | 1.57                | 0.004                      | ~50K       | ~200              | Best traditional ML model, excellent efficiency |\n",
    "| DistilBERT          | Transformer    | 0.9996   | 0.9996   | 48.69               | 61.76                      | 67M        | ~1500             | Excellent performance, moderate resource usage |\n",
    "| TinyBERT            | Transformer    | 0.9991   | 0.9991   | 8.99                | 17.08                      | 15M        | ~1000             | Best efficiency-performance balance          |\n",
    "| RoBERTa             | Transformer    | 1.0000   | 1.0000   | 62.35               | 118.37                     | 125M       | ~2000             | Highest accuracy, highest resource requirements |\n",
    "| MobileBERT          | Transformer    | 0.9996   | 0.9996   | 39.18               | 113.50                     | 25M        | ~1200             | Good for mobile/edge deployment              |\n",
    "\n",
    "## 5. Decision Framework for Model Selection\n",
    "\n",
    "Based on our comprehensive analysis, here's a decision framework to help select the most appropriate model for different use cases:\n",
    "\n",
    "### For Maximum Accuracy (>99.9%)\n",
    "- **Best Choice:** RoBERTa (100% accuracy)\n",
    "- **Alternatives:** DistilBERT or MobileBERT (99.96% accuracy)\n",
    "- **Considerations:** Requires significant computational resources\n",
    "\n",
    "### For Resource-Constrained Environments\n",
    "- **Best Choice:** Linear SVM (99.10% accuracy with minimal resources)\n",
    "- **Alternatives:** Logistic Regression (98.99% accuracy)\n",
    "- **Considerations:** Orders of magnitude faster inference and smaller model size\n",
    "\n",
    "### For Balanced Performance and Efficiency\n",
    "- **Best Choice:** TinyBERT (99.91% accuracy with moderate resources)\n",
    "- **Alternatives:** MobileBERT (99.96% accuracy with slightly higher resource usage)\n",
    "- **Considerations:** Good compromise between accuracy and computational requirements\n",
    "\n",
    "### For Mobile/Edge Deployment\n",
    "- **Best Choice:** Linear SVM for extreme constraints, TinyBERT for better accuracy\n",
    "- **Alternatives:** MobileBERT if memory is not severely limited\n",
    "- **Considerations:** Inference time and model size are critical factors\n",
    "\n",
    "### For Rapid Development and Iteration\n",
    "- **Best Choice:** Naive Bayes (fastest training time)\n",
    "- **Alternatives:** Linear SVM (good balance of speed and accuracy)\n",
    "- **Considerations:** Useful for quick prototyping and testing\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "This comparative analysis reveals several key insights:\n",
    "\n",
    "1. **Performance Gap:** While transformer models achieve slightly higher accuracy (99.9-100%) compared to traditional ML models (96.6-99.1%), the gap is relatively small for this particular dataset and task.\n",
    "\n",
    "2. **Efficiency Difference:** Traditional ML models are dramatically more efficient in terms of:\n",
    "   - Training time: 10-300x faster\n",
    "   - Inference time: 1000-30000x faster\n",
    "   - Model size: 300-2500x smaller\n",
    "   - Memory usage: 5-10x lower\n",
    "\n",
    "3. **Optimal Trade-offs:**\n",
    "   - TinyBERT offers the best balance between high accuracy (99.91%) and reasonable resource requirements\n",
    "   - Linear SVM provides excellent accuracy (99.10%) with minimal computational demands\n",
    "   - RoBERTa achieves perfect accuracy (100%) but requires substantial resources\n",
    "\n",
    "4. **Deployment Considerations:**\n",
    "   - For server-based applications with available GPU resources, transformer models are viable\n",
    "   - For edge devices or applications with strict latency requirements, traditional ML models are preferable\n",
    "   - For mobile applications, TinyBERT or MobileBERT offer good compromises\n",
    "\n",
    "This analysis demonstrates that the choice of model should be guided by the specific requirements and constraints of the application, rather than simply selecting the model with the highest accuracy. For many real-world fake news detection applications, traditional ML models may be sufficient and significantly more practical to deploy."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
