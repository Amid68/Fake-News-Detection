{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ac988a",
   "metadata": {},
   "source": [
    "# Fake News Dataset Analysis: Understanding Linguistic Patterns for Transformer Model Comparison\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook documents the detailed analysis of linguistic patterns that distinguish real from fake news in the ISOT dataset. This analysis is crucial for understanding how different transformer models learn to detect deceptive content and for evaluating their effectiveness at capturing linguistic and stylistic differences. By identifying these patterns, we can better interpret model performance and provide context for comparing lightweight pretrained models like DistilBERT, TinyBERT, MobileBERT, and RoBERTa on fake news detection tasks.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "### Library Imports\n",
    "\n",
    "First, I import the necessary libraries for data manipulation, visualization, and text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8447e",
   "metadata": {},
   "source": [
    "I've selected these libraries for the following reasons:\n",
    "- `pandas` and `numpy` for efficient data manipulation and numerical operations\n",
    "- `matplotlib` and `seaborn` for creating informative visualizations\n",
    "- `re` for regular expression pattern matching in text analysis\n",
    "- `Counter` for frequency analysis of words and patterns\n",
    "- `WordCloud` for visualizing common terms in an intuitive way\n",
    "- `CountVectorizer` and `TfidfVectorizer` for text feature extraction\n",
    "- `nltk` for natural language processing tasks like tokenization and stopword removal\n",
    "- `sklearn.model_selection` for data splitting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a2e0",
   "metadata": {},
   "source": [
    "I configure visualization settings to ensure clear and readable plots, and set pandas display options to show more comprehensive information during analysis.\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "Now I load the datasets, checking first for previously cleaned versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    # Try to load datasets with basic cleaning already applied\n",
    "    true_news = pd.read_csv('./datasets/true_news_basic_cleaned.csv')\n",
    "    fake_news = pd.read_csv('./datasets/fake_news_basic_cleaned.csv')\n",
    "    print(\"Loaded previously cleaned datasets\")\n",
    "except:\n",
    "    # If not available, load raw data\n",
    "    print(\"Loading raw datasets\")\n",
    "    true_news = pd.read_csv('../data/ISOT/True.csv')\n",
    "    fake_news = pd.read_csv('../data/ISOT/Fake.csv')\n",
    "    \n",
    "    # Add labels\n",
    "    true_news['label'] = 'Real'\n",
    "    fake_news['label'] = 'Fake'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c57754",
   "metadata": {},
   "source": [
    "This approach is efficient because:\n",
    "1. It attempts to use previously cleaned data if available, saving processing time\n",
    "2. It falls back to loading raw data if needed\n",
    "3. It ensures consistent labeling of the data for analysis\n",
    "\n",
    "## 2. Critical Dataset Bias Identification\n",
    "\n",
    "Before proceeding with linguistic analysis, I investigate potential biases in the dataset that could affect model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f93c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Reuters pattern in true news\n",
    "reuters_count = true_news['text'].str.contains('\\(Reuters\\)').sum()\n",
    "print(f\"Number of true news articles containing '(Reuters)': {reuters_count}\")\n",
    "print(f\"Percentage: {reuters_count / len(true_news) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fake news contains this pattern\n",
    "fake_reuters_count = fake_news['text'].str.contains('\\(Reuters\\)').sum()\n",
    "print(f\"Number of fake news articles containing '(Reuters)': {fake_reuters_count}\")\n",
    "print(f\"Percentage: {fake_reuters_count / len(fake_news) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905f9dc",
   "metadata": {},
   "source": [
    "This analysis reveals a critical finding: 99.21% of true news articles contain the \"(Reuters)\" pattern, while only 0.04% of fake news articles do. This pattern creates a significant bias that would allow models to \"cheat\" by using this marker rather than learning substantive differences between real and fake news content.\n",
    "\n",
    "This finding is crucial for our comparative evaluation of transformer models because:\n",
    "1. Models might learn to rely on this simple pattern rather than deeper linguistic features\n",
    "2. This would lead to poor generalization when applied to news from other sources\n",
    "3. It would make model comparison less meaningful, as all models might simply learn this shortcut\n",
    "\n",
    "For a fair comparison of transformer models on this dataset, we need to remove these dataset-specific markers.\n",
    "\n",
    "## 3. Enhanced Data Cleaning\n",
    "\n",
    "Based on the identified biases, I implement an enhanced cleaning function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddde756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced cleaning function\n",
    "def enhanced_clean_text(text, is_true_news=True):\n",
    "    \"\"\"\n",
    "    Enhanced cleaning to remove bias-inducing patterns while preserving legitimate signals\n",
    "    \n",
    "    Args:\n",
    "        text: Text to clean\n",
    "        is_true_news: Whether the text is from true news (affects which patterns are removed)\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned_text = text\n",
    "    \n",
    "    if is_true_news:\n",
    "        # For true news, remove Reuters tag but preserve location\n",
    "        cleaned_text = re.sub(r'(\\b[A-Z]+(?:\\s[A-Z]+)*)\\s*\\(Reuters\\)', r'\\1', cleaned_text)\n",
    "        \n",
    "        # Remove other potentially biasing source markers specific to true news\n",
    "        for source in ['(SPD)', '(FDP)', '(AfD)', '(CDU)', '(SDF)', '(KRG)', '(NAFTA)', '(PKK)']:\n",
    "            cleaned_text = re.sub(re.escape(source), '', cleaned_text)\n",
    "    else:\n",
    "        # For fake news, remove patterns like (ACR) that are specific to fake news\n",
    "        for source in ['(ACR)', '(s)', '(id)', '(a)', '(R)', '(D)']:\n",
    "            cleaned_text = re.sub(re.escape(source), '', cleaned_text)\n",
    "            \n",
    "        # Remove links that are common in fake news\n",
    "        cleaned_text = re.sub(r'https?://\\S+', '', cleaned_text)\n",
    "        \n",
    "        # Remove specific phrases highly associated with fake news sources\n",
    "        fake_phrases = [\n",
    "            'Tune in to the Alternate Current Radio',\n",
    "            '21st Century Wire',\n",
    "            'Featured Image'\n",
    "        ]\n",
    "        for phrase in fake_phrases:\n",
    "            cleaned_text = cleaned_text.replace(phrase, '')\n",
    "    \n",
    "    # Common cleaning for both types\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef67c60",
   "metadata": {},
   "source": [
    "This cleaning function is carefully designed to:\n",
    "1. Remove dataset-specific patterns that could create shortcuts for models\n",
    "2. Preserve legitimate stylistic differences between real and fake news\n",
    "3. Handle different types of artifacts in each dataset\n",
    "4. Apply appropriate cleaning based on the source type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enhanced cleaning\n",
    "true_news['enhanced_cleaned_text'] = true_news['text'].apply(lambda x: enhanced_clean_text(x, is_true_news=True))\n",
    "fake_news['enhanced_cleaned_text'] = fake_news['text'].apply(lambda x: enhanced_clean_text(x, is_true_news=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93e095",
   "metadata": {},
   "source": [
    "By carefully removing dataset-specific patterns while preserving legitimate stylistic differences, we ensure that our transformer models learn to distinguish fake news based on meaningful content rather than artifacts. This cleaning approach is essential for a fair comparison of model architectures.\n",
    "\n",
    "## 4. News Content Analysis\n",
    "\n",
    "After cleaning the data, I analyze different aspects of news content to understand what linguistic patterns transformer models need to learn for effective fake news detection.\n",
    "\n",
    "### 4.1 Text Structure Analysis\n",
    "\n",
    "I begin by analyzing basic structural features of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11262710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text length metrics\n",
    "true_news['text_length'] = true_news['enhanced_cleaned_text'].apply(len)\n",
    "fake_news['text_length'] = fake_news['enhanced_cleaned_text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count metrics\n",
    "true_news['word_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: len(str(x).split()))\n",
    "fake_news['word_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average word length\n",
    "true_news['avg_word_length'] = true_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")\n",
    "fake_news['avg_word_length'] = fake_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence structure metrics\n",
    "true_news['sentence_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: len(re.findall(r'[.!?]+', str(x))) + 1)\n",
    "fake_news['sentence_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: len(re.findall(r'[.!?]+', str(x))) + 1)\n",
    "true_news['avg_sentence_length'] = true_news['word_count'] / true_news['sentence_count']\n",
    "fake_news['avg_sentence_length'] = fake_news['word_count'] / fake_news['sentence_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3717c53",
   "metadata": {},
   "source": [
    "These metrics capture different aspects of text structure:\n",
    "- Text length (in characters) measures overall content volume\n",
    "- Word count provides a measure of information density\n",
    "- Average word length indicates vocabulary complexity\n",
    "- Sentence count and average sentence length reflect syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text structure statistics\n",
    "print(\"Average word length in Real News: {:.2f}\".format(true_news['avg_word_length'].mean()))\n",
    "print(\"Average word length in Fake News: {:.2f}\".format(fake_news['avg_word_length'].mean()))\n",
    "print(\"Average sentence length in Real News: {:.2f} words\".format(true_news['avg_sentence_length'].mean()))\n",
    "print(\"Average sentence length in Fake News: {:.2f} words\".format(fake_news['avg_sentence_length'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf117a2f",
   "metadata": {},
   "source": [
    "The results show that real news tends to use longer words (5.16 characters on average) compared to fake news (4.81 characters), suggesting more technical or sophisticated vocabulary. However, sentence lengths are comparable (17.98 words for real news vs. 17.78 for fake news).\n",
    "\n",
    "This analysis informs our understanding of what transformer models need to detect: they must be sensitive to vocabulary sophistication rather than just sentence complexity. The attention mechanisms in transformer architectures should allow them to focus on word choice patterns while processing the text.\n",
    "\n",
    "### 4.2 Citation Patterns Analysis\n",
    "\n",
    "Next, I analyze how sources are cited in real versus fake news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic citation features\n",
    "true_news['said_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count(' said '))\n",
    "fake_news['said_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count(' said '))\n",
    "true_news['told_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count(' told '))\n",
    "fake_news['told_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count(' told '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional citation features\n",
    "true_news['according_to_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count('according to'))\n",
    "fake_news['according_to_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).lower().count('according to'))\n",
    "true_news['quote_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('\"'))\n",
    "fake_news['quote_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c421ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize citation counts by text length\n",
    "true_news['said_per_1000_words'] = true_news['said_count'] * 1000 / true_news['word_count']\n",
    "fake_news['said_per_1000_words'] = fake_news['said_count'] * 1000 / fake_news['word_count']\n",
    "true_news['quotes_per_1000_words'] = true_news['quote_count'] * 1000 / true_news['word_count']\n",
    "fake_news['quotes_per_1000_words'] = fake_news['quote_count'] * 1000 / fake_news['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a702d41",
   "metadata": {},
   "source": [
    "I normalize these counts by text length to ensure fair comparison between articles of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea752270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare citation patterns\n",
    "print(\"\\nCitation patterns per 1000 words:\")\n",
    "print(\"'Said' in Real News: {:.2f}\".format(true_news['said_per_1000_words'].mean()))\n",
    "print(\"'Said' in Fake News: {:.2f}\".format(fake_news['said_per_1000_words'].mean()))\n",
    "print(\"Quotes in Real News: {:.2f}\".format(true_news['quotes_per_1000_words'].mean()))\n",
    "print(\"Quotes in Fake News: {:.2f}\".format(fake_news['quotes_per_1000_words'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874e582",
   "metadata": {},
   "source": [
    "The analysis reveals a striking difference: real news articles contain approximately 10.16 mentions of 'said' per 1000 words compared to only 2.20 in fake news. This reflects journalistic conventions of attribution and source citation that are more common in legitimate reporting.\n",
    "\n",
    "This finding is particularly relevant for transformer models because:\n",
    "1. The self-attention mechanism should be able to detect these attribution patterns\n",
    "2. Models can learn to associate higher frequencies of attribution verbs with real news\n",
    "3. This pattern represents a legitimate stylistic difference rather than a dataset artifact\n",
    "\n",
    "### 4.3 Emotional Language Analysis\n",
    "\n",
    "I analyze the use of emotional language and emphatic punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emotional language metrics\n",
    "emotional_words = ['believe', 'think', 'feel', 'opinion', 'incredible', 'amazing', 'terrible', 'horrible']\n",
    "true_news['emotional_word_count'] = true_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in emotional_words)\n",
    ")\n",
    "fake_news['emotional_word_count'] = fake_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in emotional_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize emotional language by text length\n",
    "true_news['emotional_per_1000_words'] = true_news['emotional_word_count'] * 1000 / true_news['word_count']\n",
    "fake_news['emotional_per_1000_words'] = fake_news['emotional_word_count'] * 1000 / fake_news['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate punctuation features\n",
    "true_news['question_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('?'))\n",
    "fake_news['question_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('?'))\n",
    "true_news['exclamation_count'] = true_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('!'))\n",
    "fake_news['exclamation_count'] = fake_news['enhanced_cleaned_text'].apply(lambda x: str(x).count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d751ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize punctuation by text length\n",
    "true_news['question_exclamation_per_1000_words'] = (true_news['question_count'] + true_news['exclamation_count']) * 1000 / true_news['word_count']\n",
    "fake_news['question_exclamation_per_1000_words'] = (fake_news['question_count'] + fake_news['exclamation_count']) * 1000 / fake_news['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare emotional tone metrics\n",
    "print(\"\\nEmotional language per 1000 words:\")\n",
    "print(\"Emotional words in Real News: {:.2f}\".format(true_news['emotional_per_1000_words'].mean()))\n",
    "print(\"Emotional words in Fake News: {:.2f}\".format(fake_news['emotional_per_1000_words'].mean()))\n",
    "print(\"Question/exclamation marks in Real News: {:.2f}\".format(true_news['question_exclamation_per_1000_words'].mean()))\n",
    "print(\"Question/exclamation marks in Fake News: {:.2f}\".format(fake_news['question_exclamation_per_1000_words'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d08b30",
   "metadata": {},
   "source": [
    "The results show that fake news uses significantly more emotional language (1.75 emotional words per 1000 words vs. 0.86 in real news) and dramatically more emphatic punctuation (7.14 question/exclamation marks per 1000 words vs. 0.40 in real news).\n",
    "\n",
    "This finding is important for transformer models because:\n",
    "1. The emotional tone difference is a key distinguishing feature between real and fake news\n",
    "2. Transformer attention mechanisms should be able to detect these patterns in context\n",
    "3. The models need to recognize when news content is attempting to evoke emotional responses rather than inform\n",
    "\n",
    "### 4.4 Voice and Perspective Analysis\n",
    "\n",
    "I analyze the use of pronouns to understand differences in narrative voice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0409d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pronoun categories\n",
    "first_person = ['i', 'we', 'our', 'us', 'my']\n",
    "second_person = ['you', 'your', 'yours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb98cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pronoun counts\n",
    "true_news['first_person_count'] = true_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in first_person)\n",
    ")\n",
    "fake_news['first_person_count'] = fake_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in first_person)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news['second_person_count'] = true_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in second_person)\n",
    ")\n",
    "fake_news['second_person_count'] = fake_news['enhanced_cleaned_text'].apply(\n",
    "    lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in second_person)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fdcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pronoun usage by text length\n",
    "true_news['first_person_per_1000_words'] = true_news['first_person_count'] * 1000 / true_news['word_count']\n",
    "fake_news['first_person_per_1000_words'] = fake_news['first_person_count'] * 1000 / fake_news['word_count']\n",
    "true_news['second_person_per_1000_words'] = true_news['second_person_count'] * 1000 / true_news['word_count']\n",
    "fake_news['second_person_per_1000_words'] = fake_news['second_person_count'] * 1000 / fake_news['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare voice and perspective metrics\n",
    "print(\"\\nVoice and perspective per 1000 words:\")\n",
    "print(\"First-person pronouns in Real News: {:.2f}\".format(true_news['first_person_per_1000_words'].mean()))\n",
    "print(\"First-person pronouns in Fake News: {:.2f}\".format(fake_news['first_person_per_1000_words'].mean()))\n",
    "print(\"Second-person pronouns in Real News: {:.2f}\".format(true_news['second_person_per_1000_words'].mean()))\n",
    "print(\"Second-person pronouns in Fake News: {:.2f}\".format(fake_news['second_person_per_1000_words'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab17cd1",
   "metadata": {},
   "source": [
    "The analysis reveals that fake news uses significantly more first-person (12.00 vs. 4.78 per 1000 words) and second-person pronouns (5.00 vs. 0.55 per 1000 words) compared to real news.\n",
    "\n",
    "This difference in narrative voice is important for transformer models to detect because:\n",
    "1. Real news tends to maintain an objective third-person stance\n",
    "2. Fake news often directly addresses the reader or includes the author's perspective\n",
    "3. This difference in voice can be a reliable signal of journalistic standards\n",
    "\n",
    "### 4.5 Policy Coverage Analysis\n",
    "\n",
    "I analyze how different policy areas are covered in real versus fake news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define policy areas and related terms\n",
    "policy_areas = {\n",
    "    'economy': ['economy', 'economic', 'tax', 'budget', 'deficit', 'gdp', 'inflation', 'unemployment', 'jobs', 'trade'],\n",
    "    'healthcare': ['healthcare', 'health', 'obamacare', 'insurance', 'hospital', 'medical', 'medicare', 'medicaid'],\n",
    "    'immigration': ['immigration', 'immigrant', 'border', 'refugee', 'asylum', 'visa', 'deportation'],\n",
    "    'foreign_policy': ['foreign', 'diplomatic', 'embassy', 'sanctions', 'treaty', 'international', 'relations'],\n",
    "    'environment': ['environment', 'climate', 'pollution', 'emissions', 'epa', 'warming', 'renewable', 'carbon']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1187ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate policy term frequencies\n",
    "for area, terms in policy_areas.items():\n",
    "    true_news[f'{area}_count'] = true_news['enhanced_cleaned_text'].apply(\n",
    "        lambda x: sum(str(x).lower().count(' ' + term + ' ') for term in terms)\n",
    "    )\n",
    "    fake_news[f'{area}_count'] = fake_news['enhanced_cleaned_text'].apply(\n",
    "        lambda x: sum(str(x).lower().count(' ' + term + ' ') for term in terms)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c175224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize policy term frequencies by text length\n",
    "for area in policy_areas:\n",
    "    true_news[f'{area}_per_1000_words'] = true_news[f'{area}_count'] * 1000 / true_news['word_count']\n",
    "    fake_news[f'{area}_per_1000_words'] = fake_news[f'{area}_count'] * 1000 / fake_news['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0572975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a policy comparison dataframe\n",
    "policy_comparison = pd.DataFrame({\n",
    "    'Policy Area': list(policy_areas.keys()),\n",
    "    'Real News': [true_news[f'{area}_per_1000_words'].mean() for area in policy_areas],\n",
    "    'Fake News': [fake_news[f'{area}_per_1000_words'].mean() for area in policy_areas]\n",
    "})\n",
    "\n",
    "print(\"\\nPolicy coverage per 1000 words:\")\n",
    "print(policy_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c716a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy coverage\n",
    "plt.figure(figsize=(12, 6))\n",
    "policy_comparison.plot(x='Policy Area', y=['Real News', 'Fake News'], kind='bar', figsize=(12, 6))\n",
    "plt.title('Policy Area Coverage in Real vs. Fake News')\n",
    "plt.ylabel('Average Mentions per 1000 Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('policy_coverage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64cbac",
   "metadata": {},
   "source": [
    "The analysis shows that real news contains substantially more coverage of substantive policy areas, particularly economy (2.74 vs. 0.77), foreign policy (2.95 vs. 0.66), and healthcare (1.22 vs. 0.71).\n",
    "\n",
    "This finding is significant for transformer models because:\n",
    "1. The depth of policy discussion is a key indicator of journalistic quality\n",
    "2. Models need to detect both the presence and depth of these policy discussions\n",
    "3. The attention mechanism in transformers should allow them to recognize when articles focus on substantive issues versus personality-driven content\n",
    "\n",
    "## 5. Word Choice and Vocabulary Analysis\n",
    "\n",
    "I analyze the vocabulary differences between real and fake news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465dc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define word cloud creation function\n",
    "def create_wordcloud(text_series, title):\n",
    "    \"\"\"Create and save wordcloud from text series\"\"\"\n",
    "    all_text = ' '.join([str(text) for text in text_series])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, contour_width=3).generate(all_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.lower().replace(' ', '_')}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ceec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for visual comparison\n",
    "create_wordcloud(true_news['enhanced_cleaned_text'], 'Real News Word Cloud')\n",
    "create_wordcloud(fake_news['enhanced_cleaned_text'], 'Fake News Word Cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80bd29",
   "metadata": {},
   "source": [
    "The word clouds provide a visual representation of the most common terms in each dataset, making it easier to identify thematic differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common words extraction function\n",
    "def get_common_words(texts, n=20, min_length=3):\n",
    "    \"\"\"Get most common words in a list of texts\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            # Tokenize, convert to lowercase, remove punctuation and stopwords\n",
    "            words_in_text = [word.lower().strip(string.punctuation) for word in nltk.word_tokenize(text)]\n",
    "            words_in_text = [word for word in words_in_text if word not in stop_words and len(word) >= min_length and word.isalpha()]\n",
    "            words.extend(words_in_text)\n",
    "    \n",
    "    return Counter(words).most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ec82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get common words for both datasets\n",
    "true_common_words = get_common_words(true_news['enhanced_cleaned_text'])\n",
    "fake_common_words = get_common_words(fake_news['enhanced_cleaned_text'])\n",
    "\n",
    "print(\"Most common words in Real News:\")\n",
    "print(true_common_words)\n",
    "\n",
    "print(\"\\nMost common words in Fake News:\")\n",
    "print(fake_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ca336",
   "metadata": {},
   "source": [
    "The vocabulary analysis shows that:\n",
    "1. Real news focuses more on institutional terms (government, states, party)\n",
    "2. Fake news emphasizes personalities (Trump, Clinton, Obama)\n",
    "3. Both mention \"said\" frequently, but it's much more common in real news\n",
    "4. Fake news uses more informal language and includes terms like \"media\" and \"via\"\n",
    "\n",
    "Transformer models will need to be sensitive to these content focus differences, detecting when news emphasizes personalities over substance. The contextual embeddings in transformer architectures should allow them to understand these words in their proper context.\n",
    "\n",
    "## 6. TF-IDF Analysis of Distinctive Terms\n",
    "\n",
    "Finally, I use TF-IDF analysis to identify the most distinctive terms in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TF-IDF vectorization function\n",
    "def create_tfidf_vectors(true_texts, fake_texts, max_features=5000):\n",
    "    \"\"\"Create TF-IDF vectors for text classification\"\"\"\n",
    "    # Combine texts for fitting the vectorizer\n",
    "    all_texts = pd.concat([true_texts, fake_texts])\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Fit and transform all texts\n",
    "    all_vectors = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Split vectors back into true and fake\n",
    "    true_vectors = all_vectors[:len(true_texts)]\n",
    "    fake_vectors = all_vectors[len(true_texts):]\n",
    "    \n",
    "    return vectorizer, true_vectors, fake_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54513d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectors\n",
    "vectorizer, true_vectors, fake_vectors = create_tfidf_vectors(\n",
    "    true_news['enhanced_cleaned_text'],\n",
    "    fake_news['enhanced_cleaned_text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top features (words/phrases) by their IDF scores\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "feature_idf = sorted(zip(feature_names, idf_scores), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nTop 10 most common terms (lowest IDF):\")\n",
    "for feature, score in feature_idf[:10]:\n",
    "    print(f\"- {feature}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 most distinctive terms (highest IDF):\")\n",
    "for feature, score in feature_idf[-10:]:\n",
    "    print(f\"- {feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325741d",
   "metadata": {},
   "source": [
    "The TF-IDF analysis identifies both common terms across all articles and distinctive terms that appear in few documents. This helps us understand what features might be most informative for classification.\n",
    "\n",
    "## 7. Implications for Transformer Model Comparison\n",
    "\n",
    "Based on this linguistic analysis, I can draw several conclusions about what transformer models need to detect for effective fake news classification:\n",
    "\n",
    "1. **Attribution Patterns**: Real news uses more source attribution (\"said\", \"told\") than fake news. Transformer attention mechanisms should be able to detect these patterns.\n",
    "\n",
    "2. **Emotional Language**: Fake news uses more emotional language and emphatic punctuation. Models need to recognize when content is attempting to evoke emotional responses rather than inform.\n",
    "\n",
    "3. **Narrative Voice**: Real news maintains an objective third-person stance, while fake news uses more first-person and second-person pronouns. This difference in voice is a reliable signal of journalistic standards.\n",
    "\n",
    "4. **Policy Coverage**: Real news contains more substantive policy discussions across multiple domains. Models need to detect both the presence and depth of these policy discussions.\n",
    "\n",
    "5. **Vocabulary Differences**: Real news focuses on institutional terms, while fake news emphasizes personalities. The contextual embeddings in transformer architectures should allow them to understand these words in their proper context.\n",
    "\n",
    "These findings provide a framework for interpreting the performance of different transformer models in our comparative evaluation. Models that effectively capture these linguistic patterns should perform better at fake news detection.\n",
    "\n",
    "## 8. Data Preparation for Model Training\n",
    "\n",
    "Based on this analysis, I prepare the data for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99beaa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for splitting\n",
    "combined_df = pd.concat([\n",
    "    true_news[['title', 'enhanced_cleaned_text', 'label']],\n",
    "    fake_news[['title', 'enhanced_cleaned_text', 'label']]\n",
    "])\n",
    "\n",
    "# Convert labels to numeric\n",
    "combined_df['label'] = combined_df['label'].map({'Real': 1, 'Fake': 0})\n",
    "\n",
    "# Split into train, validation, and test sets (70/15/15)\n",
    "train_df, temp_df = train_test_split(combined_df, test_size=0.3, random_state=42, stratify=combined_df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "# Save the datasets\n",
    "train_df.to_csv('./datasets/train_fake_news.csv', index=False)\n",
    "val_df.to_csv('./datasets/val_fake_news.csv', index=False)\n",
    "test_df.to_csv('./datasets/test_fake_news.csv', index=False)\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Validation set: {val_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a5261",
   "metadata": {},
   "source": [
    "This data preparation process:\n",
    "1. Combines the cleaned real and fake news datasets\n",
    "2. Converts labels to numeric format (0 for fake, 1 for real)\n",
    "3. Splits the data into training (70%), validation (15%), and test (15%) sets\n",
    "4. Ensures stratification to maintain the same class distribution in all splits\n",
    "5. Saves the prepared datasets for use in model training\n",
    "\n",
    "The resulting datasets will be used in subsequent notebooks to train and evaluate different transformer models for fake news detection.\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "This analysis has provided deep insights into the linguistic patterns that distinguish real from fake news in the ISOT dataset. By identifying these patterns, we can better understand what features transformer models need to learn for effective fake news detection.\n",
    "\n",
    "The key findings include:\n",
    "1. Real news follows journalistic conventions like attribution and objective reporting\n",
    "2. Fake news uses more emotional language and direct reader address\n",
    "3. Real news contains more substantive policy discussions\n",
    "4. Vocabulary and focus differ significantly between the two types\n",
    "\n",
    "These insights will inform our interpretation of model performance in the comparative evaluation of lightweight pretrained transformer models. By understanding what linguistic patterns each model needs to capture, we can better assess their strengths and limitations for fake news detection."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
