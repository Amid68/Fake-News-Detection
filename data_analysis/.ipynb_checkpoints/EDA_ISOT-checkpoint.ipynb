{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c446e0",
   "metadata": {},
   "source": [
    "# ISOT Fake News Dataset - Exploratory Data Analysis\n",
    "\n",
    "This notebook focuses on exploratory data analysis of the ISOT Fake News Dataset, which contains real news from Reuters.com and fake news from various unreliable sources. My goal is to understand the data characteristics and identify potential biases that might impact model training.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, I'll import the necessary libraries and load our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3264180",
   "metadata": {},
   "source": [
    "Now I'll load both datasets and take a quick look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e636ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "true_news = pd.read_csv('True.csv')\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Display basic info about the datasets\n",
    "print(\"True News Dataset Shape:\", true_news.shape)\n",
    "print(\"Fake News Dataset Shape:\", fake_news.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532ffd2",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration\n",
    "\n",
    "I'll examine both datasets to understand their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807eba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"True News Dataset Sample:\")\n",
    "true_news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fake News Dataset Sample:\")\n",
    "fake_news.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951dffe",
   "metadata": {},
   "source": [
    "Let's check the columns in each dataset to ensure they have similar structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc48d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns in each dataset\n",
    "print(\"True News Columns:\", true_news.columns.tolist())\n",
    "print(\"Fake News Columns:\", fake_news.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in True News:\")\n",
    "print(true_news.isnull().sum())\n",
    "print(\"\\nMissing Values in Fake News:\")\n",
    "print(fake_news.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4867d5c",
   "metadata": {},
   "source": [
    "## 3. Identifying the \"(Reuters)\" Pattern\n",
    "\n",
    "I suspect that true news articles contain a specific pattern \"(Reuters)\" that might lead to model overfitting. Let's investigate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for \"(Reuters)\" in the text of true news articles\n",
    "reuters_count = true_news['text'].str.contains('\\(Reuters\\)').sum()\n",
    "print(f\"Number of true news articles containing '(Reuters)': {reuters_count}\")\n",
    "print(f\"Percentage: {reuters_count / len(true_news) * 100:.2f}%\")\n",
    "\n",
    "# Let's see some examples\n",
    "print(\"\\nSample of true news beginning:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nArticle {i+1} beginning:\")\n",
    "    print(true_news['text'].iloc[i][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949621af",
   "metadata": {},
   "source": [
    "Let's also check if fake news ever contains this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1767f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fake news articles contain \"(Reuters)\"\n",
    "fake_reuters_count = fake_news['text'].str.contains('\\(Reuters\\)').sum()\n",
    "print(f\"Number of fake news articles containing '(Reuters)': {fake_reuters_count}\")\n",
    "print(f\"Percentage: {fake_reuters_count / len(fake_news) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8cbce",
   "metadata": {},
   "source": [
    "I'm checking for the \"(Reuters)\" pattern because if all true news articles contain this pattern and fake news doesn't, our model might learn to classify articles based on this pattern alone rather than learning the actual substantive differences. This would lead to poor generalization when applied to new data without this specific marker.\n",
    "\n",
    "## 4. Exploring Other Potential Patterns or Biases\n",
    "\n",
    "Now I'll look for other patterns or markers that might create similar biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for common prefixes/suffixes in the text\n",
    "def check_common_patterns(series, n=20, prefix_length=30):\n",
    "    \"\"\"\n",
    "    Check for common patterns at the beginning and end of texts\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas series containing text data\n",
    "        n: Number of most common patterns to return\n",
    "        prefix_length: Length of prefix/suffix to check\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with common prefixes and suffixes\n",
    "    \"\"\"\n",
    "    prefixes = Counter([text[:prefix_length].strip() for text in series if isinstance(text, str) and len(text) > prefix_length])\n",
    "    suffixes = Counter([text[-prefix_length:].strip() for text in series if isinstance(text, str) and len(text) > prefix_length])\n",
    "    \n",
    "    return {\n",
    "        'prefixes': prefixes.most_common(n),\n",
    "        'suffixes': suffixes.most_common(n)\n",
    "    }\n",
    "\n",
    "# Check patterns in true news\n",
    "print(\"Common patterns in True News:\")\n",
    "true_patterns = check_common_patterns(true_news['text'])\n",
    "for prefix, count in true_patterns['prefixes'][:5]:\n",
    "    print(f\"Prefix: '{prefix}' - Count: {count}\")\n",
    "\n",
    "# Check patterns in fake news\n",
    "print(\"\\nCommon patterns in Fake News:\")\n",
    "fake_patterns = check_common_patterns(fake_news['text'])\n",
    "for prefix, count in fake_patterns['prefixes'][:5]:\n",
    "    print(f\"Prefix: '{prefix}' - Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c91aa0",
   "metadata": {},
   "source": [
    "Let's analyze location patterns in true news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze location patterns in true news\n",
    "def extract_locations(texts):\n",
    "    \"\"\"Extract location datelines from the beginning of articles\"\"\"\n",
    "    locations = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            # Look for capitalized words at the beginning followed by Reuters\n",
    "            match = re.match(r'^([A-Z]+(?:\\s[A-Z]+)*)\\s*\\(Reuters\\)', text)\n",
    "            if match:\n",
    "                locations.append(match.group(1))\n",
    "    return Counter(locations)\n",
    "\n",
    "true_locations = extract_locations(true_news['text'])\n",
    "print(\"Most common locations in True News:\")\n",
    "print(true_locations.most_common(10))\n",
    "\n",
    "# Check if fake news has similar location patterns\n",
    "fake_locations_pattern = fake_news['text'].str.match(r'^([A-Z]+(?:\\s[A-Z]+)*)\\s*\\(').sum()\n",
    "print(f\"Fake news articles with apparent location datelines: {fake_locations_pattern}\")\n",
    "print(f\"Percentage: {fake_locations_pattern / len(fake_news) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba59493",
   "metadata": {},
   "source": [
    "Let's also look at common sources mentioned in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90aa06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract potential source patterns\n",
    "def extract_sources(text):\n",
    "    \"\"\"Extract potential source identifiers from text\"\"\"\n",
    "    # Look for patterns like \"(Reuters)\", \"(CNN)\", etc.\n",
    "    sources = re.findall(r'\\([A-Za-z]+\\)', text)\n",
    "    return sources\n",
    "\n",
    "# Apply to both datasets\n",
    "true_sources = []\n",
    "for text in true_news['text']:\n",
    "    if isinstance(text, str):\n",
    "        true_sources.extend(extract_sources(text))\n",
    "\n",
    "fake_sources = []\n",
    "for text in fake_news['text']:\n",
    "    if isinstance(text, str):\n",
    "        fake_sources.extend(extract_sources(text))\n",
    "\n",
    "# Count and display the most common sources\n",
    "print(\"Most common sources in True News:\")\n",
    "print(Counter(true_sources).most_common(10))\n",
    "\n",
    "print(\"\\nMost common sources in Fake News:\")\n",
    "print(Counter(fake_sources).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306e993",
   "metadata": {},
   "source": [
    "## 5. Text Length Analysis\n",
    "\n",
    "I'll analyze the text length distribution for both real and fake news to identify any significant differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length as a feature\n",
    "true_news['text_length'] = true_news['text'].apply(lambda x: len(str(x)))\n",
    "fake_news['text_length'] = fake_news['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Create a combined dataset for visualization\n",
    "true_news['label'] = 'Real'\n",
    "fake_news['label'] = 'Fake'\n",
    "combined_df = pd.concat([true_news[['text_length', 'label']], fake_news[['text_length', 'label']]], axis=0)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=combined_df, x='text_length', hue='label', bins=50, kde=True, alpha=0.6)\n",
    "plt.title('Distribution of Text Length for Real and Fake News')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, combined_df['text_length'].quantile(0.99))  # Limit to 99th percentile for better visualization\n",
    "plt.savefig('text_length_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Text Length Statistics:\")\n",
    "print(combined_df.groupby('label')['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435387e",
   "metadata": {},
   "source": [
    "Analyzing text length is important because significant differences between real and fake news could become a feature that the model relies on too heavily. For instance, if fake news articles are consistently shorter, the model might classify short articles as fake regardless of content.\n",
    "\n",
    "## 6. Basic Content Cleaning\n",
    "\n",
    "Now I'll create a basic cleaning function to remove the \"(Reuters)\" pattern and any other identified markers that might bias our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text, patterns_to_remove=None):\n",
    "    \"\"\"\n",
    "    Clean text by removing specified patterns\n",
    "    \n",
    "    Args:\n",
    "        text: Text to clean\n",
    "        patterns_to_remove: List of regex patterns to remove\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned_text = text\n",
    "    \n",
    "    if patterns_to_remove:\n",
    "        for pattern in patterns_to_remove:\n",
    "            cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Patterns to remove from true news\n",
    "true_patterns_to_remove = [\n",
    "    r'\\(Reuters\\)',  # Remove (Reuters)\n",
    "    # Add other patterns identified in the exploration\n",
    "]\n",
    "\n",
    "# Patterns to remove from fake news\n",
    "fake_patterns_to_remove = [\n",
    "    # Add patterns identified in the exploration\n",
    "]\n",
    "\n",
    "# Apply cleaning\n",
    "true_news['cleaned_text'] = true_news['text'].apply(lambda x: clean_text(x, true_patterns_to_remove))\n",
    "fake_news['cleaned_text'] = fake_news['text'].apply(lambda x: clean_text(x, fake_patterns_to_remove))\n",
    "\n",
    "# Verify cleaning worked\n",
    "print(\"Sample of cleaned true news:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal text beginning: {true_news['text'].iloc[i][:100]}\")\n",
    "    print(f\"Cleaned text beginning: {true_news['cleaned_text'].iloc[i][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbdd75",
   "metadata": {},
   "source": [
    "I'm removing these patterns because they could create artificial signals that the model might latch onto during training. By removing them, I'm forcing the model to learn the actual stylistic and content differences between real and fake news rather than relying on specific markers.\n",
    "\n",
    "## 7. Content Analysis\n",
    "\n",
    "Let's analyze the actual content differences between real and fake news using word frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb27feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get most common words\n",
    "def get_common_words(texts, n=20, min_length=3):\n",
    "    \"\"\"\n",
    "    Get most common words in a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        n: Number of most common words to return\n",
    "        min_length: Minimum word length to consider\n",
    "    \n",
    "    Returns:\n",
    "        Counter object with most common words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            # Tokenize, convert to lowercase, remove punctuation and stopwords\n",
    "            words_in_text = [word.lower().strip(string.punctuation) for word in nltk.word_tokenize(text)]\n",
    "            words_in_text = [word for word in words_in_text if word not in stop_words and len(word) >= min_length and word.isalpha()]\n",
    "            words.extend(words_in_text)\n",
    "    \n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Get common words for both datasets\n",
    "true_common_words = get_common_words(true_news['cleaned_text'])\n",
    "fake_common_words = get_common_words(fake_news['cleaned_text'])\n",
    "\n",
    "print(\"Most common words in True News:\")\n",
    "print(true_common_words)\n",
    "\n",
    "print(\"\\nMost common words in Fake News:\")\n",
    "print(fake_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa9138",
   "metadata": {},
   "source": [
    "Let's analyze which words are disproportionately common in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze word ratio between datasets\n",
    "def word_ratio_analysis(true_words, fake_words, min_count=1000):\n",
    "    \"\"\"\n",
    "    Analyze the ratio of word frequencies between fake and real news\n",
    "    \n",
    "    Args:\n",
    "        true_words: Counter object with word counts from true news\n",
    "        fake_words: Counter object with word counts from fake news\n",
    "        min_count: Minimum count for a word to be considered\n",
    "        \n",
    "    Returns:\n",
    "        DataFrames with words more common in fake and true news respectively\n",
    "    \"\"\"\n",
    "    # Convert counters to dictionaries\n",
    "    true_dict = dict(true_words)\n",
    "    fake_dict = dict(fake_words)\n",
    "    \n",
    "    # Get all words\n",
    "    all_words = set(list(true_dict.keys()) + list(fake_dict.keys()))\n",
    "    \n",
    "    # Calculate ratios\n",
    "    word_ratios = []\n",
    "    for word in all_words:\n",
    "        true_count = true_dict.get(word, 0)\n",
    "        fake_count = fake_dict.get(word, 0)\n",
    "        \n",
    "        # Only consider words with sufficient frequency\n",
    "        if true_count + fake_count >= min_count:\n",
    "            # Add small value to avoid division by zero\n",
    "            fake_true_ratio = (fake_count + 0.1) / (true_count + 0.1)\n",
    "            true_fake_ratio = (true_count + 0.1) / (fake_count + 0.1)\n",
    "            \n",
    "            word_ratios.append({\n",
    "                'word': word,\n",
    "                'true_count': true_count,\n",
    "                'fake_count': fake_count,\n",
    "                'fake_true_ratio': fake_true_ratio,\n",
    "                'true_fake_ratio': true_fake_ratio\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    df = pd.DataFrame(word_ratios)\n",
    "    more_in_fake = df.sort_values('fake_true_ratio', ascending=False).head(20)\n",
    "    more_in_true = df.sort_values('true_fake_ratio', ascending=False).head(20)\n",
    "    \n",
    "    return more_in_fake, more_in_true\n",
    "\n",
    "# Get words that are disproportionately common in each dataset\n",
    "more_in_fake, more_in_true = word_ratio_analysis(\n",
    "    dict(true_common_words), \n",
    "    dict(fake_common_words),\n",
    "    min_count=1000\n",
    ")\n",
    "\n",
    "print(\"Words much more common in fake news:\")\n",
    "print(more_in_fake[['word', 'fake_count', 'true_count', 'fake_true_ratio']].head(10))\n",
    "\n",
    "print(\"\\nWords much more common in true news:\")\n",
    "print(more_in_true[['word', 'true_count', 'fake_count', 'true_fake_ratio']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f3bf5",
   "metadata": {},
   "source": [
    "Let's create word clouds to visualize the most common words in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds\n",
    "def create_wordcloud(text_series, title):\n",
    "    \"\"\"Create and save wordcloud from text series\"\"\"\n",
    "    all_text = ' '.join([str(text) for text in text_series])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, contour_width=3).generate(all_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.lower().replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "create_wordcloud(true_news['cleaned_text'], 'True News Word Cloud')\n",
    "create_wordcloud(fake_news['cleaned_text'], 'Fake News Word Cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96a561",
   "metadata": {},
   "source": [
    "## 8. Topic Analysis\n",
    "\n",
    "Let's try to identify the main topics in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ca27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple topic analysis using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def extract_topics(texts, n_topics=5, n_words=10):\n",
    "    \"\"\"\n",
    "    Extract topics from texts using NMF\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        n_topics: Number of topics to extract\n",
    "        n_words: Number of words per topic to display\n",
    "    \n",
    "    Returns:\n",
    "        List of topics (each topic is a list of words)\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Run NMF\n",
    "    nmf = NMF(n_components=n_topics, random_state=42)\n",
    "    nmf.fit(tfidf)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Extract topics\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_features_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_idx]\n",
    "        topics.append(top_features)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Extract topics from true and fake news\n",
    "true_topics = extract_topics([text for text in true_news['cleaned_text'] if isinstance(text, str)])\n",
    "fake_topics = extract_topics([text for text in fake_news['cleaned_text'] if isinstance(text, str)])\n",
    "\n",
    "print(\"Topics in True News:\")\n",
    "for i, topic in enumerate(true_topics):\n",
    "    print(f\"Topic {i+1}: {', '.join(topic)}\")\n",
    "\n",
    "print(\"\\nTopics in Fake News:\")\n",
    "for i, topic in enumerate(fake_topics):\n",
    "    print(f\"Topic {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07efc2",
   "metadata": {},
   "source": [
    "Let's analyze policy area coverage in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8765848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define policy areas and related terms\n",
    "policy_areas = {\n",
    "    'economy': ['economy', 'economic', 'tax', 'budget', 'deficit', 'gdp', 'inflation', 'unemployment', 'jobs', 'trade'],\n",
    "    'healthcare': ['healthcare', 'health', 'obamacare', 'insurance', 'hospital', 'medical', 'medicare', 'medicaid'],\n",
    "    'immigration': ['immigration', 'immigrant', 'border', 'refugee', 'asylum', 'visa', 'deportation'],\n",
    "    'foreign_policy': ['foreign', 'diplomatic', 'embassy', 'sanctions', 'treaty', 'international', 'relations'],\n",
    "    'environment': ['environment', 'climate', 'pollution', 'emissions', 'epa', 'warming', 'renewable', 'carbon']\n",
    "}\n",
    "\n",
    "# Function to count policy terms\n",
    "def count_policy_terms(texts, terms_dict):\n",
    "    \"\"\"Count occurrences of terms related to different policy areas\"\"\"\n",
    "    results = {area: 0 for area in terms_dict.keys()}\n",
    "    \n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        lowercase_text = text.lower()\n",
    "        for area, terms in terms_dict.items():\n",
    "            for term in terms:\n",
    "                results[area] += lowercase_text.count(term)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Count terms in both datasets\n",
    "true_policy_counts = count_policy_terms(true_news['cleaned_text'], policy_areas)\n",
    "fake_policy_counts = count_policy_terms(fake_news['cleaned_text'], policy_areas)\n",
    "\n",
    "# Calculate per-document averages\n",
    "true_per_doc = {area: count / len(true_news) for area, count in true_policy_counts.items()}\n",
    "fake_per_doc = {area: count / len(fake_news) for area, count in fake_policy_counts.items()}\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "policy_df = pd.DataFrame({\n",
    "    'Policy Area': list(policy_areas.keys()),\n",
    "    'True News (per doc)': list(true_per_doc.values()),\n",
    "    'Fake News (per doc)': list(fake_per_doc.values())\n",
    "})\n",
    "\n",
    "# Calculate ratio of fake to true\n",
    "policy_df['Fake/True Ratio'] = policy_df['Fake News (per doc)'] / policy_df['True News (per doc)']\n",
    "\n",
    "print(\"Policy area coverage comparison:\")\n",
    "print(policy_df)\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "policy_df.plot(x='Policy Area', y=['True News (per doc)', 'Fake News (per doc)'], kind='bar', figsize=(12, 6))\n",
    "plt.title('Policy Area Coverage in Real vs. Fake News')\n",
    "plt.ylabel('Average Mentions per Document')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('policy_coverage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfd68f",
   "metadata": {},
   "source": [
    "## 9. Citation Analysis\n",
    "\n",
    "Let's examine how sources are cited in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e939408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze citation patterns\n",
    "def analyze_citations(texts):\n",
    "    \"\"\"Analyze how sources are cited in articles\"\"\"\n",
    "    said_patterns = [\n",
    "        r'\"([^\"]+)\" said',\n",
    "        r\"'([^']+)' said\",\n",
    "        r'said ([A-Z][a-z]+ [A-Z][a-z]+)',\n",
    "        r'according to ([^,.]+)'\n",
    "    ]\n",
    "    \n",
    "    citations = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "        \n",
    "        for pattern in said_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            citations.extend(matches)\n",
    "    \n",
    "    return Counter(citations).most_common(20)\n",
    "\n",
    "# Analyze citation patterns\n",
    "true_citations = analyze_citations(true_news['cleaned_text'])\n",
    "fake_citations = analyze_citations(fake_news['cleaned_text'])\n",
    "\n",
    "print(\"Most common citation patterns in True News:\")\n",
    "for citation, count in true_citations[:5]:\n",
    "    print(f\"- '{citation}': {count} occurrences\")\n",
    "\n",
    "print(\"\\nMost common citation patterns in Fake News:\")\n",
    "for citation, count in fake_citations[:5]:\n",
    "    print(f\"- '{citation}': {count} occurrences\")\n",
    "\n",
    "# Analyze citation frequency\n",
    "def count_citation_phrases(texts):\n",
    "    \"\"\"Count occurrences of common citation phrases\"\"\"\n",
    "    citation_phrases = ['said', 'told', 'according to', 'reported', 'stated', 'announced', 'claimed']\n",
    "    \n",
    "    results = {phrase: 0 for phrase in citation_phrases}\n",
    "    total_words = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "            \n",
    "        words = text.lower().split()\n",
    "        total_words += len(words)\n",
    "        \n",
    "        for phrase in citation_phrases:\n",
    "            if ' ' in phrase:\n",
    "                results[phrase] += text.lower().count(phrase)\n",
    "            else:\n",
    "                results[phrase] += words.count(phrase)\n",
    "    \n",
    "    # Calculate per 1000 words\n",
    "    for phrase in results:\n",
    "        results[phrase] = results[phrase] * 1000 / total_words if total_words > 0 else 0\n",
    "        \n",
    "    return results\n",
    "\n",
    "true_citation_freq = count_citation_phrases(true_news['cleaned_text'])\n",
    "fake_citation_freq = count_citation_phrases(fake_news['cleaned_text'])\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "citation_df = pd.DataFrame({\n",
    "    'Citation Phrase': list(true_citation_freq.keys()),\n",
    "    'True News (per 1000 words)': list(true_citation_freq.values()),\n",
    "    'Fake News (per 1000 words)': list(fake_citation_freq.values())\n",
    "})\n",
    "\n",
    "citation_df['Ratio (True/Fake)'] = citation_df['True News (per 1000 words)'] / citation_df['Fake News (per 1000 words)']\n",
    "\n",
    "print(\"\\nCitation phrase frequency comparison (per 1000 words):\")\n",
    "print(citation_df)\n",
    "\n",
    "# Visualize the citation frequency comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "citation_df.plot(x='Citation Phrase', y=['True News (per 1000 words)', 'Fake News (per 1000 words)'], \n",
    "                kind='bar', figsize=(12, 6))\n",
    "plt.title('Citation Phrase Frequency in Real vs. Fake News')\n",
    "plt.ylabel('Occurrences per 1000 words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('citation_frequency.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf131116",
   "metadata": {},
   "source": [
    "## 10. Summary of Findings and Next Steps\n",
    "\n",
    "Based on my exploratory data analysis, here are the key findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of findings\n",
    "findings = [\n",
    "    \"99.21% of real news articles contain '(Reuters)', making it a strong bias signal\",\n",
    "    \"Real news articles typically begin with a location dateline (e.g., 'WASHINGTON') followed by '(Reuters)'\",\n",
    "    \"Text length distributions differ between real and fake news, but not dramatically\",\n",
    "    \"Vocabulary usage shows meaningful differences: real news uses more formal institutional language while fake news is more personality-focused\",\n",
    "    \"Real news has more citations and source attributions than fake news\",\n",
    "    \"Real news provides more substantive policy coverage across most policy areas\",\n",
    "    \"Topic analysis shows different focuses: real news focuses on formal reporting while fake news leans toward political personalities\"\n",
    "]\n",
    "\n",
    "print(\"Key Findings from Exploratory Data Analysis:\")\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f652af",
   "metadata": {},
   "source": [
    "In the next notebook, I'll focus on:\n",
    "\n",
    "1. Enhanced data cleaning to remove identified biases\n",
    "2. Feature engineering to capture legitimate stylistic and content differences\n",
    "3. Preparing the datasets for model training\n",
    "\n",
    "This initial analysis has given us a solid understanding of the characteristics and potential biases in our datasets. By addressing these issues, we can build more robust models that truly learn to distinguish between real and fake news based on substantive differences rather than dataset-specific artifacts."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
