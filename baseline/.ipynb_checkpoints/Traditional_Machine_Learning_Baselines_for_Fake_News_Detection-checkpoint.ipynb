{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc13617d",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Baselines for Fake News Detection\n",
    "\n",
    "In this notebook, I'll implement and evaluate traditional machine learning approaches for fake news detection on the ISOT dataset. These baseline models will provide important reference points for comparing with the more complex transformer-based models (DistilBERT, RoBERTa, TinyBERT, and MobileBERT) explored in other notebooks.\n",
    "\n",
    "## 1. Setup and Library Installation\n",
    "\n",
    "First, I'll import the necessary libraries and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda7590",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193a8fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6f9a9",
   "metadata": {},
   "source": [
    "    [nltk_data] Downloading package stopwords to /root/nltk_data...\n",
    "    [nltk_data]   Package stopwords is already up-to-date!\n",
    "    [nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "    [nltk_data]   Package punkt is already up-to-date!\n",
    "    [nltk_data] Downloading package wordnet to /root/nltk_data...\n",
    "    [nltk_data]   Package wordnet is already up-to-date!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    True\n",
    "\n",
    "\n",
    "\n",
    "## 2. Set Random Seeds for Reproducibility\n",
    "\n",
    "Setting random seeds ensures that our experiments are reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e77ce",
   "metadata": {},
   "source": [
    "## 3. Load and Explore the Dataset\n",
    "\n",
    "I'll load the same ISOT dataset used for the transformer models to ensure a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/train_fake_news.csv')\n",
    "    val_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/val_fake_news.csv') \n",
    "    test_df = pd.read_csv('/kaggle/input/isot-processed-and-splitted/test_fake_news.csv')\n",
    "    \n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed files not found. Using sample data for demonstration.\")\n",
    "    # Create sample data for demonstration\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    data = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "    df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    print(f\"Training set: {train_df.shape}\")\n",
    "    print(f\"Validation set: {val_df.shape}\")\n",
    "    print(f\"Test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3baf6c",
   "metadata": {},
   "source": [
    "    Training set: (31428, 3)\n",
    "    Validation set: (6735, 3)\n",
    "    Test set: (6735, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample of training data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23237e6",
   "metadata": {},
   "source": [
    "    Sample of training data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436a362",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>title</th>\n",
    "      <th>enhanced_cleaned_text</th>\n",
    "      <th>label</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Trump 'Diversity Council' Member Threatens to ...</td>\n",
    "      <td>A member of President Trump s Diversity Counci...</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>DID BEYONCE AND JAY Z's \"Vacation\" To Communis...</td>\n",
    "      <td>Notorious radical Black Panther and NJ cop kil...</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>CNN Host Calls Out Trump's Uncle Tom Spokeswo...</td>\n",
    "      <td>Katrina Pierson is a black woman. She is also ...</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff994d",
   "metadata": {},
   "source": [
    "Let's check the class distribution to ensure our dataset is balanced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad67455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution in training set:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(val_df['label'].value_counts())\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc328e",
   "metadata": {},
   "source": [
    "    Class distribution in training set:\n",
    "    0    16428\n",
    "    1    15000\n",
    "    Name: label, dtype: int64\n",
    "    \n",
    "    Class distribution in validation set:\n",
    "    0    3523\n",
    "    1    3212\n",
    "    Name: label, dtype: int64\n",
    "    \n",
    "    Class distribution in test set:\n",
    "    0    3523\n",
    "    1    3212\n",
    "    Name: label, dtype: int64\n",
    "\n",
    "\n",
    "The dataset is reasonably balanced, with a slight majority of fake news (label 0) compared to real news (label 1).\n",
    "\n",
    "## 4. Text Preprocessing\n",
    "\n",
    "I'll create a text preprocessing function to clean and normalize the text data. This step is crucial for traditional ML models that rely on feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation\n",
    "    3. Removing numbers\n",
    "    4. Removing stopwords\n",
    "    5. Lemmatizing words\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddfda5",
   "metadata": {},
   "source": [
    "Now, I'll apply this preprocessing function to our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a880080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text for a more comprehensive analysis\n",
    "train_df['combined_text'] = train_df['title'] + \" \" + train_df['enhanced_cleaned_text']\n",
    "val_df['combined_text'] = val_df['title'] + \" \" + val_df['enhanced_cleaned_text']\n",
    "test_df['combined_text'] = test_df['title'] + \" \" + test_df['enhanced_cleaned_text']\n",
    "\n",
    "# Apply preprocessing to the combined text\n",
    "print(\"Preprocessing text data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply preprocessing to a sample first to estimate time\n",
    "sample_size = min(1000, len(train_df))\n",
    "time_per_sample = time.time() - start_time\n",
    "\n",
    "# Estimate total time\n",
    "estimated_time = (len(train_df) + len(val_df) + len(test_df)) * time_per_sample / sample_size\n",
    "print(f\"Estimated preprocessing time: {estimated_time:.2f} seconds\")\n",
    "\n",
    "# Process the data\n",
    "train_df['processed_text'] = train_df['combined_text'].apply(preprocess_text)\n",
    "val_df['processed_text'] = val_df['combined_text'].apply(preprocess_text)\n",
    "test_df['processed_text'] = test_df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "preprocessing_time = time.time() - start_time\n",
    "print(f\"Preprocessing completed in {preprocessing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4ca69",
   "metadata": {},
   "source": [
    "    Preprocessing text data...\n",
    "    Estimated preprocessing time: 0.00 seconds\n",
    "    Preprocessing completed in 1249.37 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e3cfc",
   "metadata": {},
   "source": [
    "Let's examine a sample of the preprocessed text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37341ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of preprocessed text\n",
    "print(\"Original text:\")\n",
    "print(train_df['combined_text'].iloc[0][:300])\n",
    "print(\"\\nPreprocessed text:\")\n",
    "print(train_df['processed_text'].iloc[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e8a59",
   "metadata": {},
   "source": [
    "    Original text:\n",
    "    Trump 'Diversity Council' Member Threatens to Quit If Trump Ends DACAâ€¦Bye, Bye! [Video] A member of President Trump s Diversity Council is threatening to quit because he opposes Trump s cancelation of DACA. Bye Bye!Trump diversity council member tells @Acosta he may quit the council if Trump moves ahead to end DACA CNN Newsroom (@CNNnewsroom) September 4, 2017 I want to remind him and his team that from an economic standpoint, and again, we re business people if you look at this from a purely economic standpoint again, none of these young people gets government benefits of any sorts so they re not costing us anything. They pay over $2 billion in taxes Is anyone else out there sick of the American people being told illegals cost nothing?DACA Will Cost Americans And Their Government A Huge Amount of Money.On average, people with college degrees pay more in taxes than they receive in government benefits. People without a degree consume more taxes than they pay to federal, state and local tax officials.In 2013, a Heritage Foundation study showed that amnesty for 11 million illegals would spike federal spending by $6,300 billion over the next five decades. That is roughly equivalent to $550,000 per illegal, or $10,000 per illegal per year, much of which will be spent when the immigrant becomes eligible for Social Security and Medicare. That cost estimate does not include the extra costs created when immigrants use their new legal powers as a citizen to bring in more low-skilled migrants.If those 3 million DACA people and their parents soon become legal residents or citizens, then Obama s DACA will cost Americans roughly $1,700 billion over the next 50 years, according to Heritage Foundation s numbers.Moreover, th\n",
    "    \n",
    "    Preprocessed text:\n",
    "    trump diversity council member threatens quit trump ends daca bye bye video member president trump diversity council threatening quit opposes trump cancelation daca bye bye trump diversity council member tell acosta may quit council trump move ahead end daca cnn newsroom cnnnewsroom september want remind team economic standpoint business people look purely economic standpoint young people get government benefit sort costing pay billion tax anyone else sick american people told illegal cost nothing daca cost american government huge amount money average people college degree pay tax receive government benefit people without degree consume tax pay federal state local tax official heritage foundation study showed amnesty million illegal would spike federal spending billion next five decade roughly equivalent per illegal per illegal per year much spent immigrant become eligible social security medicare cost estimate include extra cost created immigrant use new legal power citizen bring skilled migrant million daca people parent soon become legal resident citizen obama daca cost american roughly billion next year according heritage foundation number moreover\n",
    "\n",
    "\n",
    "## 5. Feature Engineering with TF-IDF\n",
    "\n",
    "For traditional ML models, I'll use TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to convert text into numerical features. This approach is widely used in text classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TF-IDF vectorization parameters\n",
    "# I'm using a combination of unigrams and bigrams, with a maximum of 50,000 features\n",
    "# This balances feature richness with computational efficiency\n",
    "tfidf_params = {\n",
    "    'max_features': 50000,\n",
    "    'min_df': 5,  # Minimum document frequency\n",
    "    'max_df': 0.8,  # Maximum document frequency\n",
    "    'ngram_range': (1, 2),  # Use both unigrams and bigrams\n",
    "    'sublinear_tf': True  # Apply sublinear tf scaling (1 + log(tf))\n",
    "}\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "\n",
    "# Fit and transform the training data\n",
    "print(\"Generating TF-IDF features...\")\n",
    "start_time = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['processed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_df['processed_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['processed_text'])\n",
    "vectorization_time = time.time() - start_time\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(f\"TF-IDF vectorization completed in {vectorization_time:.2f} seconds\")\n",
    "print(f\"Number of features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Training set shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation set shape: {X_val_tfidf.shape}\")\n",
    "print(f\"Test set shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff6e9e",
   "metadata": {},
   "source": [
    "    Generating TF-IDF features...\n",
    "    TF-IDF vectorization completed in 12.47 seconds\n",
    "    Number of features: 50000\n",
    "    Training set shape: (31428, 50000)\n",
    "    Validation set shape: (6735, 50000)\n",
    "    Test set shape: (6735, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe650c65",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "\n",
    "Now I'll train and evaluate three traditional ML models:\n",
    "1. Logistic Regression\n",
    "2. Multinomial Naive Bayes\n",
    "3. Linear Support Vector Machine (SVM)\n",
    "\n",
    "### 6.1 Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "    'solver': ['liblinear', 'saga'],  # Solver algorithms\n",
    "    'max_iter': [100, 200]  # Maximum iterations\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    lr_model, \n",
    "    param_grid, \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1_weighted',  # Optimize for F1 score\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model\n",
    "lr_best = grid_search.best_estimator_\n",
    "lr_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Logistic Regression training completed in {lr_training_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f93ba",
   "metadata": {},
   "source": [
    "    Training Logistic Regression model...\n",
    "    Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
    "    Logistic Regression training completed in 183.45 seconds\n",
    "    Best parameters: {'C': 10.0, 'max_iter': 100, 'solver': 'liblinear'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3367333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression on validation set\n",
    "print(\"Evaluating Logistic Regression on validation set...\")\n",
    "start_time = time.time()\n",
    "lr_val_preds = lr_best.predict(X_val_tfidf)\n",
    "lr_val_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "lr_val_accuracy = accuracy_score(y_val, lr_val_preds)\n",
    "lr_val_precision, lr_val_recall, lr_val_f1, _ = precision_recall_fscore_support(\n",
    "    y_val, lr_val_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Validation inference time: {lr_val_time:.4f} seconds\")\n",
    "print(f\"Validation accuracy: {lr_val_accuracy:.4f}\")\n",
    "print(f\"Validation precision: {lr_val_precision:.4f}\")\n",
    "print(f\"Validation recall: {lr_val_recall:.4f}\")\n",
    "print(f\"Validation F1 score: {lr_val_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating Logistic Regression on test set...\")\n",
    "start_time = time.time()\n",
    "lr_test_preds = lr_best.predict(X_test_tfidf)\n",
    "lr_test_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_preds)\n",
    "lr_test_precision, lr_test_recall, lr_test_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, lr_test_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Test inference time: {lr_test_time:.4f} seconds\")\n",
    "print(f\"Test accuracy: {lr_test_accuracy:.4f}\")\n",
    "print(f\"Test precision: {lr_test_precision:.4f}\")\n",
    "print(f\"Test recall: {lr_test_recall:.4f}\")\n",
    "print(f\"Test F1 score: {lr_test_f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake News', 'Real News'],\n",
    "            yticklabels=['Fake News', 'Real News'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lr_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_test_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e78777",
   "metadata": {},
   "source": [
    "    Evaluating Logistic Regression on validation set...\n",
    "    Validation inference time: 0.0282 seconds\n",
    "    Validation accuracy: 0.9901\n",
    "    Validation precision: 0.9901\n",
    "    Validation recall: 0.9901\n",
    "    Validation F1 score: 0.9901\n",
    "    \n",
    "    Evaluating Logistic Regression on test set...\n",
    "    Test inference time: 0.0275 seconds\n",
    "    Test accuracy: 0.9899\n",
    "    Test precision: 0.9899\n",
    "    Test recall: 0.9899\n",
    "    Test F1 score: 0.9899\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "![png](output_21_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "       Fake News       0.99      0.99      0.99      3523\n",
    "       Real News       0.99      0.99      0.99      3212\n",
    "    \n",
    "        accuracy                           0.99      6735\n",
    "       macro avg       0.99      0.99      0.99      6735\n",
    "    weighted avg       0.99      0.99      0.99      6735\n",
    "    \n",
    "\n",
    "\n",
    "### 6.2 Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multinomial Naive Bayes model\n",
    "print(\"Training Multinomial Naive Bayes model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0],  # Smoothing parameter\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    nb_model, \n",
    "    param_grid, \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1_weighted',  # Optimize for F1 score\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model\n",
    "nb_best = grid_search.best_estimator_\n",
    "nb_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Naive Bayes training completed in {nb_training_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056a62e",
   "metadata": {},
   "source": [
    "    Training Multinomial Naive Bayes model...\n",
    "    Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
    "    Naive Bayes training completed in 9.42 seconds\n",
    "    Best parameters: {'alpha': 0.01}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8526193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes on validation set\n",
    "print(\"Evaluating Naive Bayes on validation set...\")\n",
    "start_time = time.time()\n",
    "nb_val_preds = nb_best.predict(X_val_tfidf)\n",
    "nb_val_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "nb_val_accuracy = accuracy_score(y_val, nb_val_preds)\n",
    "nb_val_precision, nb_val_recall, nb_val_f1, _ = precision_recall_fscore_support(\n",
    "    y_val, nb_val_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Validation inference time: {nb_val_time:.4f} seconds\")\n",
    "print(f\"Validation accuracy: {nb_val_accuracy:.4f}\")\n",
    "print(f\"Validation precision: {nb_val_precision:.4f}\")\n",
    "print(f\"Validation recall: {nb_val_recall:.4f}\")\n",
    "print(f\"Validation F1 score: {nb_val_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating Naive Bayes on test set...\")\n",
    "start_time = time.time()\n",
    "nb_test_preds = nb_best.predict(X_test_tfidf)\n",
    "nb_test_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "nb_test_accuracy = accuracy_score(y_test, nb_test_preds)\n",
    "nb_test_precision, nb_test_recall, nb_test_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, nb_test_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Test inference time: {nb_test_time:.4f} seconds\")\n",
    "print(f\"Test accuracy: {nb_test_accuracy:.4f}\")\n",
    "print(f\"Test precision: {nb_test_precision:.4f}\")\n",
    "print(f\"Test recall: {nb_test_recall:.4f}\")\n",
    "print(f\"Test F1 score: {nb_test_f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "nb_cm = confusion_matrix(y_test, nb_test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake News', 'Real News'],\n",
    "            yticklabels=['Fake News', 'Real News'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('nb_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, nb_test_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559b8b5",
   "metadata": {},
   "source": [
    "    Evaluating Naive Bayes on validation set...\n",
    "    Validation inference time: 0.0241 seconds\n",
    "    Validation accuracy: 0.9654\n",
    "    Validation precision: 0.9655\n",
    "    Validation recall: 0.9654\n",
    "    Validation F1 score: 0.9654\n",
    "    \n",
    "    Evaluating Naive Bayes on test set...\n",
    "    Test inference time: 0.0242 seconds\n",
    "    Test accuracy: 0.9661\n",
    "    Test precision: 0.9662\n",
    "    Test recall: 0.9661\n",
    "    Test F1 score: 0.9661\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "![png](output_24_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "       Fake News       0.97      0.97      0.97      3523\n",
    "       Real News       0.96      0.96      0.96      3212\n",
    "    \n",
    "        accuracy                           0.97      6735\n",
    "       macro avg       0.97      0.97      0.97      6735\n",
    "    weighted avg       0.97      0.97      0.97      6735\n",
    "    \n",
    "\n",
    "\n",
    "### 6.3 Linear Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear SVM model\n",
    "print(\"Training Linear SVM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "    'loss': ['hinge', 'squared_hinge'],  # Loss function\n",
    "    'max_iter': [1000]  # Maximum iterations\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    svm_model, \n",
    "    param_grid, \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='f1_weighted',  # Optimize for F1 score\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model\n",
    "svm_best = grid_search.best_estimator_\n",
    "svm_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Linear SVM training completed in {svm_training_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fe652",
   "metadata": {},
   "source": [
    "    Training Linear SVM model...\n",
    "    Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
    "    Linear SVM training completed in 94.37 seconds\n",
    "    Best parameters: {'C': 1.0, 'loss': 'squared_hinge', 'max_iter': 1000}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c235926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Linear SVM on validation set\n",
    "print(\"Evaluating Linear SVM on validation set...\")\n",
    "start_time = time.time()\n",
    "svm_val_preds = svm_best.predict(X_val_tfidf)\n",
    "svm_val_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "svm_val_accuracy = accuracy_score(y_val, svm_val_preds)\n",
    "svm_val_precision, svm_val_recall, svm_val_f1, _ = precision_recall_fscore_support(\n",
    "    y_val, svm_val_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Validation inference time: {svm_val_time:.4f} seconds\")\n",
    "print(f\"Validation accuracy: {svm_val_accuracy:.4f}\")\n",
    "print(f\"Validation precision: {svm_val_precision:.4f}\")\n",
    "print(f\"Validation recall: {svm_val_recall:.4f}\")\n",
    "print(f\"Validation F1 score: {svm_val_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating Linear SVM on test set...\")\n",
    "start_time = time.time()\n",
    "svm_test_preds = svm_best.predict(X_test_tfidf)\n",
    "svm_test_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "svm_test_accuracy = accuracy_score(y_test, svm_test_preds)\n",
    "svm_test_precision, svm_test_recall, svm_test_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, svm_test_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Test inference time: {svm_test_time:.4f} seconds\")\n",
    "print(f\"Test accuracy: {svm_test_accuracy:.4f}\")\n",
    "print(f\"Test precision: {svm_test_precision:.4f}\")\n",
    "print(f\"Test recall: {svm_test_recall:.4f}\")\n",
    "print(f\"Test F1 score: {svm_test_f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "svm_cm = confusion_matrix(y_test, svm_test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake News', 'Real News'],\n",
    "            yticklabels=['Fake News', 'Real News'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Linear SVM Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, svm_test_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef1227",
   "metadata": {},
   "source": [
    "    Evaluating Linear SVM on validation set...\n",
    "    Validation inference time: 0.0246 seconds\n",
    "    Validation accuracy: 0.9907\n",
    "    Validation precision: 0.9907\n",
    "    Validation recall: 0.9907\n",
    "    Validation F1 score: 0.9907\n",
    "    \n",
    "    Evaluating Linear SVM on test set...\n",
    "    Test inference time: 0.0246 seconds\n",
    "    Test accuracy: 0.9910\n",
    "    Test precision: 0.9910\n",
    "    Test recall: 0.9910\n",
    "    Test F1 score: 0.9910\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "![png](output_27_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "    \n",
    "       Fake News       0.99      0.99      0.99      3523\n",
    "       Real News       0.99      0.99      0.99      3212\n",
    "    \n",
    "        accuracy                           0.99      6735\n",
    "       macro avg       0.99      0.99      0.99      6735\n",
    "    weighted avg       0.99      0.99      0.99      6735\n",
    "    \n",
    "\n",
    "\n",
    "## 7. Feature Importance Analysis\n",
    "\n",
    "For the Logistic Regression model, I'll analyze the most important features (words or phrases) that influence the classification decision. This provides insights into what textual patterns the model is using to distinguish between fake and real news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1ead6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Get feature names from the TF-IDF vectorizer\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# For Logistic Regression, get the coefficients\n",
    "if hasattr(lr_best, 'coef_'):\n",
    "    # Get the coefficients for the positive class (real news)\n",
    "    coef = lr_best.coef_[0]\n",
    "    \n",
    "    # Get the top positive and negative coefficients\n",
    "    top_positive_idx = np.argsort(coef)[-20:]  # Top 20 features for real news\n",
    "    top_negative_idx = np.argsort(coef)[:20]   # Top 20 features for fake news\n",
    "    \n",
    "    # Get the corresponding feature names\n",
    "    top_positive_features = feature_names[top_positive_idx]\n",
    "    top_negative_features = feature_names[top_negative_idx]\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    top_features_df = pd.DataFrame({\n",
    "        'Feature': np.concatenate([top_positive_features, top_negative_features]),\n",
    "        'Coefficient': np.concatenate([coef[top_positive_idx], coef[top_negative_idx]]),\n",
    "        'Class': np.concatenate([['Real News']*20, ['Fake News']*20])\n",
    "    })\n",
    "    \n",
    "    # Plot the top features\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Coefficient', y='Feature', hue='Class', data=top_features_df)\n",
    "    plt.title('Top Features for Fake News Detection (Logistic Regression)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lr_feature_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top features for Real News:\")\n",
    "    for feature, coef_val in zip(top_positive_features, coef[top_positive_idx]):\n",
    "        print(f\"{feature}: {coef_val:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop features for Fake News:\")\n",
    "    for feature, coef_val in zip(top_negative_features, coef[top_negative_idx]):\n",
    "        print(f\"{feature}: {coef_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da7c59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b1c21a",
   "metadata": {},
   "source": [
    "![png](output_29_0.png)\n",
    "    \n",
    "\n",
    "\n",
    "    Top features for Real News:\n",
    "    reuters: 1.4635\n",
    "    said: 0.9989\n",
    "    percent: 0.7532\n",
    "    monday: 0.6639\n",
    "    tuesday: 0.6493\n",
    "    wednesday: 0.6326\n",
    "    thursday: 0.6268\n",
    "    friday: 0.6152\n",
    "    according: 0.5958\n",
    "    billion: 0.5923\n",
    "    million: 0.5893\n",
    "    company: 0.5761\n",
    "    bank: 0.5659\n",
    "    market: 0.5641\n",
    "    government: 0.5599\n",
    "    minister: 0.5594\n",
    "    year: 0.5584\n",
    "    told: 0.5572\n",
    "    sunday: 0.5559\n",
    "    saturday: 0.5538\n",
    "    \n",
    "    Top features for Fake News:\n",
    "    hillary: -0.8642\n",
    "    obama: -0.8187\n",
    "    trump: -0.7996\n",
    "    clinton: -0.7848\n",
    "    video: -0.6866\n",
    "    liberal: -0.6618\n",
    "    fake: -0.6518\n",
    "    democrat: -0.6494\n",
    "    media: -0.6401\n",
    "    cnn: -0.6349\n",
    "    american: -0.6322\n",
    "    conservative: -0.6313\n",
    "    president: -0.6302\n",
    "    news: -0.6287\n",
    "    people: -0.6243\n",
    "    america: -0.6217\n",
    "    donald: -0.6205\n",
    "    muslim: -0.6196\n",
    "    watch: -0.6180\n",
    "    facebook: -0.6179\n",
    "\n",
    "\n",
    "## 8. Analyze Misclassified Examples\n",
    "\n",
    "Let's examine some of the examples that were misclassified by the best-performing model (Linear SVM) to understand potential limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples from the SVM model\n",
    "misclassified_indices = np.where(svm_test_preds != y_test)[0]\n",
    "print(f\"Number of misclassified examples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Display a sample of misclassified examples\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Get a sample of misclassified examples (up to 5)\n",
    "    sample_size = min(5, len(misclassified_indices))\n",
    "    sample_indices = misclassified_indices[:sample_size]\n",
    "    \n",
    "    # Create a DataFrame for display\n",
    "    misclassified_df = pd.DataFrame({\n",
    "        'Title': test_df.iloc[sample_indices]['title'].values,\n",
    "        'Text Excerpt': [text[:200] + '...' for text in test_df.iloc[sample_indices]['enhanced_cleaned_text'].values],\n",
    "        'True Label': ['Real' if label == 1 else 'Fake' for label in y_test[sample_indices]],\n",
    "        'Predicted Label': ['Real' if label == 1 else 'Fake' for label in svm_test_preds[sample_indices]]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSample of misclassified examples:\")\n",
    "    display(misclassified_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cb99d",
   "metadata": {},
   "source": [
    "    Number of misclassified examples: 61\n",
    "    \n",
    "    Sample of misclassified examples:\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Title</th>\n",
    "      <th>Text Excerpt</th>\n",
    "      <th>True Label</th>\n",
    "      <th>Predicted Label</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Exclusive: Overruling diplomats, U.S. to drop ...</td>\n",
    "      <td>WASHINGTON (Reuters) - The United States plans to drop Iraq and Myanmar from a list of countries that recruit and use child soldiers, U.S. officials said, in a step back from using global human rights law to ...</td>\n",
    "      <td>Real</td>\n",
    "      <td>Fake</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Exclusive: Trump administration seeks to withd...</td>\n",
    "      <td>WASHINGTON (Reuters) - The Trump administration is preparing to submit to the U.S. Congress up to $25 billion in immediate spending cuts, including possible reductions to social safety net programs, lawmakers ...</td>\n",
    "      <td>Real</td>\n",
    "      <td>Fake</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Exclusive: Trump administration prepares to wi...</td>\n",
    "      <td>WASHINGTON (Reuters) - The Trump administration is preparing to submit to the U.S. Congress a legislative package to ease restrictions on the overseas sales of U.S.-made weapons, a U.S. official said on Wedne...</td>\n",
    "      <td>Real</td>\n",
    "      <td>Fake</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>Exclusive: Trump administration to announce fr...</td>\n",
    "      <td>WASHINGTON (Reuters) - The Trump administration is expected to announce a freeze on $1.9 billion in military aid to Pakistan, The New York Times reported on Friday, citing U.S. officials, a step the newspaper ...</td>\n",
    "      <td>Real</td>\n",
    "      <td>Fake</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>Exclusive: Trump to focus counter-extremism pr...</td>\n",
    "      <td>WASHINGTON (Reuters) - The Trump administration wants to revamp and rename a U.S. government program designed to counter all violent ideologies so that it focuses solely on Islamist extremism, five people fam...</td>\n",
    "      <td>Real</td>\n",
    "      <td>Fake</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15ad90",
   "metadata": {},
   "source": [
    "Interesting! Looking at the misclassified examples, I notice a pattern: many of them are real news articles about Trump administration policies that were misclassified as fake news. This suggests that the model may have learned to associate Trump-related content with fake news due to patterns in the training data. This highlights a potential bias in the model that would need to be addressed for real-world applications.\n",
    "\n",
    "## 9. Model Comparison and Resource Usage\n",
    "\n",
    "Let's compare the performance and resource usage of all three traditional ML models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "models = ['Logistic Regression', 'Naive Bayes', 'Linear SVM']\n",
    "accuracy = [lr_test_accuracy, nb_test_accuracy, svm_test_accuracy]\n",
    "f1_scores = [lr_test_f1, nb_test_f1, svm_test_f1]\n",
    "precision = [lr_test_precision, nb_test_precision, svm_test_precision]\n",
    "recall = [lr_test_recall, nb_test_recall, svm_test_recall]\n",
    "training_times = [lr_training_time, nb_training_time, svm_training_time]\n",
    "inference_times = [lr_test_time, nb_test_time, svm_test_time]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'Training Time (s)': training_times,\n",
    "    'Inference Time (s)': inference_times\n",
    "})\n",
    "\n",
    "print(\"Model Performance and Resource Usage Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Plot performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "for metric in metrics:\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, comparison_df[metric], width, label=metric)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x + width, models)\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.95, 1.0)  # Adjust y-axis to better show differences\n",
    "plt.tight_layout()\n",
    "plt.savefig('traditional_models_performance.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and inference times (log scale)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(models, comparison_df['Training Time (s)'], label='Training Time (s)')\n",
    "plt.bar(models, comparison_df['Inference Time (s)'], label='Inference Time (s)', alpha=0.5)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Time (seconds, log scale)')\n",
    "plt.title('Training and Inference Times')\n",
    "plt.yscale('log')  # Use log scale to show both training and inference times\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('traditional_models_time.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c2261",
   "metadata": {},
   "source": [
    "    Model Performance and Resource Usage Comparison:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e06b88",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Model</th>\n",
    "      <th>Accuracy</th>\n",
    "      <th>F1 Score</th>\n",
    "      <th>Precision</th>\n",
    "      <th>Recall</th>\n",
    "      <th>Training Time (s)</th>\n",
    "      <th>Inference Time (s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Logistic Regression</td>\n",
    "      <td>0.989901</td>\n",
    "      <td>0.989899</td>\n",
    "      <td>0.989901</td>\n",
    "      <td>0.989901</td>\n",
    "      <td>183.450000</td>\n",
    "      <td>0.027500</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Naive Bayes</td>\n",
    "      <td>0.966147</td>\n",
    "      <td>0.966121</td>\n",
    "      <td>0.966231</td>\n",
    "      <td>0.966147</td>\n",
    "      <td>9.420000</td>\n",
    "      <td>0.024200</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Linear SVM</td>\n",
    "      <td>0.990944</td>\n",
    "      <td>0.990943</td>\n",
    "      <td>0.990944</td>\n",
    "      <td>0.990944</td>\n",
    "      <td>94.370000</td>\n",
    "      <td>0.024600</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20cef05",
   "metadata": {},
   "source": [
    "![png](output_34_2.png)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ff5b0",
   "metadata": {},
   "source": [
    "![png](output_34_3.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330f65b",
   "metadata": {},
   "source": [
    "## 10. Save the Best Model\n",
    "\n",
    "Let's save the best-performing model (Linear SVM) for future use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ba1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (Linear SVM)\n",
    "best_model = svm_best\n",
    "joblib.dump(best_model, 'linear_svm_fake_news_detector.joblib')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "print(\"Best model and vectorizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3898a8",
   "metadata": {},
   "source": [
    "    Best model and vectorizer saved successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc764b2",
   "metadata": {},
   "source": [
    "## 11. Comparison with Transformer Models\n",
    "\n",
    "Now, let's compare our traditional ML models with the transformer models from previous notebooks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "all_models = [\n",
    "    'Logistic Regression', \n",
    "    'Naive Bayes', \n",
    "    'Linear SVM', \n",
    "    'DistilBERT', \n",
    "    'TinyBERT', \n",
    "    'RoBERTa', \n",
    "    'MobileBERT'\n",
    "]\n",
    "\n",
    "# Performance metrics\n",
    "all_accuracy = [\n",
    "    lr_test_accuracy, \n",
    "    nb_test_accuracy, \n",
    "    svm_test_accuracy,\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "all_f1 = [\n",
    "    lr_test_f1, \n",
    "    nb_test_f1, \n",
    "    svm_test_f1,\n",
    "    0.9996,  # DistilBERT\n",
    "    0.9991,  # TinyBERT\n",
    "    1.0000,  # RoBERTa\n",
    "    0.9996   # MobileBERT\n",
    "]\n",
    "\n",
    "# Training times (minutes)\n",
    "all_training_times = [\n",
    "    lr_training_time / 60,  # Convert to minutes\n",
    "    nb_training_time / 60,\n",
    "    svm_training_time / 60,\n",
    "    48.69,  # DistilBERT\n",
    "    8.99,   # TinyBERT\n",
    "    62.35,  # RoBERTa\n",
    "    39.18   # MobileBERT\n",
    "]\n",
    "\n",
    "# Inference times (ms per sample)\n",
    "all_inference_times = [\n",
    "    lr_test_time / len(y_test) * 1000,  # Convert to ms per sample\n",
    "    nb_test_time / len(y_test) * 1000,\n",
    "    svm_test_time / len(y_test) * 1000,\n",
    "    61.76,   # DistilBERT\n",
    "    17.08,   # TinyBERT\n",
    "    118.37,  # RoBERTa\n",
    "    113.50   # MobileBERT\n",
    "]\n",
    "\n",
    "# Model sizes (parameters)\n",
    "all_model_sizes = [\n",
    "    \"~50K\",    # Logistic Regression (depends on features)\n",
    "    \"~50K\",    # Naive Bayes (depends on features)\n",
    "    \"~50K\",    # Linear SVM (depends on features)\n",
    "    \"67M\",     # DistilBERT\n",
    "    \"15M\",     # TinyBERT\n",
    "    \"125M\",    # RoBERTa\n",
    "    \"25M\"      # MobileBERT\n",
    "]\n",
    "\n",
    "# Create the comparison DataFrame\n",
    "all_comparison_df = pd.DataFrame({\n",
    "    'Model': all_models,\n",
    "    'Accuracy': all_accuracy,\n",
    "    'F1 Score': all_f1,\n",
    "    'Training Time (min)': all_training_times,\n",
    "    'Inference Time (ms/sample)': all_inference_times,\n",
    "    'Model Size': all_model_sizes\n",
    "})\n",
    "\n",
    "# Add model type column\n",
    "model_types = ['Traditional ML'] * 3 + ['Transformer'] * 4\n",
    "all_comparison_df['Model Type'] = model_types\n",
    "\n",
    "print(\"Comprehensive Model Comparison:\")\n",
    "display(all_comparison_df)\n",
    "\n",
    "# Plot performance comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(all_models))\n",
    "\n",
    "plt.bar(index, all_comparison_df['Accuracy'], bar_width, label='Accuracy')\n",
    "plt.bar(index + bar_width, all_comparison_df['F1 Score'], bar_width, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Comparison: Traditional ML vs. Transformer Models')\n",
    "plt.xticks(index + bar_width/2, all_models, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_performance.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and inference times\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Training Time (min)', color=color)\n",
    "ax1.bar(index, all_comparison_df['Training Time (min)'], bar_width, color=color, label='Training Time')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Inference Time (ms/sample)', color=color)\n",
    "ax2.bar(index + bar_width, all_comparison_df['Inference Time (ms/sample)'], bar_width, color=color, label='Inference Time')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Resource Usage Comparison: Traditional ML vs. Transformer Models')\n",
    "plt.xticks(index + bar_width/2, all_models, rotation=45, ha='right')\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_resource_usage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ec1fa",
   "metadata": {},
   "source": [
    "    Comprehensive Model Comparison:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111a382",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Model</th>\n",
    "      <th>Accuracy</th>\n",
    "      <th>F1 Score</th>\n",
    "      <th>Training Time (min)</th>\n",
    "      <th>Inference Time (ms/sample)</th>\n",
    "      <th>Model Size</th>\n",
    "      <th>Model Type</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Logistic Regression</td>\n",
    "      <td>0.989901</td>\n",
    "      <td>0.989899</td>\n",
    "      <td>3.057500</td>\n",
    "      <td>0.004083</td>\n",
    "      <td>~50K</td>\n",
    "      <td>Traditional ML</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Naive Bayes</td>\n",
    "      <td>0.966147</td>\n",
    "      <td>0.966121</td>\n",
    "      <td>0.157000</td>\n",
    "      <td>0.003593</td>\n",
    "      <td>~50K</td>\n",
    "      <td>Traditional ML</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Linear SVM</td>\n",
    "      <td>0.990944</td>\n",
    "      <td>0.990943</td>\n",
    "      <td>1.572833</td>\n",
    "      <td>0.003653</td>\n",
    "      <td>~50K</td>\n",
    "      <td>Traditional ML</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>DistilBERT</td>\n",
    "      <td>0.999600</td>\n",
    "      <td>0.999600</td>\n",
    "      <td>48.690000</td>\n",
    "      <td>61.760000</td>\n",
    "      <td>67M</td>\n",
    "      <td>Transformer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>TinyBERT</td>\n",
    "      <td>0.999100</td>\n",
    "      <td>0.999100</td>\n",
    "      <td>8.990000</td>\n",
    "      <td>17.080000</td>\n",
    "      <td>15M</td>\n",
    "      <td>Transformer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>RoBERTa</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>62.350000</td>\n",
    "      <td>118.370000</td>\n",
    "      <td>125M</td>\n",
    "      <td>Transformer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>MobileBERT</td>\n",
    "      <td>0.999600</td>\n",
    "      <td>0.999600</td>\n",
    "      <td>39.180000</td>\n",
    "      <td>113.500000</td>\n",
    "      <td>25M</td>\n",
    "      <td>Transformer</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655291d",
   "metadata": {},
   "source": [
    "![png](output_38_2.png)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860be79f",
   "metadata": {},
   "source": [
    "![png](output_38_3.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e7c42",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Discussion\n",
    "\n",
    "In this notebook, I've implemented and evaluated three traditional machine learning approaches for fake news detection on the ISOT dataset:\n",
    "\n",
    "1. **Logistic Regression**: Achieved 98.99% accuracy and F1 score, with relatively fast training and inference times.\n",
    "2. **Naive Bayes**: Achieved 96.61% accuracy and F1 score, with the fastest training time but slightly lower performance.\n",
    "3. **Linear SVM**: Achieved 99.09% accuracy and F1 score, making it the best-performing traditional model.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Comparison**: \n",
    "   - Traditional ML models achieve impressive performance (96-99% accuracy), but transformer models still outperform them slightly (99.9-100% accuracy).\n",
    "   - The performance gap is relatively small, suggesting that for many applications, traditional models might be sufficient.\n",
    "\n",
    "2. **Resource Efficiency**:\n",
    "   - Traditional ML models are significantly more efficient in terms of:\n",
    "     - Training time: 10-300x faster than transformer models\n",
    "     - Inference time: 1000-30000x faster per sample\n",
    "     - Model size: ~50K parameters vs. 15M-125M parameters\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - The analysis of important features reveals interesting patterns:\n",
    "     - Real news is strongly associated with terms like \"reuters\", \"said\", \"percent\", and days of the week\n",
    "     - Fake news is associated with political terms like \"hillary\", \"obama\", \"trump\", and emotional/sensational terms like \"fake\", \"liberal\", \"conservative\"\n",
    "\n",
    "4. **Error Analysis**:\n",
    "   - Misclassified examples reveal potential biases in the model, particularly around political content\n",
    "   - Many real news articles about Trump administration policies were misclassified as fake news\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "1. **Model Selection Trade-offs**:\n",
    "   - For applications requiring maximum accuracy: Transformer models (especially RoBERTa)\n",
    "   - For applications with resource constraints: Traditional ML models (especially Linear SVM)\n",
    "   - For balanced performance and efficiency: TinyBERT offers a good compromise\n",
    "\n",
    "2. **Deployment Considerations**:\n",
    "   - Traditional ML models are much more suitable for edge devices or applications with strict latency requirements\n",
    "   - The TF-IDF + SVM approach could be deployed on mobile devices or low-power servers\n",
    "\n",
    "3. **Bias Mitigation**:\n",
    "   - The feature importance analysis and error analysis highlight potential biases that should be addressed before deployment\n",
    "   - More diverse training data or domain adaptation techniques might help reduce these biases\n",
    "\n",
    "This comprehensive evaluation demonstrates that while transformer models achieve slightly higher accuracy, traditional ML approaches remain competitive and offer significant advantages in terms of computational efficiency and resource requirements. The choice between these approaches should be guided by the specific requirements and constraints of the application.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
