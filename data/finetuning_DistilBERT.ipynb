{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6dU_D-igHSA"
      },
      "source": [
        "# Fake News Detection using DistilBERT\n",
        "\n",
        "This project implements a lightweight fake news detector using DistilBERT. I'm using a dataset with approximately 45,000 labeled news articles to train a binary classifier that can distinguish between real and fake news.\n",
        "\n",
        "## Objectives\n",
        "- Train a transformer-based model for reliable fake news detection\n",
        "- Evaluate model performance on standard metrics\n",
        "- Measure resource usage (memory and processing time)\n",
        "- Create code for integration with my Django web application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFN4-GaIgHSD"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installing required packages and importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-03T12:12:30.179012Z",
          "start_time": "2025-05-03T12:12:29.099850Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfo683zQgHSD",
        "outputId": "2e18f0ff-48be-44d5-d224-3288063ec679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets scikit-learn pandas numpy matplotlib seaborn psutil tqdm accelerate nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtCxf0ZgHSE"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-03T12:05:06.745211Z",
          "start_time": "2025-05-03T12:05:06.715128Z"
        },
        "id": "yvWpXAOdGWKz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import psutil\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-03T12:12:23.152077Z",
          "start_time": "2025-05-03T12:12:23.135006Z"
        },
        "id": "HFgKovXbGWKz"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug5lwjiJGWK0"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d-_IIyIGWK0"
      },
      "outputs": [],
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ON3GJNGgHSE"
      },
      "source": [
        "## Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoseaQBPgHSE"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    fake_news = pd.read_csv(\"/content/Fake.csv\")\n",
        "    real_news = pd.read_csv(\"/content/True.csv\")\n",
        "\n",
        "    print(f\"Fake news dataset shape: {fake_news.shape}\")\n",
        "    print(f\"Real news dataset shape: {real_news.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    raise SystemExit(f\"Critical data missing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mieNXt9agHSE"
      },
      "outputs": [],
      "source": [
        "# Add labels\n",
        "fake_news['label'] = 1\n",
        "real_news['label'] = 0\n",
        "\n",
        "# Fix: Correct splitting to prevent data leakage\n",
        "full_df = pd.concat([fake_news, real_news])\n",
        "\n",
        "full_df = full_df.drop_duplicates(subset=['title', 'text'])\n",
        "full_df = full_df.groupby('title').first().reset_index()\n",
        "\n",
        "full_df = full_df.sample(frac=1, random_state=SEED)\n",
        "\n",
        "train_val_df, test_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=full_df['label']\n",
        ")\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.15,\n",
        "    random_state=SEED,\n",
        "    stratify=train_val_df['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfNrvR_ogHSE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import psutil\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ousU1CpdAtIV"
      },
      "source": [
        "## Dataset Analysis\n",
        "\n",
        "The FakeNewsNet dataset contains:\n",
        "- 23,481 fake news articles\n",
        "- 21,417 real news articles\n",
        "\n",
        "Each article includes the title, full text, subject category, and publication date. I'm preprocessing this data by:\n",
        "1. Combining titles and article text\n",
        "2. Converting to lowercase\n",
        "3. Removing URLs and excessive whitespace\n",
        "4. Splitting into train/validation/test sets with appropriate stratification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and normalize text data\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "# Preprocess each split individually\n",
        "for df_split in [train_df, val_df, test_df]:\n",
        "    # Apply preprocessing\n",
        "    df_split['processed_text'] = df_split['title'].fillna('').apply(preprocess_text)\n",
        "    # Remove empty texts\n",
        "    empty_mask = df_split['processed_text'].apply(lambda x: len(x.strip()) == 0)\n",
        "    if empty_mask.sum() > 0:\n",
        "        print(f\"Removing {empty_mask.sum()} empty texts\")\n",
        "        df_split.drop(df_split[empty_mask].index, inplace=True)\n",
        "        df_split.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Update the combined df if needed for reference\n",
        "df = pd.concat([train_df, val_df, test_df])\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "oW3-wiLmWin7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imc6wzTAgHSF"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWNlV7lYgHSF"
      },
      "outputs": [],
      "source": [
        "# Inspect dataset columns to find title and text columns\n",
        "for col in df.columns:\n",
        "    if col != 'label':\n",
        "        print(f\"Column: {col}\")\n",
        "        print(f\"Example: {df[col].iloc[0][:100]}...\")\n",
        "        print(f\"Average length: {df[col].str.len().mean():.2f} characters\")\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6HMrx6mGWK1"
      },
      "outputs": [],
      "source": [
        "# Use only the title field for training\n",
        "df['processed_text'] = df['title'].fillna('').apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoN9zzwHGWK1"
      },
      "outputs": [],
      "source": [
        "# Check for empty texts after preprocessing\n",
        "empty_texts = df['processed_text'].apply(lambda x: len(x.strip()) == 0).sum()\n",
        "print(f\"Number of empty texts after preprocessing: {empty_texts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezmm0QWiGWK1"
      },
      "outputs": [],
      "source": [
        "# Remove empty texts if any\n",
        "if empty_texts > 0:\n",
        "    df = df[df['processed_text'].apply(lambda x: len(x.strip()) > 0)].reset_index(drop=True)\n",
        "    print(f\"Dataset size after removing empty texts: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwCVdc_fGWK1"
      },
      "outputs": [],
      "source": [
        "# Display a sample preprocessed text\n",
        "print(\"\\nSample processed title:\")\n",
        "print(df['processed_text'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkowR6aFgHSF"
      },
      "source": [
        "## Train/Val/Test Split\n",
        "\n",
        "Dividing the dataset into:\n",
        "- 30,524 training samples (68%)\n",
        "- 5,387 validation samples (12%)\n",
        "- 8,978 test samples (20%)\n",
        "\n",
        "Using stratification to ensure balanced class distribution across splits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before splitting:\n",
        "full_df = full_df.drop_duplicates(subset=['title', 'text'])  # Remove duplicates\n",
        "full_df = full_df.groupby('title').first().reset_index()  # Ensure unique titles"
      ],
      "metadata": {
        "id": "FFRVMNwVkup2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKa6c4JfgHSF"
      },
      "outputs": [],
      "source": [
        "# Split data into train, validation, and test sets\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=SEED, stratify=df['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9OLegwwGWK1"
      },
      "outputs": [],
      "source": [
        "# Then split train+val into train and validation\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df, test_size=0.15, random_state=SEED, stratify=train_val_df['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIxD5dEXGWK1"
      },
      "outputs": [],
      "source": [
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd1gH7ujGWK1"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights for balanced training\n",
        "# Get class distribution\n",
        "class_counts = df['label'].value_counts()\n",
        "print(\"Class distribution:\")\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Pf81OPGWK1"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "classes = np.unique(df['label'])\n",
        "weights = compute_class_weight('balanced', classes=classes, y=train_df['label'])\n",
        "class_weights = {i: weights[i] for i in range(len(weights))}\n",
        "print(\"Class weights:\")\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLtaFwsngHSF"
      },
      "source": [
        "## Create PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRXCymcMgHSF"
      },
      "outputs": [],
      "source": [
        "# Define model configuration\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 128  # Reduced from 512 as titles are much shorter\n",
        "BATCH_SIZE = 16   # Increased batch size since we're using shorter sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqSgVGGQgHSG"
      },
      "outputs": [],
      "source": [
        "# Replace the existing FakeNewsDataset class with this modified version\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Remove padding from tokenization\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGwBSwQGWK2"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymE_fO4LGWK2"
      },
      "source": [
        "### Create datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cll_HdQQGWK2"
      },
      "outputs": [],
      "source": [
        "train_dataset = FakeNewsDataset(\n",
        "    train_df['processed_text'].tolist(),\n",
        "    train_df['label'].tolist(),\n",
        "    tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_hnJ2EgGWK2"
      },
      "outputs": [],
      "source": [
        "val_dataset = FakeNewsDataset(\n",
        "    val_df['processed_text'].tolist(),\n",
        "    val_df['label'].tolist(),\n",
        "    tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bMugf-RGWK2"
      },
      "outputs": [],
      "source": [
        "test_dataset = FakeNewsDataset(\n",
        "    test_df['processed_text'].tolist(),\n",
        "    test_df['label'].tolist(),\n",
        "    tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_7oJlgTGWK2"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD3TI3dXGWK2"
      },
      "source": [
        "### Create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDb7gAbNGWK2"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NMxBzaLGWK7"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ACHG8qGWK7"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V0YXCjAGWK7"
      },
      "source": [
        "### Custom model with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6qJ47P6gHSG"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel\n",
        "\"\"\"\n",
        "class DistilBERTForFakeNews(nn.Module):\n",
        "    def __init__(self, num_labels=2, dropout_rate=0.3):\n",
        "        super(DistilBERTForFakeNews, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = nn.Linear(768, 768)\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Add dropout for regularization\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
        "\n",
        "        return logits\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiHpzs4ugHSG",
        "outputId": "3e0de92c-ccd1-426c-f455-751997124133"
      },
      "source": [
        "### Manual training loop with class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXGsGtMiGWK7"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, epochs=3, lr=2e-5):\n",
        "    \"\"\"Train model with class weights and early stopping\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    scaler = None\n",
        "    if torch.cuda.is_available():\n",
        "        scaler = torch.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5)\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Set up scheduler\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Set up loss function with class weights\n",
        "    class_weights_tensor = torch.tensor([class_weights[0], class_weights[1]], device=device, dtype=torch.float32)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_f1 = 0\n",
        "    patience = 2\n",
        "    counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with conditional autocast\n",
        "            if scaler:\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "                loss = criterion(outputs.float(), batch['labels'])\n",
        "            else:\n",
        "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "                loss = criterion(outputs, batch['labels'])\n",
        "\n",
        "            # Backward pass\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
        "            labels = batch['labels'].detach().cpu().numpy()\n",
        "            all_train_preds.extend(preds)\n",
        "            all_train_labels.extend(labels)\n",
        "\n",
        "            # Debug print (occasionally)\n",
        "            if len(all_train_preds) % 500 == 0:\n",
        "                print(f\"Unique predictions: {np.unique(preds)}\")\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = train_loss / len(train_dataloader)\n",
        "        train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
        "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
        "            all_train_labels, all_train_preds, average='binary', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_preds = []\n",
        "        all_val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "                loss = criterion(outputs, batch['labels'])\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
        "                labels = batch['labels'].detach().cpu().numpy()\n",
        "                all_val_preds.extend(preds)\n",
        "                all_val_labels.extend(labels)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_dataloader)\n",
        "        val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
        "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
        "            all_val_labels, all_val_preds, average='binary', zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Print class distributions to debug\n",
        "        print(f\"  Train predictions: {np.bincount(all_train_preds)}\")\n",
        "        print(f\"  Val predictions: {np.bincount(all_val_preds)}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_titles = set(train_df['title']).intersection(set(val_df['title']))\n",
        "print(f\"Shared titles between train/val: {len(common_titles)}\")"
      ],
      "metadata": {
        "id": "tUxYvk2PP7-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3nM6UszGWK7"
      },
      "outputs": [],
      "source": [
        "# Initialize our custom model with dropout\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AeKVT25GWK7"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting model training with manual loop and early stopping...\")\n",
        "model = train_model(model, train_dataloader, val_dataloader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class weights:\", class_weights)  # Should be ~[0.93, 1.07]"
      ],
      "metadata": {
        "id": "gGiYN-iyM-Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.grad.abs().mean())  # Should not be all zeros"
      ],
      "metadata": {
        "id": "FI44NYmcNEpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train label distribution:\", train_df['label'].value_counts(normalize=True))\n",
        "print(\"Val label distribution:\", val_df['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "A6XZ-FlgNF5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(train_dataloader))\n",
        "print(\"Input IDs shape:\", sample['input_ids'].shape)\n",
        "print(\"Labels:\", sample['labels'].unique())"
      ],
      "metadata": {
        "id": "BJMVYCdpNnpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfbqgFn6GWK7"
      },
      "source": [
        "### Calibrate classification threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ympdj6YNGWK7"
      },
      "outputs": [],
      "source": [
        "def calibrate_threshold(model, dataloader):\n",
        "    \"\"\"Find optimal classification threshold using precision-recall curve\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Calibrating threshold\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Prob of class 1 (fake)\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    # Find optimal threshold\n",
        "    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
        "    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    # Get the index of the best F1 score\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    # Get the threshold that gives the best F1 score\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "    return optimal_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znCP9kKGGWK8"
      },
      "outputs": [],
      "source": [
        "# Calibrate threshold on validation set\n",
        "optimal_threshold = calibrate_threshold(model, val_dataloader)\n",
        "print(f\"Optimal classification threshold: {optimal_threshold:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhFXzwP3Nir9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmN2fcOlgHSH"
      },
      "source": [
        "## Results and Evaluation\n",
        "\n",
        "Assessing model performance on the test set with standard classification metrics:\n",
        "- Accuracy\n",
        "- Precision and recall\n",
        "- F1 score\n",
        "- Confusion matrix visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBTcP_kKgHSH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, threshold=0.5):\n",
        "    \"\"\"Evaluate model with calibrated threshold\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_probs = []\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Prob of class 1 (fake)\n",
        "\n",
        "            # Apply threshold\n",
        "            preds = (probs >= threshold).astype(int)\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='binary'\n",
        "    )\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'probabilities': all_probs,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhQhVh3MGWK8"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set with calibrated threshold\n",
        "test_results = evaluate_model(model, test_dataloader, threshold=optimal_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7l6iznRGWK8"
      },
      "outputs": [],
      "source": [
        "print(f\"Test Results (with threshold {optimal_threshold:.4f}):\")\n",
        "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
        "print(f\"Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"Recall: {test_results['recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_results['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH77ronoGWK8"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = test_results['confusion_matrix']\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(f'Confusion Matrix (Threshold: {optimal_threshold:.4f})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwnBHOG4GWK8"
      },
      "outputs": [],
      "source": [
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_results['labels'], test_results['predictions'], target_names=['Real', 'Fake']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lp4Hcr5gHSI"
      },
      "outputs": [],
      "source": [
        "def analyze_title(title, model, tokenizer, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze a news title using the trained model\n",
        "\n",
        "    Args:\n",
        "        title: The news title to analyze\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer\n",
        "        threshold: Classification threshold\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    processed_title = preprocess_text(title)\n",
        "\n",
        "    # Tokenize\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoded_input = tokenizer(\n",
        "        processed_title,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "\n",
        "    # Get prediction\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    fake_prob = probs[0, 1].item()\n",
        "    real_prob = probs[0, 0].item()\n",
        "    prediction = \"Fake\" if fake_prob >= threshold else \"Real\"\n",
        "    processing_time = time.time() - start_time\n",
        "\n",
        "    # Calculate credibility score (inverted fake probability)\n",
        "    credibility_score = 1.0 - fake_prob\n",
        "\n",
        "    # Determine category\n",
        "    if credibility_score > 0.8:\n",
        "        category = \"credible\"\n",
        "    elif credibility_score < 0.2:\n",
        "        category = \"fake\"\n",
        "    else:\n",
        "        category = \"mixed\"\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"prediction\": prediction,\n",
        "        \"fake_probability\": fake_prob,\n",
        "        \"real_probability\": real_prob,\n",
        "        \"credibility_score\": credibility_score,\n",
        "        \"category\": category,\n",
        "        \"processing_time\": processing_time\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOklaX-1GWK8"
      },
      "outputs": [],
      "source": [
        "# Test the model with some examples\n",
        "test_titles = [\n",
        "    \"Scientists discover breakthrough treatment for cancer that pharmaceutical companies don't want you to know about.\",\n",
        "    \"According to a study published in the Journal of Medicine, regular exercise may reduce the risk of heart disease.\",\n",
        "    \"Secret government documents reveal aliens have been living among us for decades.\",\n",
        "    \"The Supreme Court announced its decision on the case yesterday, with a 6-3 majority opinion.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbK1kGxIGWK8"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTesting model on example titles:\")\n",
        "for title in test_titles:\n",
        "    result = analyze_title(title, model, tokenizer, threshold=optimal_threshold)\n",
        "    print(f\"\\nTitle: {result['title']}\")\n",
        "    print(f\"Prediction: {result['prediction']} (confidence: {max(result['fake_probability'], result['real_probability']):.4f})\")\n",
        "    print(f\"Category: {result['category']}\")\n",
        "    print(f\"Credibility score: {result['credibility_score']:.4f}\")\n",
        "    print(f\"Processing time: {result['processing_time']:.4f} seconds\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eambQlyaGWK8"
      },
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "MODEL_OUTPUT_DIR = \"./models/distilbert_titles_only\"\n",
        "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIR, \"model.pt\"))\n",
        "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3BiA9biGWK8"
      },
      "outputs": [],
      "source": [
        "# Save evaluation metrics\n",
        "metrics = {\n",
        "    \"model_name\": \"DistilBERT (Titles Only)\",\n",
        "    \"accuracy\": float(test_results['accuracy']),\n",
        "    \"precision\": float(test_results['precision']),\n",
        "    \"recall\": float(test_results['recall']),\n",
        "    \"f1_score\": float(test_results['f1']),\n",
        "    \"optimal_threshold\": float(optimal_threshold),\n",
        "    \"train_size\": len(train_df),\n",
        "    \"val_size\": len(val_df),\n",
        "    \"test_size\": len(test_df)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcIUISSxGWK9"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(MODEL_OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "print(f\"\\nModel saved to {MODEL_OUTPUT_DIR}\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evqsKOOtcccZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}