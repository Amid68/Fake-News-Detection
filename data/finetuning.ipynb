{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection with DistilBERT\n",
    "\n",
    "In this notebook, we will finetune DistilBERT, a lightweight transformer model, for fake news detection using the FakeNewsNet dataset (CSV format). We'll compare its performance with TinyBERT to determine which model is better suited for your Django application.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Install dependencies (PyTorch-only, avoiding TensorFlow/Keras conflicts)\n",
    "2. Load and explore the FakeNewsNet CSV data\n",
    "3. Preprocess the text data\n",
    "4. Implement DistilBERT for sequence classification\n",
    "5. Train and evaluate the model\n",
    "6. Compare with TinyBERT\n",
    "7. Export the model for Django integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore the FakeNewsNet Dataset (CSV Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake news dataset shape: (23481, 4)\n",
      "Real news dataset shape: (21417, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    fake_news = pd.read_csv(\"./fake-news-net/Fake.csv\")\n",
    "    real_news = pd.read_csv(\"./fake-news-net/True.csv\")\n",
    "    \n",
    "    print(f\"Fake news dataset shape: {fake_news.shape}\")\n",
    "    print(f\"Real news dataset shape: {real_news.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please ensure the CSV files are in the correct location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake news dataset columns:\n",
      "['title', 'text', 'subject', 'date']\n",
      "\n",
      "Real news dataset columns:\n",
      "['title', 'text', 'subject', 'date']\n",
      "\n",
      "Missing values in fake news dataset:\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in real news dataset:\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore the datasets\n",
    "print(\"Fake news dataset columns:\")\n",
    "print(fake_news.columns.tolist())\n",
    "\n",
    "print(\"\\nReal news dataset columns:\")\n",
    "print(real_news.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in fake news dataset:\")\n",
    "print(fake_news.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in real news dataset:\")\n",
    "print(real_news.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from fake news dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from real news dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a few examples from each dataset\n",
    "print(\"Sample from fake news dataset:\")\n",
    "display(fake_news.head(2))\n",
    "\n",
    "print(\"\\nSample from real news dataset:\")\n",
    "display(real_news.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (44898, 5)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare combined dataset with labels\n",
    "# Add a label column (1 for fake, 0 for real)\n",
    "fake_news['label'] = 1\n",
    "real_news['label'] = 0\n",
    "\n",
    "# Combine the datasets\n",
    "df = pd.concat([fake_news, real_news], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: ['title', 'text', 'subject', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Check for text columns (title, text, etc.)\n",
    "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Text columns: {text_columns}\")\n",
    "\n",
    "# Let's identify which columns contain the article text and title\n",
    "# Assuming the dataset has columns like 'title' and 'text'\n",
    "title_col = 'title' if 'title' in df.columns else None\n",
    "text_col = 'text' if 'text' in df.columns else None\n",
    "\n",
    "# If the columns have different names, try to guess based on content\n",
    "if title_col is None or text_col is None:\n",
    "    for col in text_columns:\n",
    "        # Sample the first few values to determine type of content\n",
    "        sample_lengths = df[col].str.len().head(10)\n",
    "        avg_length = sample_lengths.mean()\n",
    "        \n",
    "        if avg_length < 100 and title_col is None:\n",
    "            title_col = col\n",
    "            print(f\"Using '{col}' as title column\")\n",
    "        elif avg_length > 100 and text_col is None:\n",
    "            text_col = col\n",
    "            print(f\"Using '{col}' as text column\")\n",
    "\n",
    "# If we still don't have a text column, use the longest text column\n",
    "if text_col is None:\n",
    "    avg_lengths = {col: df[col].str.len().mean() for col in text_columns}\n",
    "    text_col = max(avg_lengths, key=avg_lengths.get)\n",
    "    print(f\"Selected '{text_col}' as text column based on length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty texts after preprocessing: 9\n",
      "Dataset size after removing empty texts: 44889\n"
     ]
    }
   ],
   "source": [
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine title and text if both are available\n",
    "if title_col and text_col:\n",
    "    df['combined_text'] = df[title_col].fillna('') + ' ' + df[text_col].fillna('')\n",
    "elif title_col:\n",
    "    df['combined_text'] = df[title_col].fillna('')\n",
    "else:\n",
    "    df['combined_text'] = df[text_col].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Check for empty texts after preprocessing\n",
    "empty_texts = df['processed_text'].apply(lambda x: len(x.strip()) == 0).sum()\n",
    "print(f\"Number of empty texts after preprocessing: {empty_texts}\")\n",
    "\n",
    "# Remove empty texts if any\n",
    "if empty_texts > 0:\n",
    "    df = df[df['processed_text'].apply(lambda x: len(x.strip()) > 0)].reset_index(drop=True)\n",
    "    print(f\"Dataset size after removing empty texts: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length statistics:\n",
      "         count         mean          std   min     25%     50%     75%  \\\n",
      "label                                                                    \n",
      "0      21417.0  2439.876173  1678.425520  30.0   977.0  2278.0  3287.0   \n",
      "1      23472.0  2624.662065  2520.052044  29.0  1518.0  2241.0  3104.0   \n",
      "\n",
      "           max  \n",
      "label           \n",
      "0      29835.0  \n",
      "1      51592.0  \n"
     ]
    }
   ],
   "source": [
    "# Visualize text length distribution\n",
    "df['text_length'] = df['processed_text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=30, log_scale=(False, True))\n",
    "plt.title('Text Length Distribution by Label')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.show()\n",
    "\n",
    "# Display statistics\n",
    "print(\"Text length statistics:\")\n",
    "print(df.groupby('label')['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30524\n",
      "Validation set size: 5387\n",
      "Test set size: 8978\n",
      "\n",
      "Label distribution in training set:\n",
      "label\n",
      "1    0.522867\n",
      "0    0.477133\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label distribution in validation set:\n",
      "label\n",
      "1    0.522926\n",
      "0    0.477074\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label distribution in test set:\n",
      "label\n",
      "1    0.522945\n",
      "0    0.477055\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "# First, split into train+val and test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Then split train+val into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.15, random_state=SEED, stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in validation set:\")\n",
    "print(val_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(test_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Dataset for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors and remove batch dimension the tokenizer adds\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load DistilBERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dec825b0d249f78927d3256d97dc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b8ee05dd794611b23307434be993fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ddce15d87b4e958e58bb684490f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb93fc9e72d457180d7bedadad679a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FakeNewsDataset(\n",
    "    train_df['processed_text'].tolist(),\n",
    "    train_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = FakeNewsDataset(\n",
    "    val_df['processed_text'].tolist(),\n",
    "    val_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = FakeNewsDataset(\n",
    "    test_df['processed_text'].tolist(),\n",
    "    test_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20a5dde135c43808f358afb0156544b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased\n",
      "Number of parameters: 66955010\n"
     ]
    }
   ],
   "source": [
    "# Load model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training Arguments and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: acc,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results_distilbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs_distilbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1773\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:2299\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:2172\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2173\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2174\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2175\u001b[0m         )\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_distilbert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_distilbert',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "train_start = time.time()\n",
    "trainer.train()\n",
    "train_end = time.time()\n",
    "train_time = train_end - train_start\n",
    "print(f\"Training completed in {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Test results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "test_pred_output = trainer.predict(test_dataset)\n",
    "test_preds = test_pred_output.predictions.argmax(-1)\n",
    "test_labels = test_pred_output.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Memory Usage and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)\n",
    "\n",
    "# Create pipeline for inference\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Memory before\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "# Measure inference time on sample texts\n",
    "sample_texts = test_df['processed_text'].head(50).tolist()\n",
    "start_time = time.time()\n",
    "for text in sample_texts:\n",
    "    _ = classifier(text[:512])\n",
    "end_time = time.time()\n",
    "\n",
    "# Memory after\n",
    "mem_after = get_memory_usage()\n",
    "mem_used = mem_after - mem_before\n",
    "\n",
    "avg_inference_time = (end_time - start_time) / len(sample_texts)\n",
    "\n",
    "print(f\"Average inference time: {avg_inference_time:.4f} seconds per sample\")\n",
    "print(f\"Memory usage during inference: {mem_used:.2f} MB\")\n",
    "\n",
    "# Add these to a metrics dictionary\n",
    "model_metrics = {\n",
    "    \"model_name\": \"DistilBERT\",\n",
    "    \"accuracy\": results[\"eval_accuracy\"],\n",
    "    \"f1_score\": results[\"eval_f1\"],\n",
    "    \"precision\": results[\"eval_precision\"],\n",
    "    \"recall\": results[\"eval_recall\"],\n",
    "    \"avg_processing_time\": avg_inference_time,\n",
    "    \"avg_memory_usage\": mem_used,\n",
    "    \"parameter_count\": sum(p.numel() for p in model.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Metrics for Django Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "MODEL_OUTPUT_DIR = \"./models/distilbert_fakenewsnet\"\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(f\"Model saved to {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "# Save metrics\n",
    "METRICS_OUTPUT_PATH = \"./models/distilbert_fakenewsnet_metrics.json\"\n",
    "with open(METRICS_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(model_metrics, f, indent=4)\n",
    "print(f\"Model metrics saved to {METRICS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Example of How to Use the Model in Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a few examples\n",
    "examples = [\n",
    "    \"Scientists discover breakthrough treatment for cancer that pharmaceutical companies don't want you to know about.\",\n",
    "    \"According to a study published in the Journal of Medicine, regular exercise may reduce the risk of heart disease.\",\n",
    "    \"Secret government documents reveal aliens have been living among us for decades.\",\n",
    "    \"The Supreme Court announced its decision on the case yesterday, with a 6-3 majority opinion.\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    result = classifier(example)\n",
    "    label = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    \n",
    "    # Convert label index to text\n",
    "    label_text = \"Fake\" if \"LABEL_1\" in label else \"Real\"\n",
    "    \n",
    "    print(f\"Text: {example}\")\n",
    "    print(f\"Prediction: {label_text} (confidence: {score:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to integrate with Django (example for services.py)\n",
    "def analyze_with_distilbert(text, model_dir=\"./models/distilbert_fakenewsnet\"):\n",
    "    \"\"\"\n",
    "    Analyze text using the DistilBERT model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        model_dir: Path to the saved model directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detection results\n",
    "    \"\"\"\n",
    "    # Import the required libraries (inside the function to avoid loading at startup)\n",
    "    import time\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create pipeline\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=device)\n",
    "    \n",
    "    # Measure performance\n",
    "    start_time = time.time()\n",
    "    result = classifier(text[:512])[0]\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Map the result\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # In this model, LABEL_0 = real, LABEL_1 = fake\n",
    "    if \"LABEL_0\" in label:\n",
    "        credibility_score = score\n",
    "        category = \"credible\" if score > 0.7 else \"mixed\"\n",
    "    else:\n",
    "        credibility_score = 1 - score\n",
    "        category = \"fake\" if score > 0.7 else \"mixed\"\n",
    "    \n",
    "    return {\n",
    "        \"credibility_score\": credibility_score,\n",
    "        \"category\": category,\n",
    "        \"confidence\": score,\n",
    "        \"model_name\": \"DistilBERT\",\n",
    "        \"processing_time\": processing_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Compare with TinyBERT (If Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've also trained TinyBERT, load its metrics for comparison\n",
    "try:\n",
    "    with open(\"./models/tinybert_fakenewsnet_metrics.json\", 'r') as f:\n",
    "        tinybert_metrics = json.load(f)\n",
    "    \n",
    "    # Create comparison table\n",
    "    metrics_comparison = pd.DataFrame({\n",
    "        'DistilBERT': [\n",
    "            model_metrics['accuracy'],\n",
    "            model_metrics['f1_score'],\n",
    "            model_metrics['precision'],\n",
    "            model_metrics['recall'],\n",
    "            model_metrics['avg_processing_time'],\n",
    "            model_metrics['avg_memory_usage'],\n",
    "            model_metrics['parameter_count']\n",
    "        ],\n",
    "        'TinyBERT': [\n",
    "            tinybert_metrics['accuracy'],\n",
    "            tinybert_metrics['f1_score'],\n",
    "            tinybert_metrics['precision'],\n",
    "            tinybert_metrics['recall'],\n",
    "            tinybert_metrics['avg_processing_time'],\n",
    "            tinybert_metrics['avg_memory_usage'],\n",
    "            tinybert_metrics['parameter_count']\n",
    "        ]\n",
    "    }, index=[\n",
    "        'Accuracy',\n",
    "        'F1 Score',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Avg. Processing Time (s)',\n",
    "        'Memory Usage (MB)',\n",
    "        'Parameter Count'\n",
    "    ])\n",
    "    \n",
    "    display(metrics_comparison)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    # Performance metrics\n",
    "    performance_metrics = metrics_comparison.iloc[:4]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    performance_metrics.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Performance Metrics Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resource usage\n",
    "    resource_metrics = metrics_comparison.iloc[4:6]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    resource_metrics.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Resource Usage Comparison')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"TinyBERT metrics not available for comparison: {e}\")\n",
    "    print(\"Complete the TinyBERT notebook first to enable model comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "1. Train models on the LIAR dataset (see the LIAR dataset notebook)\n",
    "2. Compare the performance between models trained on FakeNewsNet and LIAR\n",
    "3. Select the best performing model for your Django application\n",
    "4. Integrate the model into your Django application using the provided code\n",
    "\n",
    "### Django Integration Tips\n",
    "\n",
    "1. Install PyTorch and Transformers in your Django environment\n",
    "2. Copy the saved model to a directory accessible by your Django app\n",
    "3. Use the `analyze_with_distilbert` function in your `services.py` file\n",
    "4. Initialize the model once in a worker or use lazy loading to avoid startup delays\n",
    "5. Consider using a message queue (Celery) for asynchronous processing of large articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
