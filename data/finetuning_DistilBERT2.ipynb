{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning DistilBERT for Fake News Detection (Resource-Efficient & Robust)\n",
    "\n",
    "This notebook demonstrates an improved pipeline to fine-tune DistilBERT on FakeNewsNet,\n",
    "addressing data leakage, overfitting, and resource constraints.\n",
    "Key improvements:\n",
    "  - Publisher/source stripping & deduplication\n",
    "  - Group-based train/test split to avoid overlap\n",
    "  - Dynamic padding & FP16 for memory efficiency\n",
    "  - Custom weighted loss & early stopping\n",
    "  - Minimal epochs/batch sizes + gradient accumulation"
   ],
   "id": "82bb600260e12ad4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-03T11:10:45.358190Z",
     "start_time": "2025-05-03T10:56:56.121284Z"
    }
   },
   "source": [
    "# 1. Install dependencies (run once)\n",
    "!pip install transformers datasets scikit-learn torch accelerate --upgrade"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.12/site-packages (4.46.2)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\r\n",
      "Requirement already satisfied: datasets in /opt/miniconda3/lib/python3.12/site-packages (3.3.2)\r\n",
      "Collecting datasets\r\n",
      "  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.12/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.12/site-packages (2.6.0)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: accelerate in /opt/miniconda3/lib/python3.12/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (3.17.0)\r\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\r\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\r\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.12)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch) (75.8.1)\r\n",
      "Collecting sympy>=1.13.3 (from torch)\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.1.5)\r\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.12/site-packages (from accelerate) (6.1.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.4/10.4 MB\u001B[0m \u001B[31m266.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hUsing cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\r\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.7/2.7 MB\u001B[0m \u001B[31m97.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:03\u001B[0mm\r\n",
      "\u001B[?25hUsing cached datasets-3.5.1-py3-none-any.whl (491 kB)\r\n",
      "Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.6/68.6 MB\u001B[0m \u001B[31m159.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:10\u001B[0m\r\n",
      "\u001B[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "Installing collected packages: sympy, torch, huggingface-hub, tokenizers, transformers, datasets\r\n",
      "\u001B[2K  Attempting uninstall: sympy\r\n",
      "\u001B[2K    Found existing installation: sympy 1.13.1\r\n",
      "\u001B[2K    Uninstalling sympy-1.13.1:\r\n",
      "\u001B[2K      Successfully uninstalled sympy-1.13.1━━━━━\u001B[0m \u001B[32m0/6\u001B[0m [sympy]\r\n",
      "\u001B[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0/6\u001B[0m [sympy]\r\n",
      "\u001B[2K    Found existing installation: torch 2.6.0\u001B[0m \u001B[32m0/6\u001B[0m [sympy]\r\n",
      "\u001B[2K    Uninstalling torch-2.6.0:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K      Successfully uninstalled torch-2.6.0━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K  Attempting uninstall: huggingface-hub━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K    Found existing installation: huggingface-hub 0.29.1━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K    Uninstalling huggingface-hub-0.29.1:━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K      Successfully uninstalled huggingface-hub-0.29.1━━━━━━━━━\u001B[0m \u001B[32m1/6\u001B[0m [torch]\r\n",
      "\u001B[2K  Attempting uninstall: tokenizersm\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/6\u001B[0m [huggingface-hub]\r\n",
      "\u001B[2K    Found existing installation: tokenizers 0.20.3━━━━━━━━━━━━\u001B[0m \u001B[32m2/6\u001B[0m [huggingface-hub]\r\n",
      "\u001B[2K    Uninstalling tokenizers-0.20.3:m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/6\u001B[0m [huggingface-hub]\r\n",
      "\u001B[2K      Successfully uninstalled tokenizers-0.20.3━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/6\u001B[0m [huggingface-hub]\r\n",
      "\u001B[2K  Attempting uninstall: transformersm╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/6\u001B[0m [tokenizers]]\r\n",
      "\u001B[2K    Found existing installation: transformers 4.46.2━━━━━━━━━━\u001B[0m \u001B[32m3/6\u001B[0m [tokenizers]\r\n",
      "\u001B[2K    Uninstalling transformers-4.46.2:m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/6\u001B[0m [tokenizers]\r\n",
      "\u001B[2K      Successfully uninstalled transformers-4.46.2━━━━━━━━━━━━\u001B[0m \u001B[32m3/6\u001B[0m [tokenizers]\r\n",
      "\u001B[2K  Attempting uninstall: datasets━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m4/6\u001B[0m [transformers]\r\n",
      "\u001B[2K    Found existing installation: datasets 3.3.290m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m5/6\u001B[0m [datasets]\r\n",
      "\u001B[2K    Uninstalling datasets-3.3.2:━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m5/6\u001B[0m [datasets]\r\n",
      "\u001B[2K      Successfully uninstalled datasets-3.3.2m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m5/6\u001B[0m [datasets]\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6/6\u001B[0m [datasets]5/6\u001B[0m [datasets]\r\n",
      "\u001B[1A\u001B[2K\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchaudio 2.6.0 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\r\n",
      "coqui-tts 0.25.3 requires spacy[ja]<3.8,>=3, but you have spacy 3.8.4 which is incompatible.\r\n",
      "coqui-tts 0.25.3 requires transformers<=4.46.2,>=4.43.0, but you have transformers 4.51.3 which is incompatible.\r\n",
      "torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed datasets-3.5.1 huggingface-hub-0.30.2 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 transformers-4.51.3\r\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Imports",
   "id": "8807e22c91171243"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:06.875731Z",
     "start_time": "2025-05-03T11:11:06.872879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Suppress known FutureWarnings from HuggingFace and other libraries\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ],
   "id": "3eeced11f300e763",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load & label data",
   "id": "8060e72cd75bd39c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:11.756320Z",
     "start_time": "2025-05-03T11:11:10.836923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_fake = pd.read_csv(\"fake-news-net/Fake.csv\")\n",
    "df_true = pd.read_csv(\"fake-news-net/True.csv\")"
   ],
   "id": "134282c9fa8627b8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:12.249752Z",
     "start_time": "2025-05-03T11:11:12.211512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_fake[\"label\"] = 1  # fake = 1, real = 0\n",
    "df_true[\"label\"] = 0\n",
    "df = pd.concat([df_fake, df_true], ignore_index=True)"
   ],
   "id": "17d61ba8a09e5e90",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Preprocessing: strip publisher metadata",
   "id": "13e29efb781e3134"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:16.333Z",
     "start_time": "2025-05-03T11:11:13.810308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pattern = r\"\\([^)]+Reuters\\)|[A-Z ]+:\"\n",
    "# e.g. (Reuters), WASHINGTON:, NEW YORK:\n",
    "def clean_text(text):\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "df['title'] = df['title'].map(lambda x: re.sub(pattern, '', x).strip())"
   ],
   "id": "7f8b9cda58ef4bc5",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Deduplicate",
   "id": "71e2c7fe065d755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:17.554725Z",
     "start_time": "2025-05-03T11:11:17.233855Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.drop_duplicates(subset=['title', 'text']).reset_index(drop=True)",
   "id": "eb6d21ebe6e40418",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Extract publisher/source for grouping (approximate)\n",
   "id": "548736c523843f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:18.414115Z",
     "start_time": "2025-05-03T11:11:18.383473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_source(text):\n",
    "    m = re.match(r\"([A-Z][A-Za-z ]+):\", text)\n",
    "    return m.group(1) if m else 'UNKNOWN'\n",
    "\n",
    "df['source'] = df['text'].map(extract_source)"
   ],
   "id": "224e2487c3121b88",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Group-based train/test split\n",
   "id": "f6b74c1ebeac9f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:24.772372Z",
     "start_time": "2025-05-03T11:11:24.738598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=df['source']))\n",
    "train_df, test_df = df.loc[train_idx], df.loc[test_idx]"
   ],
   "id": "21f54d44ecc4490b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Tokenizer & Dataset preparation\n",
   "id": "5887e884ee1a06d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:26.239948Z",
     "start_time": "2025-05-03T11:11:25.935156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "d1c370a556c9b888",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:26.802683Z",
     "start_time": "2025-05-03T11:11:26.800203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Map texts -> tokenized\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'], batch['title'],\n",
    "        truncation=True, max_length=512\n",
    "    )"
   ],
   "id": "1e28c0e21ec65c0d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:27.652802Z",
     "start_time": "2025-05-03T11:11:27.402845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use HuggingFace `datasets` for efficiency\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train_df[['text','title','label']])\n",
    "eval_ds  = Dataset.from_pandas(test_df[['text','title','label']])"
   ],
   "id": "a585ff985060b2e8",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:36.590597Z",
     "start_time": "2025-05-03T11:11:28.300403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "eval_ds  = eval_ds.map(tokenize_batch, batched=True)"
   ],
   "id": "84b796dac3eb50ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39009/39009 [00:08<00:00, 4726.78 examples/s]\n",
      "Map: 100%|██████████| 93/93 [00:00<00:00, 4240.17 examples/s]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:38.091165Z",
     "start_time": "2025-05-03T11:11:38.087813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set format\n",
    "train_ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "eval_ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])"
   ],
   "id": "aa2d72d527d0a032",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. Compute class weights",
   "id": "cdb34005056056c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:40.804714Z",
     "start_time": "2025-05-03T11:11:40.795248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = train_df['label'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}"
   ],
   "id": "2dd01b7ec25c4753",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10. Custom Trainer for weighted loss",
   "id": "c13b7af67e940c7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:42.675483Z",
     "start_time": "2025-05-03T11:11:42.671347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop('label')\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor(list(class_weights.values()), device=model.device)\n",
    "        )\n",
    "        loss = loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ],
   "id": "a2ac569fcbcba288",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11. Metrics",
   "id": "2de242a55d826a74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:44.316839Z",
     "start_time": "2025-05-03T11:11:44.313738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}"
   ],
   "id": "a8ebb81bc55cc8ea",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 12. Training arguments (resource-efficient)",
   "id": "d6b42919b22b2e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:45.884957Z",
     "start_time": "2025-05-03T11:11:45.877138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_fp16 = torch.cuda.is_available()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=use_fp16,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    ")"
   ],
   "id": "72e3448d0d717494",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 13. Initialize model & Trainer\n",
   "id": "e01bf3047bdb4717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:47.977023Z",
     "start_time": "2025-05-03T11:11:47.472885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: Classification head weights are newly initialized (random) and will be trained from scratch.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "id": "6cec5b2857e48855",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:52.108696Z",
     "start_time": "2025-05-03T11:11:52.106311Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = DataCollatorWithPadding(tokenizer)",
   "id": "f545ddfb2a3fb019",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T11:11:53.252578Z",
     "start_time": "2025-05-03T11:11:53.187085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ],
   "id": "806b719761123e03",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m trainer = \u001B[43mWeightedTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mEarlyStoppingCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mearly_stopping_patience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/documents/uni/cs/graduation/LightFakeDetect/.venv/lib/python3.12/site-packages/transformers/trainer.py:343\u001B[39m, in \u001B[36mTrainer.__init__\u001B[39m\u001B[34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28mself\u001B[39m.deepspeed = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    341\u001B[39m \u001B[38;5;28mself\u001B[39m.is_in_train = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcreate_accelerator_and_postprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# memory metrics - must set up as early as possible\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;28mself\u001B[39m._memory_tracker = TrainerMemoryTracker(\u001B[38;5;28mself\u001B[39m.args.skip_memory_metrics)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/documents/uni/cs/graduation/LightFakeDetect/.venv/lib/python3.12/site-packages/transformers/trainer.py:3882\u001B[39m, in \u001B[36mTrainer.create_accelerator_and_postprocess\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3879\u001B[39m gradient_accumulation_plugin = GradientAccumulationPlugin(**grad_acc_kwargs)\n\u001B[32m   3881\u001B[39m \u001B[38;5;66;03m# create accelerator object\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3882\u001B[39m \u001B[38;5;28mself\u001B[39m.accelerator = \u001B[43mAccelerator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdispatch_batches\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdispatch_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3884\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit_batches\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdeepspeed_plugin\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdeepspeed_plugin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgradient_accumulation_plugin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgradient_accumulation_plugin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3887\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3888\u001B[39m \u001B[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001B[39;00m\n\u001B[32m   3889\u001B[39m \u001B[38;5;28mself\u001B[39m.gather_function = \u001B[38;5;28mself\u001B[39m.accelerator.gather_for_metrics\n",
      "\u001B[31mTypeError\u001B[39m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 14. Train & evaluate Train & evaluate\n",
   "id": "ac32b6610abfc2e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "df5dedfc58e73404"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
