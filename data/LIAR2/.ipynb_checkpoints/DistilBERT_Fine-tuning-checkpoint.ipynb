{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f28b008-e34e-417d-91b1-c9e86cdd0eed",
   "metadata": {},
   "source": [
    "# DistilBERT Fine-tuning on LIAR2 Dataset\n",
    "# Based on EDA findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc7729f-b89a-4fd9-a8f8-da6ff2659ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b5b0a7-f97f-4d74-bc8e-6323a7e51aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cdaa26-e05b-448a-b5b7-7932b4905498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761764ca-4977-4df6-868f-b81010a4517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training set shape: (18369, 16)\n",
      "Validation set shape: (2297, 16)\n",
      "Testing set shape: (2296, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {valid_df.shape}\")\n",
    "print(f\"Testing set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908a013a-adb5-4d18-8185-7e9dc63caa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "0    2425\n",
      "1    5284\n",
      "2    2882\n",
      "3    2967\n",
      "4    2743\n",
      "5    2068\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e900ec6-db2d-49eb-a51e-7cbf2a779226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for the labels\n",
    "label_mapping = {\n",
    "    0: \"pants_on_fire\",\n",
    "    1: \"false\",\n",
    "    2: \"mostly_false\",\n",
    "    3: \"half_true\",\n",
    "    4: \"mostly_true\",\n",
    "    5: \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbd0e0b-3477-43aa-ad9e-94ffa5a39e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping:\n",
      "0: pants_on_fire\n",
      "1: false\n",
      "2: mostly_false\n",
      "3: half_true\n",
      "4: mostly_true\n",
      "5: true\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLabel mapping:\")\n",
    "for key, value in label_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8899a689-0726-4a13-9fc6-08c382d62906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_df['label'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "for i, count in enumerate(train_df['label'].value_counts().sort_index()):\n",
    "    plt.text(i, count + 100, label_mapping[i], ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd26ff6a-18ce-4b5c-9d7f-32e94133bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "class_weights = {}\n",
    "for label, count in class_counts.items():\n",
    "    class_weights[label] = total_samples / (len(class_counts) * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "023d6e63-8690-421c-855a-dbe7e3f6e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights to handle imbalance:\n",
      "0 (pants_on_fire): 1.2625\n",
      "1 (false): 0.5794\n",
      "2 (mostly_false): 1.0623\n",
      "3 (half_true): 1.0319\n",
      "4 (mostly_true): 1.1161\n",
      "5 (true): 1.4804\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass weights to handle imbalance:\")\n",
    "for label, weight in class_weights.items():\n",
    "    print(f\"{label} ({label_mapping[label]}): {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8ee178-50b5-4fe8-87b0-24a908171a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined feature from metadata as suggested in the EDA\n",
    "def combine_features(row):\n",
    "    subject = str(row['subject']) if pd.notna(row['subject']) else \"unknown\"\n",
    "    speaker = str(row['speaker']) if pd.notna(row['speaker']) else \"unknown\"\n",
    "    context = str(row['context']) if pd.notna(row['context']) else \"unknown\"\n",
    "    \n",
    "    # Combine features with special tokens\n",
    "    combined = f\"{row['statement']} [SEP] Subject: {subject} [SEP] Speaker: {speaker} [SEP] Context: {context}\"\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c0f883-90cc-4f2a-98bc-afddb9ab79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining features...\n",
      "\n",
      "Sample of combined text:\n",
      "90 percent of Americans \"support universal background checks\" for gun purchases. [SEP] Subject: government regulation;polls and public opinion;guns [SEP] Speaker: chris abele [SEP] Context: a tweet\n"
     ]
    }
   ],
   "source": [
    "# Apply the feature combination\n",
    "print(\"\\nCombining features...\")\n",
    "train_df['combined_text'] = train_df.apply(combine_features, axis=1)\n",
    "valid_df['combined_text'] = valid_df.apply(combine_features, axis=1)\n",
    "test_df['combined_text'] = test_df.apply(combine_features, axis=1)\n",
    "\n",
    "# Sample of combined text\n",
    "print(\"\\nSample of combined text:\")\n",
    "print(train_df['combined_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778c0759-bc0d-4d20-aba2-fbdf5a1c21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8967dd06-7dc9-40f7-9b1e-10a6d3966b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94612a80-f3c9-4425-aa8f-ab4ae5e3a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(texts, labels, max_len=MAX_LEN):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "761d5961-b39b-4bb6-b911-1325f32a2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing data...\")\n",
    "# Tokenize training data\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(\n",
    "    train_df['combined_text'].tolist(), \n",
    "    train_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Tokenize validation data\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(\n",
    "    valid_df['combined_text'].tolist(), \n",
    "    valid_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Tokenize test data\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(\n",
    "    test_df['combined_text'].tolist(), \n",
    "    test_df['label'].tolist()\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a641e71-23c7-4c0d-b55b-d28ab34fd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89b7d283-cd06-41dc-8a58-5d86aac17075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 18369 samples\n",
      "Validation dataset: 2297 samples\n",
      "Test dataset: 2296 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa6d3a0-5226-447c-b8c9-63a26139bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=6,  # 6 truth classes\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91019e89-d764-46ea-91f6-9453fbf98683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae344325-19a9-4d60-a123-bd287e9dab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "# We'll use different learning rates for different layers\n",
    "# Higher learning rate for the classification head, lower for the base model\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": model.distilbert.parameters(), \"lr\": 2e-5},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 5e-5}\n",
    "    ],\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b56034a-0283-4ddb-8703-21d7829baa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe505767-c664-426c-82b3-fbfbb6d7bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4a0412b-a2cb-4ef0-b5a9-2e21864d9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47e4a82c-b2fa-4c04-82b8-9f93106310b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a weighted loss function to handle class imbalance\n",
    "class_weights_tensor = torch.tensor([class_weights[i] for i in range(6)]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a200de-4c48-4a09-83ab-3332cd0e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3da45396-7f51-4e9b-9acf-6a7ab82e580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the F1 score (macro)\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b99e9b-0a8e-4620-8b00-05239bd2bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, loss_fn, epochs=4):\n",
    "    # Store the average loss after each epoch for plotting\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "        \n",
    "        # Reset the total loss for this epoch\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            # Unpack this training batch from our dataloader\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Clear any previously calculated gradients\n",
    "            model.zero_grad()        \n",
    "            \n",
    "            # Perform a forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                attention_mask=b_attention_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Accumulate the training loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0\n",
    "            # This is to help prevent the \"exploding gradients\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the learning rate\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Calculate the average loss over the training data\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        loss_values.append(avg_train_loss)\n",
    "        \n",
    "        print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # After the training epoch, evaluate on the validation set\n",
    "        print(\"Running Validation...\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Tracking variables \n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        eval_f1 = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Evaluate data for one epoch\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # Unpack this validation batch from the dataloader\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph\n",
    "            # during the forward pass, since this is only needed for backprop\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    attention_mask=b_attention_mask,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Accumulate the validation loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                \n",
    "                # Calculate the accuracy\n",
    "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "                \n",
    "                # Calculate the F1 score\n",
    "                tmp_eval_f1 = flat_f1(logits, label_ids)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(np.argmax(logits, axis=1).flatten())\n",
    "                all_labels.extend(label_ids.flatten())\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "                eval_f1 += tmp_eval_f1\n",
    "                \n",
    "                # Track the number of batches\n",
    "                nb_eval_steps += 1\n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_loss = eval_loss / len(val_dataloader)\n",
    "        val_loss_values.append(avg_val_loss)\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {eval_accuracy/nb_eval_steps:.4f}\")\n",
    "        print(f\"  Validation F1 (macro): {eval_f1/nb_eval_steps:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=list(label_mapping.values())))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=list(label_mapping.values()),\n",
    "                    yticklabels=list(label_mapping.values()))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch_i + 1}')\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "    return model, loss_values, val_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ba4dc15-3edb-4cf3-b6c6-f33984a47572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139f70a9c6e640d888ac0870fdc688a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model, train_loss_values, val_loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()        \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Perform a forward pass\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_labels\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:976\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    974\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 976\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    986\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:796\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    792\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    793\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    794\u001b[0m         )\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    541\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    542\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    543\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         output_attentions,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:475\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    484\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:401\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    399\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 401\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[1;32m    411\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "model, train_loss_values, val_loss_values = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    loss_fn, \n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943cd65-5ca4-4476-b246-d931d97022a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss_values, 'b-', label='Training Loss')\n",
    "plt.plot(val_loss_values, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(train_loss_values)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4dfd7-b8cc-499d-80cd-300ca630351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af73029-1e97-452b-a0c4-91dc8575b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking variables\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "test_f1 = 0\n",
    "nb_test_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d98ec0-da90-4737-80f0-5f77e21abf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774f1c1-1cbd-4558-b618-eae997f6e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # Unpack the test batch\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_attention_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Accumulate the test loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Calculate the F1 score\n",
    "        tmp_test_f1 = flat_f1(logits, label_ids)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        preds = np.argmax(logits, axis=1).flatten()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(label_ids.flatten())\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        test_accuracy += tmp_test_accuracy\n",
    "        test_f1 += tmp_test_f1\n",
    "        \n",
    "        # Track the number of batches\n",
    "        nb_test_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f22a-fb0b-472d-8490-7f631562dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the final accuracy for the test set\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy/nb_test_steps:.4f}\")\n",
    "print(f\"  Test F1 (macro): {test_f1/nb_test_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cee20-604f-4d87-bed4-77141b43a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(label_mapping.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114dfaf-faf9-4ad2-8f1d-54bacd2122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=list(label_mapping.values()),\n",
    "            yticklabels=list(label_mapping.values()))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Final Confusion Matrix - Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334547a-392c-4e09-a38a-ba5bafff2cd7",
   "metadata": {},
   "source": [
    "## Analysis of model's performance on different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb3831-253a-4994-a957-f546e4606b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test predictions\n",
    "test_predictions = all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47742ea0-9419-4e8f-a9e7-ed3b8e4097f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the test dataframe\n",
    "test_df['prediction'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356c51f-7cb4-40f8-b119-09b6da84ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by speaker\n",
    "top_speakers = test_df['speaker'].value_counts().head(10).index\n",
    "speaker_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee9b93-298b-48cc-b1bb-8404a8da10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in top_speakers:\n",
    "    speaker_df = test_df[test_df['speaker'] == speaker]\n",
    "    speaker_acc = accuracy_score(speaker_df['label'], speaker_df['prediction'])\n",
    "    speaker_f1 = f1_score(speaker_df['label'], speaker_df['prediction'], average='macro')\n",
    "    speaker_performance[speaker] = {'accuracy': speaker_acc, 'f1': speaker_f1, 'count': len(speaker_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca90b3-a427-4d3d-af16-6468386bb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display speaker performance\n",
    "speaker_perf_df = pd.DataFrame.from_dict(speaker_performance, orient='index')\n",
    "print(\"\\nPerformance by Top 10 Speakers:\")\n",
    "print(speaker_perf_df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddb6d-f355-492e-b6b9-20e73d97f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speaker performance\n",
    "plt.figure(figsize=(14, 7))\n",
    "speaker_perf_df.sort_values('count', ascending=False)[['accuracy', 'f1']].plot(kind='bar')\n",
    "plt.title('Model Performance by Speaker')\n",
    "plt.xlabel('Speaker')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239ff61-33ee-43f2-8726-5c7d7093b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "output_dir = './model_save/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
