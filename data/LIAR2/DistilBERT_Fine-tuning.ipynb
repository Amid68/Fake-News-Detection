{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f28b008-e34e-417d-91b1-c9e86cdd0eed",
   "metadata": {},
   "source": [
    "# DistilBERT Fine-tuning on LIAR2 Dataset\n",
    "# Based on EDA findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7729f-b89a-4fd9-a8f8-da6ff2659ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5b0a7-f97f-4d74-bc8e-6323a7e51aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdaa26-e05b-448a-b5b7-7932b4905498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761764ca-4977-4df6-868f-b81010a4517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {valid_df.shape}\")\n",
    "print(f\"Testing set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a013a-adb5-4d18-8185-7e9dc63caa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e900ec6-db2d-49eb-a51e-7cbf2a779226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for the labels\n",
    "label_mapping = {\n",
    "    0: \"pants_on_fire\",\n",
    "    1: \"false\",\n",
    "    2: \"mostly_false\",\n",
    "    3: \"half_true\",\n",
    "    4: \"mostly_true\",\n",
    "    5: \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd0e0b-3477-43aa-ad9e-94ffa5a39e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLabel mapping:\")\n",
    "for key, value in label_mapping.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899a689-0726-4a13-9fc6-08c382d62906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_df['label'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "for i, count in enumerate(train_df['label'].value_counts().sort_index()):\n",
    "    plt.text(i, count + 100, label_mapping[i], ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26ff6a-18ce-4b5c-9d7f-32e94133bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "class_weights = {}\n",
    "for label, count in class_counts.items():\n",
    "    class_weights[label] = total_samples / (len(class_counts) * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d6e63-8690-421c-855a-dbe7e3f6e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClass weights to handle imbalance:\")\n",
    "for label, weight in class_weights.items():\n",
    "    print(f\"{label} ({label_mapping[label]}): {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ee178-50b5-4fe8-87b0-24a908171a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined feature from metadata as suggested in the EDA\n",
    "def combine_features(row):\n",
    "    subject = str(row['subject']) if pd.notna(row['subject']) else \"unknown\"\n",
    "    speaker = str(row['speaker']) if pd.notna(row['speaker']) else \"unknown\"\n",
    "    context = str(row['context']) if pd.notna(row['context']) else \"unknown\"\n",
    "    \n",
    "    # Combine features with special tokens\n",
    "    combined = f\"{row['statement']} [SEP] Subject: {subject} [SEP] Speaker: {speaker} [SEP] Context: {context}\"\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0f883-90cc-4f2a-98bc-afddb9ab79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature combination\n",
    "print(\"\\nCombining features...\")\n",
    "train_df['combined_text'] = train_df.apply(combine_features, axis=1)\n",
    "valid_df['combined_text'] = valid_df.apply(combine_features, axis=1)\n",
    "test_df['combined_text'] = test_df.apply(combine_features, axis=1)\n",
    "\n",
    "# Sample of combined text\n",
    "print(\"\\nSample of combined text:\")\n",
    "print(train_df['combined_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c0759-bc0d-4d20-aba2-fbdf5a1c21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967dd06-7dc9-40f7-9b1e-10a6d3966b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94612a80-f3c9-4425-aa8f-ab4ae5e3a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_data(texts, labels, max_len=MAX_LEN):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d5961-b39b-4bb6-b911-1325f32a2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing data...\")\n",
    "# Tokenize training data\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(\n",
    "    train_df['combined_text'].tolist(), \n",
    "    train_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Tokenize validation data\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(\n",
    "    valid_df['combined_text'].tolist(), \n",
    "    valid_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Tokenize test data\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(\n",
    "    test_df['combined_text'].tolist(), \n",
    "    test_df['label'].tolist()\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a641e71-23c7-4c0d-b55b-d28ab34fd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7d283-cd06-41dc-8a58-5d86aac17075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6d3a0-5226-447c-b8c9-63a26139bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=6,  # 6 truth classes\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91019e89-d764-46ea-91f6-9453fbf98683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae344325-19a9-4d60-a123-bd287e9dab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "# We'll use different learning rates for different layers\n",
    "# Higher learning rate for the classification head, lower for the base model\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": model.distilbert.parameters(), \"lr\": 2e-5},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 5e-5}\n",
    "    ],\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56034a-0283-4ddb-8703-21d7829baa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe505767-c664-426c-82b3-fbfbb6d7bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0412b-a2cb-4ef0-b5a9-2e21864d9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4a82c-b2fa-4c04-82b8-9f93106310b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a weighted loss function to handle class imbalance\n",
    "class_weights_tensor = torch.tensor([class_weights[i] for i in range(6)]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a200de-4c48-4a09-83ab-3332cd0e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da45396-7f51-4e9b-9acf-6a7ab82e580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the F1 score (macro)\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b99e9b-0a8e-4620-8b00-05239bd2bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, loss_fn, epochs=4):\n",
    "    # Store the average loss after each epoch for plotting\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "        \n",
    "        # Reset the total loss for this epoch\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of training data\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            # Unpack this training batch from our dataloader\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Clear any previously calculated gradients\n",
    "            model.zero_grad()        \n",
    "            \n",
    "            # Perform a forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                attention_mask=b_attention_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Accumulate the training loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Perform a backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0\n",
    "            # This is to help prevent the \"exploding gradients\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the learning rate\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Calculate the average loss over the training data\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        loss_values.append(avg_train_loss)\n",
    "        \n",
    "        print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # After the training epoch, evaluate on the validation set\n",
    "        print(\"Running Validation...\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Tracking variables \n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        eval_f1 = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Evaluate data for one epoch\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # Unpack this validation batch from the dataloader\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph\n",
    "            # during the forward pass, since this is only needed for backprop\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    attention_mask=b_attention_mask,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Accumulate the validation loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                \n",
    "                # Calculate the accuracy\n",
    "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "                \n",
    "                # Calculate the F1 score\n",
    "                tmp_eval_f1 = flat_f1(logits, label_ids)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(np.argmax(logits, axis=1).flatten())\n",
    "                all_labels.extend(label_ids.flatten())\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "                eval_f1 += tmp_eval_f1\n",
    "                \n",
    "                # Track the number of batches\n",
    "                nb_eval_steps += 1\n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_loss = eval_loss / len(val_dataloader)\n",
    "        val_loss_values.append(avg_val_loss)\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {eval_accuracy/nb_eval_steps:.4f}\")\n",
    "        print(f\"  Validation F1 (macro): {eval_f1/nb_eval_steps:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=list(label_mapping.values())))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=list(label_mapping.values()),\n",
    "                    yticklabels=list(label_mapping.values()))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch_i + 1}')\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "    return model, loss_values, val_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4dc15-3edb-4cf3-b6c6-f33984a47572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "model, train_loss_values, val_loss_values = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    loss_fn, \n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943cd65-5ca4-4476-b246-d931d97022a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss_values, 'b-', label='Training Loss')\n",
    "plt.plot(val_loss_values, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(train_loss_values)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4dfd7-b8cc-499d-80cd-300ca630351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af73029-1e97-452b-a0c4-91dc8575b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking variables\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "test_f1 = 0\n",
    "nb_test_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d98ec0-da90-4737-80f0-5f77e21abf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774f1c1-1cbd-4558-b618-eae997f6e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # Unpack the test batch\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_attention_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Accumulate the test loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Calculate the F1 score\n",
    "        tmp_test_f1 = flat_f1(logits, label_ids)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        preds = np.argmax(logits, axis=1).flatten()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(label_ids.flatten())\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        test_accuracy += tmp_test_accuracy\n",
    "        test_f1 += tmp_test_f1\n",
    "        \n",
    "        # Track the number of batches\n",
    "        nb_test_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f22a-fb0b-472d-8490-7f631562dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the final accuracy for the test set\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy/nb_test_steps:.4f}\")\n",
    "print(f\"  Test F1 (macro): {test_f1/nb_test_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cee20-604f-4d87-bed4-77141b43a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(label_mapping.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114dfaf-faf9-4ad2-8f1d-54bacd2122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=list(label_mapping.values()),\n",
    "            yticklabels=list(label_mapping.values()))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Final Confusion Matrix - Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334547a-392c-4e09-a38a-ba5bafff2cd7",
   "metadata": {},
   "source": [
    "## Analysis of model's performance on different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb3831-253a-4994-a957-f546e4606b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test predictions\n",
    "test_predictions = all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47742ea0-9419-4e8f-a9e7-ed3b8e4097f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the test dataframe\n",
    "test_df['prediction'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356c51f-7cb4-40f8-b119-09b6da84ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by speaker\n",
    "top_speakers = test_df['speaker'].value_counts().head(10).index\n",
    "speaker_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee9b93-298b-48cc-b1bb-8404a8da10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in top_speakers:\n",
    "    speaker_df = test_df[test_df['speaker'] == speaker]\n",
    "    speaker_acc = accuracy_score(speaker_df['label'], speaker_df['prediction'])\n",
    "    speaker_f1 = f1_score(speaker_df['label'], speaker_df['prediction'], average='macro')\n",
    "    speaker_performance[speaker] = {'accuracy': speaker_acc, 'f1': speaker_f1, 'count': len(speaker_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca90b3-a427-4d3d-af16-6468386bb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display speaker performance\n",
    "speaker_perf_df = pd.DataFrame.from_dict(speaker_performance, orient='index')\n",
    "print(\"\\nPerformance by Top 10 Speakers:\")\n",
    "print(speaker_perf_df.sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddb6d-f355-492e-b6b9-20e73d97f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speaker performance\n",
    "plt.figure(figsize=(14, 7))\n",
    "speaker_perf_df.sort_values('count', ascending=False)[['accuracy', 'f1']].plot(kind='bar')\n",
    "plt.title('Model Performance by Speaker')\n",
    "plt.xlabel('Speaker')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239ff61-33ee-43f2-8726-5c7d7093b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "output_dir = './model_save/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
