{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection with DistilBERT\n",
    "\n",
    "This notebook demonstrates how to finetune DistilBERT, a lightweight transformer model, for fake news detection using the FakeNewsNet dataset (CSV format). We'll compare its performance with TinyBERT to determine which model is better suited for your Django application.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Install dependencies (PyTorch-only, avoiding TensorFlow/Keras conflicts)\n",
    "2. Load and explore the FakeNewsNet CSV data\n",
    "3. Preprocess the text data\n",
    "4. Implement DistilBERT for sequence classification\n",
    "5. Train and evaluate the model\n",
    "6. Compare with TinyBERT\n",
    "7. Export the model for Django integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages (PyTorch Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install torch transformers datasets scikit-learn pandas numpy matplotlib seaborn psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore the FakeNewsNet Dataset (CSV Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    fake_news = pd.read_csv(\"./fake-news-net/Fake.csv\")\n",
    "    real_news = pd.read_csv(\"./fake-news-net/True.csv\")\n",
    "    \n",
    "    print(f\"Fake news dataset shape: {fake_news.shape}\")\n",
    "    print(f\"Real news dataset shape: {real_news.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please ensure the CSV files are in the correct location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explore the datasets\n",
    "print(\"Fake news dataset columns:\")\n",
    "print(fake_news.columns.tolist())\n",
    "\n",
    "print(\"\\nReal news dataset columns:\")\n",
    "print(real_news.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in fake news dataset:\")\n",
    "print(fake_news.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in real news dataset:\")\n",
    "print(real_news.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display a few examples from each dataset\n",
    "print(\"Sample from fake news dataset:\")\n",
    "display(fake_news.head(2))\n",
    "\n",
    "print(\"\\nSample from real news dataset:\")\n",
    "display(real_news.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare combined dataset with labels\n",
    "# Add a label column (1 for fake, 0 for real)\n",
    "fake_news['label'] = 1\n",
    "real_news['label'] = 0\n",
    "\n",
    "# Combine the datasets\n",
    "df = pd.concat([fake_news, real_news], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for text columns (title, text, etc.)\n",
    "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Text columns: {text_columns}\")\n",
    "\n",
    "# Let's identify which columns contain the article text and title\n",
    "# Assuming the dataset has columns like 'title' and 'text'\n",
    "title_col = 'title' if 'title' in df.columns else None\n",
    "text_col = 'text' if 'text' in df.columns else None\n",
    "\n",
    "# If the columns have different names, try to guess based on content\n",
    "if title_col is None or text_col is None:\n",
    "    for col in text_columns:\n",
    "        # Sample the first few values to determine type of content\n",
    "        sample_lengths = df[col].str.len().head(10)\n",
    "        avg_length = sample_lengths.mean()\n",
    "        \n",
    "        if avg_length < 100 and title_col is None:\n",
    "            title_col = col\n",
    "            print(f\"Using '{col}' as title column\")\n",
    "        elif avg_length > 100 and text_col is None:\n",
    "            text_col = col\n",
    "            print(f\"Using '{col}' as text column\")\n",
    "\n",
    "# If we still don't have a text column, use the longest text column\n",
    "if text_col is None:\n",
    "    avg_lengths = {col: df[col].str.len().mean() for col in text_columns}\n",
    "    text_col = max(avg_lengths, key=avg_lengths.get)\n",
    "    print(f\"Selected '{text_col}' as text column based on length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine title and text if both are available\n",
    "if title_col and text_col:\n",
    "    df['combined_text'] = df[title_col].fillna('') + ' ' + df[text_col].fillna('')\n",
    "elif title_col:\n",
    "    df['combined_text'] = df[title_col].fillna('')\n",
    "else:\n",
    "    df['combined_text'] = df[text_col].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Check for empty texts after preprocessing\n",
    "empty_texts = df['processed_text'].apply(lambda x: len(x.strip()) == 0).sum()\n",
    "print(f\"Number of empty texts after preprocessing: {empty_texts}\")\n",
    "\n",
    "# Remove empty texts if any\n",
    "if empty_texts > 0:\n",
    "    df = df[df['processed_text'].apply(lambda x: len(x.strip()) > 0)].reset_index(drop=True)\n",
    "    print(f\"Dataset size after removing empty texts: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize text length distribution\n",
    "df['text_length'] = df['processed_text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=30, log_scale=(False, True))\n",
    "plt.title('Text Length Distribution by Label')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.show()\n",
    "\n",
    "# Display statistics\n",
    "print(\"Text length statistics:\")\n",
    "print(df.groupby('label')['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into train, validation, and test sets\n",
    "# First, split into train+val and test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Then split train+val into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.15, random_state=SEED, stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in validation set:\")\n",
    "print(val_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(test_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Dataset for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors and remove batch dimension the tokenizer adds\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load DistilBERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FakeNewsDataset(\n",
    "    train_df['processed_text'].tolist(),\n",
    "    train_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = FakeNewsDataset(\n",
    "    val_df['processed_text'].tolist(),\n",
    "    val_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = FakeNewsDataset(\n",
    "    test_df['processed_text'].tolist(),\n",
    "    test_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training Arguments and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_distilbert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_distilbert',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "train_start = time.time()\n",
    "trainer.train()\n",
    "train_end = time.time()\n",
    "train_time = train_end - train_start\n",
    "print(f\"Training completed in {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Test results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "test_pred_output = trainer.predict(test_dataset)\n",
    "test_preds = test_pred_output.predictions.argmax(-1)\n",
    "test_labels = test_pred_output.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Memory Usage and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)\n",
    "\n",
    "# Create pipeline for inference\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Memory before\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "# Measure inference time on sample texts\n",
    "sample_texts = test_df['processed_text'].head(50).tolist()\n",
    "start_time = time.time()\n",
    "for text in sample_texts:\n",
    "    _ = classifier(text[:512])\n",
    "end_time = time.time()\n",
    "\n",
    "# Memory after\n",
    "mem_after = get_memory_usage()\n",
    "mem_used = mem_after - mem_before\n",
    "\n",
    "avg_inference_time = (end_time - start_time) / len(sample_texts)\n",
    "\n",
    "print(f\"Average inference time: {avg_inference_time:.4f} seconds per sample\")\n",
    "print(f\"Memory usage during inference: {mem_used:.2f} MB\")\n",
    "\n",
    "# Add these to a metrics dictionary\n",
    "model_metrics = {\n",
    "    \"model_name\": \"DistilBERT\",\n",
    "    \"accuracy\": results[\"eval_accuracy\"],\n",
    "    \"f1_score\": results[\"eval_f1\"],\n",
    "    \"precision\": results[\"eval_precision\"],\n",
    "    \"recall\": results[\"eval_recall\"],\n",
    "    \"avg_processing_time\": avg_inference_time,\n",
    "    \"avg_memory_usage\": mem_used,\n",
    "    \"parameter_count\": sum(p.numel() for p in model.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Metrics for Django Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model\n",
    "MODEL_OUTPUT_DIR = \"./models/distilbert_fakenewsnet\"\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(f\"Model saved to {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "# Save metrics\n",
    "METRICS_OUTPUT_PATH = \"./models/distilbert_fakenewsnet_metrics.json\"\n",
    "with open(METRICS_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(model_metrics, f, indent=4)\n",
    "print(f\"Model metrics saved to {METRICS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Example of How to Use the Model in Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the model with a few examples\n",
    "examples = [\n",
    "    \"Scientists discover breakthrough treatment for cancer that pharmaceutical companies don't want you to know about.\",\n",
    "    \"According to a study published in the Journal of Medicine, regular exercise may reduce the risk of heart disease.\",\n",
    "    \"Secret government documents reveal aliens have been living among us for decades.\",\n",
    "    \"The Supreme Court announced its decision on the case yesterday, with a 6-3 majority opinion.\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    result = classifier(example)\n",
    "    label = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    \n",
    "    # Convert label index to text\n",
    "    label_text = \"Fake\" if \"LABEL_1\" in label else \"Real\"\n",
    "    \n",
    "    print(f\"Text: {example}\")\n",
    "    print(f\"Prediction: {label_text} (confidence: {score:.4f})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Code to integrate with Django (example for services.py)\n",
    "def analyze_with_distilbert(text, model_dir=\"./models/distilbert_fakenewsnet\"):\n",
    "    \"\"\"\n",
    "    Analyze text using the DistilBERT model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        model_dir: Path to the saved model directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detection results\n",
    "    \"\"\"\n",
    "    # Import the required libraries (inside the function to avoid loading at startup)\n",
    "    import time\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create pipeline\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=device)\n",
    "    \n",
    "    # Measure performance\n",
    "    start_time = time.time()\n",
    "    result = classifier(text[:512])[0]\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Map the result\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # In this model, LABEL_0 = real, LABEL_1 = fake\n",
    "    if \"LABEL_0\" in label:\n",
    "        credibility_score = score\n",
    "        category = \"credible\" if score > 0.7 else \"mixed\"\n",
    "    else:\n",
    "        credibility_score = 1 - score\n",
    "        category = \"fake\" if score > 0.7 else \"mixed\"\n",
    "    \n",
    "    return {\n",
    "        \"credibility_score\": credibility_score,\n",
    "        \"category\": category,\n",
    "        \"confidence\": score,\n",
    "        \"model_name\": \"DistilBERT\",\n",
    "        \"processing_time\": processing_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Compare with TinyBERT (If Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If you've also trained TinyBERT, load its metrics for comparison\n",
    "try:\n",
    "    with open(\"./models/tinybert_fakenewsnet_metrics.json\", 'r') as f:\n",
    "        tinybert_metrics = json.load(f)\n",
    "    \n",
    "    # Create comparison table\n",
    "    metrics_comparison = pd.DataFrame({\n",
    "        'DistilBERT': [\n",
    "            model_metrics['accuracy'],\n",
    "            model_metrics['f1_score'],\n",
    "            model_metrics['precision'],\n",
    "            model_metrics['recall'],\n",
    "            model_metrics['avg_processing_time'],\n",
    "            model_metrics['avg_memory_usage'],\n",
    "            model_metrics['parameter_count']\n",
    "        ],\n",
    "        'TinyBERT': [\n",
    "            tinybert_metrics['accuracy'],\n",
    "            tinybert_metrics['f1_score'],\n",
    "            tinybert_metrics['precision'],\n",
    "            tinybert_metrics['recall'],\n",
    "            tinybert_metrics['avg_processing_time'],\n",
    "            tinybert_metrics['avg_memory_usage'],\n",
    "            tinybert_metrics['parameter_count']\n",
    "        ]\n",
    "    }, index=[\n",
    "        'Accuracy',\n",
    "        'F1 Score',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Avg. Processing Time (s)',\n",
    "        'Memory Usage (MB)',\n",
    "        'Parameter Count'\n",
    "    ])\n",
    "    \n",
    "    display(metrics_comparison)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    # Performance metrics\n",
    "    performance_metrics = metrics_comparison.iloc[:4]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    performance_metrics.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Performance Metrics Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Resource usage\n",
    "    resource_metrics = metrics_comparison.iloc[4:6]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    resource_metrics.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Resource Usage Comparison')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"TinyBERT metrics not available for comparison: {e}\")\n",
    "    print(\"Complete the TinyBERT notebook first to enable model comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "1. Train models on the LIAR dataset (see the LIAR dataset notebook)\n",
    "2. Compare the performance between models trained on FakeNewsNet and LIAR\n",
    "3. Select the best performing model for your Django application\n",
    "4. Integrate the model into your Django application using the provided code\n",
    "\n",
    "### Django Integration Tips\n",
    "\n",
    "1. Install PyTorch and Transformers in your Django environment\n",
    "2. Copy the saved model to a directory accessible by your Django app\n",
    "3. Use the `analyze_with_distilbert` function in your `services.py` file\n",
    "4. Initialize the model once in a worker or use lazy loading to avoid startup delays\n",
    "5. Consider using a message queue (Celery) for asynchronous processing of large articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
