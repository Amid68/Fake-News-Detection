{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection with FakeNewsNet Dataset\n",
    "\n",
    "This notebook demonstrates how to finetune a lightweight transformer model (TinyBERT) on the FakeNewsNet dataset for fake news detection. This model can then be integrated into the VeriFact Django application.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load and explore the FakeNewsNet dataset\n",
    "2. Preprocess the text data\n",
    "3. Set up TinyBERT for sequence classification\n",
    "4. Finetune the model\n",
    "5. Evaluate model performance\n",
    "6. Export the model for Django integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore FakeNewsNet Dataset\n",
    "\n",
    "FakeNewsNet dataset contains news content with labels for political and gossipcop domains. We'll load the dataset, which should be organized as follows:\n",
    "\n",
    "```\n",
    "FakeNewsNet/\n",
    "├── politifact/\n",
    "│   ├── fake/\n",
    "│   └── real/\n",
    "└── gossipcop/\n",
    "    ├── fake/\n",
    "    └── real/\n",
    "```\n",
    "\n",
    "Each news article has its content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Update this path to where your FakeNewsNet dataset is stored\n",
    "DATASET_PATH = \"./FakeNewsNet/\"\n",
    "\n",
    "# Function to load dataset\n",
    "def load_fakenewsnet(dataset_path, domain=\"politifact\"):\n",
    "    \"\"\"\n",
    "    Load articles from FakeNewsNet dataset for a specific domain\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Define paths for real and fake news\n",
    "    real_path = os.path.join(dataset_path, domain, \"real\")\n",
    "    fake_path = os.path.join(dataset_path, domain, \"fake\")\n",
    "    \n",
    "    # Process real news\n",
    "    print(f\"Loading real news from {domain}...\")\n",
    "    for news_id in tqdm(os.listdir(real_path)):\n",
    "        news_json_path = os.path.join(real_path, news_id, \"news content.json\")\n",
    "        if os.path.exists(news_json_path):\n",
    "            try:\n",
    "                with open(news_json_path, 'r', encoding='utf-8') as f:\n",
    "                    news_data = json.load(f)\n",
    "                    \n",
    "                # Extract text (title + content)\n",
    "                title = news_data.get('title', '')\n",
    "                content = news_data.get('text', '')\n",
    "                text = title + \" \" + content\n",
    "                \n",
    "                if text.strip():\n",
    "                    data.append(text)\n",
    "                    labels.append(0)  # 0 for real\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {news_json_path}: {e}\")\n",
    "    \n",
    "    # Process fake news\n",
    "    print(f\"Loading fake news from {domain}...\")\n",
    "    for news_id in tqdm(os.listdir(fake_path)):\n",
    "        news_json_path = os.path.join(fake_path, news_id, \"news content.json\")\n",
    "        if os.path.exists(news_json_path):\n",
    "            try:\n",
    "                with open(news_json_path, 'r', encoding='utf-8') as f:\n",
    "                    news_data = json.load(f)\n",
    "                    \n",
    "                # Extract text (title + content)\n",
    "                title = news_data.get('title', '')\n",
    "                content = news_data.get('text', '')\n",
    "                text = title + \" \" + content\n",
    "                \n",
    "                if text.strip():\n",
    "                    data.append(text)\n",
    "                    labels.append(1)  # 1 for fake\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {news_json_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'text': data,\n",
    "        'label': labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset (adjust this if needed)\n",
    "try:\n",
    "    # Try to load politifact data\n",
    "    df_politifact = load_fakenewsnet(DATASET_PATH, domain=\"politifact\")\n",
    "    print(f\"Loaded {len(df_politifact)} articles from politifact\")\n",
    "    \n",
    "    # Try to load gossipcop data\n",
    "    df_gossipcop = load_fakenewsnet(DATASET_PATH, domain=\"gossipcop\")\n",
    "    print(f\"Loaded {len(df_gossipcop)} articles from gossipcop\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = pd.concat([df_politifact, df_gossipcop], ignore_index=True)\n",
    "    print(f\"Combined dataset size: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"If your dataset is structured differently, please adjust the loading code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic dataset exploration\n",
    "print(\"Dataset overview:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nSample articles:\")\n",
    "# Display a sample real article\n",
    "real_sample = df[df['label'] == 0].iloc[0]\n",
    "print(f\"\\nReal article sample (first 300 chars): \\n{real_sample['text'][:300]}...\")\n",
    "\n",
    "# Display a sample fake article\n",
    "fake_sample = df[df['label'] == 1].iloc[0]\n",
    "print(f\"\\nFake article sample (first 300 chars): \\n{fake_sample['text'][:300]}...\")\n",
    "\n",
    "# Text length distribution\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=30, log_scale=(False, True))\n",
    "plt.title('Text Length Distribution by Label')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Let's clean the text data and prepare it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Basic text preprocessing\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Check for empty texts after preprocessing\n",
    "empty_texts = df['processed_text'].apply(lambda x: len(x.strip()) == 0).sum()\n",
    "print(f\"Number of empty texts after preprocessing: {empty_texts}\")\n",
    "\n",
    "# Remove empty texts if any\n",
    "if empty_texts > 0:\n",
    "    df = df[df['processed_text'].apply(lambda x: len(x.strip()) > 0)].reset_index(drop=True)\n",
    "    print(f\"Dataset size after removing empty texts: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into train, validation, and test sets\n",
    "# First, split into train+val and test\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Then split train+val into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.15, random_state=SEED, stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check label distribution in each split\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in validation set:\")\n",
    "print(val_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(test_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Dataset for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors and remove batch dimension the tokenizer adds\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load TinyBERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FakeNewsDataset(\n",
    "    train_df['processed_text'].tolist(),\n",
    "    train_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = FakeNewsDataset(\n",
    "    val_df['processed_text'].tolist(),\n",
    "    val_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = FakeNewsDataset(\n",
    "    test_df['processed_text'].tolist(),\n",
    "    test_df['label'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training Arguments and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Finetune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "train_start = time.time()\n",
    "trainer.train()\n",
    "train_end = time.time()\n",
    "train_time = train_end - train_start\n",
    "print(f\"Training completed in {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Test results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "test_pred_output = trainer.predict(test_dataset)\n",
    "test_preds = test_pred_output.predictions.argmax(-1)\n",
    "test_labels = test_pred_output.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Memory Usage and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measure memory usage\n",
    "import psutil\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)\n",
    "\n",
    "# Create pipeline for inference\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Memory before\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "# Measure inference time on sample texts\n",
    "sample_texts = test_df['processed_text'].head(50).tolist()\n",
    "start_time = time.time()\n",
    "for text in sample_texts:\n",
    "    _ = classifier(text[:512])\n",
    "end_time = time.time()\n",
    "\n",
    "# Memory after\n",
    "mem_after = get_memory_usage()\n",
    "mem_used = mem_after - mem_before\n",
    "\n",
    "avg_inference_time = (end_time - start_time) / len(sample_texts)\n",
    "\n",
    "print(f\"Average inference time: {avg_inference_time:.4f} seconds per sample\")\n",
    "print(f\"Memory usage during inference: {mem_used:.2f} MB\")\n",
    "\n",
    "# Add these to a metrics dictionary\n",
    "model_metrics = {\n",
    "    \"model_name\": \"TinyBERT\",\n",
    "    \"accuracy\": results[\"eval_accuracy\"],\n",
    "    \"f1_score\": results[\"eval_f1\"],\n",
    "    \"precision\": results[\"eval_precision\"],\n",
    "    \"recall\": results[\"eval_recall\"],\n",
    "    \"avg_processing_time\": avg_inference_time,\n",
    "    \"avg_memory_usage\": mem_used,\n",
    "    \"parameter_count\": sum(p.numel() for p in model.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Metrics for Django Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model\n",
    "MODEL_OUTPUT_DIR = \"./models/tinybert_fakenewsnet\"\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(f\"Model saved to {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "# Save metrics\n",
    "METRICS_OUTPUT_PATH = \"./models/tinybert_fakenewsnet_metrics.json\"\n",
    "with open(METRICS_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(model_metrics, f, indent=4)\n",
    "print(f\"Model metrics saved to {METRICS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Example of How to Use the Model in Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This code can be used in the Django services.py file\n",
    "def analyze_with_model(text, model_dir=\"./models/tinybert_fakenewsnet\"):\n",
    "    \"\"\"\n",
    "    Analyze text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        model_dir: Path to the saved model directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detection results\n",
    "    \"\"\"\n",
    "    # For demonstration purposes\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create pipeline\n",
    "    classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Measure performance\n",
    "    start_time = time.time()\n",
    "    result = classifier(text[:512])[0]\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Map the result\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # In this model, LABEL_0 = real, LABEL_1 = fake\n",
    "    if label == 'LABEL_0':\n",
    "        credibility_score = score\n",
    "        category = \"credible\" if score > 0.7 else \"mixed\"\n",
    "    else:\n",
    "        credibility_score = 1 - score\n",
    "        category = \"fake\" if score > 0.7 else \"mixed\"\n",
    "    \n",
    "    return {\n",
    "        \"credibility_score\": credibility_score,\n",
    "        \"category\": category,\n",
    "        \"confidence\": score,\n",
    "        \"model_name\": \"TinyBERT\",\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "# Test the function with a sample text\n",
    "sample_text = \"Breaking news: Scientists discover that drinking water cures all diseases. Pharmaceutical companies don't want you to know this secret.\"\n",
    "result = analyze_with_model(sample_text, model_dir=MODEL_OUTPUT_DIR)\n",
    "print(\"Analysis result:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "1. Finetune a DistilBERT model using the same approach (see the next notebook)\n",
    "2. Finetune models on the LIAR dataset (see the LIAR dataset notebook)\n",
    "3. Compare model performance metrics\n",
    "4. Integrate the best performing model(s) into your Django application\n",
    "\n",
    "In your Django application, you can load the saved model and use it for real-time predictions as shown in section 13."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
