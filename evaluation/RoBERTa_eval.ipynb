{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39581926",
   "metadata": {},
   "source": [
    "# RoBERTa Evaluation on ISOT Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook evaluates a fine-tuned RoBERTa model on two distinct fake news detection scenarios:\n",
    "1. **Titles-only dataset**: Using only article headlines\n",
    "2. **Full-text dataset**: Using complete articles with both titles and text\n",
    "\n",
    "This dual evaluation approach provides insights into how well RoBERTa performs with limited context versus full article context. As a larger and more sophisticated model compared to DistilBERT and TinyBERT, RoBERTa may capture more subtle linguistic patterns but at the cost of greater computational requirements. Understanding these trade-offs is crucial for selecting the appropriate model for different deployment scenarios.\n",
    "\n",
    "## 1. Setting Up the Environment\n",
    "\n",
    "Let's start by importing all necessary libraries and setting up utility functions to monitor resource usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081018bb",
   "metadata": {},
   "source": [
    "These core libraries provide the foundation for data manipulation, model evaluation, and resource monitoring - especially important for understanding RoBERTa's computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and evaluation libraries\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6928527",
   "metadata": {},
   "source": [
    "These specialized libraries enable us to work with the RoBERTa model and evaluate its performance using standard metrics. RoBERTa uses a different tokenizer and model architecture compared to BERT-based models like DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d66b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90feff30",
   "metadata": {},
   "source": [
    "Visualization tools will help us understand and communicate performance results more effectively, especially for comparing RoBERTa with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - using CPU for edge device testing\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39478c5e",
   "metadata": {},
   "source": [
    "I deliberately chose to use the CPU rather than GPU for this evaluation because:\n",
    "1. It provides a consistent comparison with other models evaluated on CPU\n",
    "2. CPU performance metrics are more relevant for assessing deployment feasibility on standard hardware\n",
    "3. RoBERTa's larger size makes it particularly important to understand its CPU performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98217e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "print(f\"Starting memory usage: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1579a",
   "metadata": {},
   "source": [
    "This utility function is especially important for RoBERTa, which has approximately 125M parameters compared to DistilBERT's 67M. Understanding its memory footprint is crucial for determining its viability in memory-constrained environments.\n",
    "\n",
    "## 2. Loading the Pre-trained Model\n",
    "\n",
    "Now I'll load the RoBERTa model that was previously fine-tuned on the ISOT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained RoBERTa model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = \"../ml_models/roberta-fake-news-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "start_time = time.time()\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ee4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Move to CPU\n",
    "load_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d06d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20a3e5",
   "metadata": {},
   "source": [
    "Model loading time and memory footprint are critical metrics for RoBERTa. As a larger model, we expect it to have higher memory requirements and potentially longer loading times than DistilBERT or TinyBERT, which is important to quantify for deployment planning.\n",
    "\n",
    "## 3. Data Loading and Preparation\n",
    "\n",
    "Next, I'll load various data sources for our evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data sources\n",
    "print(\"\\nLoading all data sources...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load titles_only_real.csv (real news, titles only)\n",
    "try:\n",
    "    titles_only_real_df = pd.read_csv('../data/titles_only_real.csv')\n",
    "    # Ensure label is 1 for real news\n",
    "    titles_only_real_df['label'] = 1\n",
    "    print(f\"Loaded {len(titles_only_real_df)} real news titles from titles_only_real.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading titles_only_real.csv: {e}\")\n",
    "    titles_only_real_df = pd.DataFrame(columns=['title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1efd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load FakeNewsNet (fake news only)\n",
    "try:\n",
    "    fake_news_net_df = pd.read_csv('./datasets/simplified_FakeNewsNet.csv')\n",
    "    # Keep only fake news\n",
    "    fake_news_net_df = fake_news_net_df[fake_news_net_df['label'] == 0]\n",
    "    print(f\"Loaded {len(fake_news_net_df)} fake news articles from FakeNewsNet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FakeNewsNet: {e}\")\n",
    "    fake_news_net_df = pd.DataFrame(columns=['title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde60605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load fake_news_evaluation.csv (fake news with text)\n",
    "try:\n",
    "    fake_news_eval_df = pd.read_csv('./datasets/fake_news_evaluation.csv')\n",
    "    # Ensure label is 0 for fake news\n",
    "    fake_news_eval_df['label'] = 0\n",
    "    print(f\"Loaded {len(fake_news_eval_df)} fake news articles from fake_news_evaluation.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading fake_news_evaluation.csv: {e}\")\n",
    "    fake_news_eval_df = pd.DataFrame(columns=['title', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load manual_real.csv (real news with text)\n",
    "try:\n",
    "    manual_real_df = pd.read_csv('./datasets/manual_real.csv')\n",
    "    # Ensure label is 1 for real news\n",
    "    manual_real_df['label'] = 1\n",
    "    print(f\"Loaded {len(manual_real_df)} real news articles with text from manual_real.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading manual_real.csv: {e}\")\n",
    "    manual_real_df = pd.DataFrame(columns=['title', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a4935",
   "metadata": {},
   "source": [
    "I'm using the same diverse set of data sources as in the DistilBERT evaluation to enable direct model comparisons. Consistent data is essential for making fair comparisons between different model architectures.\n",
    "\n",
    "## 4. Preparing Title-Only Dataset\n",
    "\n",
    "Now I'll prepare the title-only dataset for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create title-only dataset\n",
    "print(\"\\nPreparing title-only dataset...\")\n",
    "\n",
    "# Get the target size (number of real news titles) for balancing\n",
    "real_titles_count = len(titles_only_real_df)\n",
    "print(f\"Target size for balanced dataset: {real_titles_count} articles per class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fake news data (titles only)\n",
    "# 1. From FakeNewsNet\n",
    "if 'text' not in fake_news_net_df.columns:\n",
    "    fake_news_net_df['text'] = fake_news_net_df['title']\n",
    "else:\n",
    "    # Use only title as text\n",
    "    fake_news_net_df['text'] = fake_news_net_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. From fake_news_evaluation.csv\n",
    "fake_news_eval_titles_df = fake_news_eval_df.copy()\n",
    "fake_news_eval_titles_df['text'] = fake_news_eval_titles_df['title']  # Use only title, not full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all fake news sources (titles only)\n",
    "fake_news_title_only = pd.concat([fake_news_net_df[['text', 'label']], \n",
    "                                 fake_news_eval_titles_df[['text', 'label']]], \n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance fake news to match real news count\n",
    "# This allows us to handle growing real news dataset without manual adjustment\n",
    "if len(fake_news_title_only) > real_titles_count:\n",
    "    print(f\"Balancing fake news dataset: sampling {real_titles_count} articles from {len(fake_news_title_only)} total\")\n",
    "    # Sample randomly to match the real news count\n",
    "    fake_news_title_only = fake_news_title_only.sample(n=real_titles_count, random_state=42)\n",
    "else:\n",
    "    print(f\"Note: Not enough fake news articles ({len(fake_news_title_only)}) to match real news count ({real_titles_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a565fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare real news data (titles only)\n",
    "if 'text' not in titles_only_real_df.columns:\n",
    "    titles_only_real_df['text'] = titles_only_real_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fake and real news (titles only)\n",
    "title_only_dataset_df = pd.concat([fake_news_title_only, titles_only_real_df[['text', 'label']]], \n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to mix real and fake news\n",
    "title_only_dataset_df = title_only_dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared title-only dataset with {len(title_only_dataset_df)} articles\")\n",
    "print(f\"Class distribution: {title_only_dataset_df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "title_only_dataset = HFDataset.from_pandas(title_only_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac237d0",
   "metadata": {},
   "source": [
    "For the title-only dataset, I follow the same procedure as with DistilBERT to ensure fair comparisons. Testing RoBERTa on headline-only data is particularly interesting because:\n",
    "\n",
    "1. RoBERTa's more sophisticated encoding of context might provide advantages even with limited text\n",
    "2. The model's larger capacity might extract more nuanced features from headlines\n",
    "3. It helps quantify whether RoBERTa's additional complexity offers benefits in minimal-context scenarios\n",
    "\n",
    "## 5. Preparing Full-Text Dataset\n",
    "\n",
    "Similarly, I'll prepare the full-text dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b01b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full-text dataset\n",
    "print(\"\\nPreparing full-text dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fake news data (with full text)\n",
    "# Use fake_news_eval_df which already has text\n",
    "fake_news_full_text_df = fake_news_eval_df.copy()\n",
    "fake_news_full_text_df['text'] = fake_news_full_text_df['title'] + \" \" + fake_news_full_text_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare real news data (with full text)\n",
    "manual_real_text_df = manual_real_df.copy()\n",
    "manual_real_text_df['text'] = manual_real_text_df['title'] + \" \" + manual_real_text_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the datasets if needed\n",
    "fake_count = len(fake_news_full_text_df)\n",
    "real_count = len(manual_real_text_df)\n",
    "target_count = min(fake_count, real_count)\n",
    "\n",
    "print(f\"Full-text dataset - Fake: {fake_count}, Real: {real_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5086172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the datasets if needed\n",
    "if fake_count > real_count:\n",
    "    print(f\"Balancing full-text dataset: sampling {real_count} fake articles from {fake_count}\")\n",
    "    fake_news_full_text_df = fake_news_full_text_df.sample(n=real_count, random_state=42)\n",
    "elif real_count > fake_count:\n",
    "    print(f\"Balancing full-text dataset: sampling {fake_count} real articles from {real_count}\")\n",
    "    manual_real_text_df = manual_real_text_df.sample(n=fake_count, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22162c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fake and real news (with full text)\n",
    "full_text_dataset_df = pd.concat([fake_news_full_text_df[['text', 'label']], \n",
    "                                manual_real_text_df[['text', 'label']]], \n",
    "                                ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to mix real and fake news\n",
    "full_text_dataset_df = full_text_dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared full-text dataset with {len(full_text_dataset_df)} articles\")\n",
    "print(f\"Class distribution: {full_text_dataset_df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0373165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "full_text_dataset = HFDataset.from_pandas(full_text_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0768d",
   "metadata": {},
   "source": [
    "The full-text evaluation is where RoBERTa's capabilities might shine the most:\n",
    "\n",
    "1. Its advanced attention mechanisms should be able to better capture long-range dependencies in text\n",
    "2. The model's larger capacity might better represent complex linguistic patterns\n",
    "3. The additional pre-training data used in RoBERTa might help it better understand nuanced language\n",
    "\n",
    "## 6. Evaluation Utility Functions\n",
    "\n",
    "Next, I'll define utility functions for evaluation, adapted for RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_dataset(dataset):\n",
    "    \"\"\"Tokenize a dataset using the RoBERTa tokenizer\"\"\"\n",
    "    print(f\"Tokenizing dataset with {len(dataset)} examples...\")\n",
    "    tokenize_start_time = time.time()\n",
    "    \n",
    "    # Define tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    # Clean dataset to handle edge cases\n",
    "    def clean_dataset(example):\n",
    "        example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "        return example\n",
    "    \n",
    "    # Clean and tokenize\n",
    "    cleaned_dataset = dataset.map(clean_dataset)\n",
    "    tokenized_dataset = cleaned_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    tokenize_time = time.time() - tokenize_start_time\n",
    "    print(f\"Dataset tokenized in {tokenize_time:.2f} seconds\")\n",
    "    print(f\"Memory usage after tokenization: {get_memory_usage():.2f} MB\")\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13479f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 1: Setup\n",
    "def evaluate_model_setup(tokenized_dataset, dataset_name):\n",
    "    \"\"\"Evaluate the model on a tokenized dataset and return metrics and resource usage\"\"\"\n",
    "    print(f\"\\nEvaluating model on {dataset_name} dataset...\")\n",
    "    \n",
    "    # Reset all counters and lists\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_inference_time = 0\n",
    "    sample_count = 0\n",
    "    inference_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, \n",
    "        batch_size=8,  # Smaller batch size for RoBERTa due to larger model size\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting evaluation on {len(tokenized_dataset)} examples\")\n",
    "    \n",
    "    return eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2f4c3",
   "metadata": {},
   "source": [
    "Note that I reduced the batch size for RoBERTa to 8 (compared to 16 for DistilBERT) due to its larger memory requirements. This adjustment ensures fair evaluation while accommodating RoBERTa's greater resource needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d64a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 2: Inference loop\n",
    "def run_evaluation_loop(eval_dataloader, all_preds, all_labels, total_inference_time, \n",
    "                       sample_count, inference_times, memory_usages):\n",
    "    \"\"\"Run the evaluation loop on the provided dataloader\"\"\"\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(eval_dataloader):\n",
    "            # Track batch progress\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Processing batch {batch_idx}/{len(eval_dataloader)}\")\n",
    "            \n",
    "            # Extract batch data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Record batch size\n",
    "            current_batch_size = input_ids.size(0)\n",
    "            sample_count += current_batch_size\n",
    "            \n",
    "            # Memory tracking\n",
    "            memory_usages.append(get_memory_usage())\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            batch_inference_time = time.time() - start_time\n",
    "            inference_times.append(batch_inference_time)\n",
    "            total_inference_time += batch_inference_time\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.softmax(logits, dim=-1)\n",
    "            predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted_labels)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(f\"Evaluation complete. Total predictions: {len(all_preds)}, Total labels: {len(all_labels)}\")\n",
    "    \n",
    "    return all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 3: Metrics calculation\n",
    "def calculate_metrics(all_preds, all_labels, total_inference_time, sample_count, \n",
    "                     inference_times, memory_usages, dataset_name):\n",
    "    \"\"\"Calculate performance metrics from evaluation results\"\"\"\n",
    "    \n",
    "    if len(all_preds) == len(all_labels):\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"\\nEvaluation Results for {dataset_name} dataset:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = np.zeros((2, 2), dtype=int)\n",
    "        for true_label, pred_label in zip(all_labels, all_preds):\n",
    "            cm[true_label, pred_label] += 1\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix for {dataset_name} dataset:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Resource consumption analysis\n",
    "        print(f\"\\nResource Consumption Analysis for {dataset_name} dataset:\")\n",
    "        print(f\"Total evaluation time: {total_inference_time:.2f} seconds\")\n",
    "        print(f\"Average inference time per batch: {np.mean(inference_times):.4f} seconds\")\n",
    "        print(f\"Average inference time per sample: {total_inference_time/sample_count*1000:.2f} ms\")\n",
    "        print(f\"Peak memory usage: {max(memory_usages):.2f} MB\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'inference_time_per_sample': total_inference_time/sample_count*1000,\n",
    "            'peak_memory': max(memory_usages),\n",
    "        }\n",
    "    else:\n",
    "        print(\"ERROR: Cannot calculate metrics - prediction and label counts don't match\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f817f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 4: Visualization\n",
    "def visualize_results(metrics_dict, all_labels, all_preds, inference_times, memory_usages, dataset_name):\n",
    "    \"\"\"Create visualizations of evaluation results\"\"\"\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = metrics_dict['confusion_matrix']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(f'roberta_confusion_matrix_{dataset_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot resource usage\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inference_times)\n",
    "    plt.title(f'Inference Time per Batch (CPU) - {dataset_name}')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(memory_usages, label='System Memory')\n",
    "    plt.title(f'Memory Usage During Evaluation (CPU) - {dataset_name}')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'roberta_resource_usage_{dataset_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(f\"\\nDetailed Classification Report for {dataset_name}:\")\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Fake News', 'Real News'])\n",
    "    print(report)\n",
    "    \n",
    "    metrics_dict['classification_report'] = report\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined evaluation function\n",
    "def evaluate_model(tokenized_dataset, dataset_name):\n",
    "    \"\"\"Complete evaluation pipeline\"\"\"\n",
    "    # Setup\n",
    "    eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages = evaluate_model_setup(tokenized_dataset, dataset_name)\n",
    "    \n",
    "    # Run inference\n",
    "    all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages = run_evaluation_loop(\n",
    "        eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_dict = calculate_metrics(\n",
    "        all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages, dataset_name\n",
    "    )\n",
    "    \n",
    "    if metrics_dict:\n",
    "        # Visualize results\n",
    "        metrics_dict = visualize_results(\n",
    "            metrics_dict, all_labels, all_preds, inference_times, memory_usages, dataset_name\n",
    "        )\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9151451",
   "metadata": {},
   "source": [
    "The evaluation functions maintain the same structure as for DistilBERT, with modifications to handle RoBERTa's specific resource requirements. The careful tracking of memory and inference time is especially valuable for RoBERTa, which we expect to be more resource-intensive than both DistilBERT and TinyBERT.\n",
    "\n",
    "## 7. Evaluating Title-Only Dataset\n",
    "\n",
    "Now I'll evaluate the model on the title-only dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the title-only dataset\n",
    "title_only_tokenized = tokenize_dataset(title_only_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on title-only dataset\n",
    "title_only_results = evaluate_model(title_only_tokenized, \"Title-Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce5912",
   "metadata": {},
   "source": [
    "For RoBERTa, the title-only evaluation is particularly interesting because:\n",
    "1. It tests whether RoBERTa's advanced architecture provides benefits even with minimal context\n",
    "2. It helps quantify whether RoBERTa's additional complexity is justified for headline-only screening\n",
    "3. It provides insight into whether RoBERTa's more sophisticated language understanding can extract better signals from limited text\n",
    "\n",
    "## 8. Evaluating Full-Text Dataset\n",
    "\n",
    "Next, I'll evaluate the model on the full-text dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the full-text dataset\n",
    "full_text_tokenized = tokenize_dataset(full_text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61917278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on full-text dataset\n",
    "full_text_results = evaluate_model(full_text_tokenized, \"Full-Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41679cf1",
   "metadata": {},
   "source": [
    "The full-text evaluation for RoBERTa is expected to showcase its strengths:\n",
    "1. Its architecture was specifically designed to better handle longer text sequences\n",
    "2. The additional pre-training data used for RoBERTa should help it better understand complex news articles\n",
    "3. The increased parameter count might allow it to capture more subtle linguistic patterns that distinguish real from fake news\n",
    "\n",
    "## 9. Comparing Results Between Datasets\n",
    "\n",
    "Finally, I'll create a comparative analysis of both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if title_only_results and full_text_results:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Inference Time (ms/sample)', 'Peak Memory (MB)'],\n",
    "        'Title-Only': [\n",
    "            title_only_results['accuracy'],\n",
    "            title_only_results['precision'],\n",
    "            title_only_results['recall'],\n",
    "            title_only_results['f1'],\n",
    "            title_only_results['inference_time_per_sample'],\n",
    "            title_only_results['peak_memory']\n",
    "        ],\n",
    "        'Full-Text': [\n",
    "            full_text_results['accuracy'],\n",
    "            full_text_results['precision'],\n",
    "            full_text_results['recall'],\n",
    "            full_text_results['f1'],\n",
    "            full_text_results['inference_time_per_sample'],\n",
    "            full_text_results['peak_memory']\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and display comparison table\n",
    "comparison_df['Title-Only'] = comparison_df['Title-Only'].apply(\n",
    "    lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) and x < 100 else f\"{x:.2f}\")\n",
    "comparison_df['Full-Text'] = comparison_df['Full-Text'].apply(\n",
    "    lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) and x < 100 else f\"{x:.2f}\")\n",
    "\n",
    "print(\"Performance Comparison Between Datasets:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dab7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of metrics comparison\n",
    "metrics = comparison_df.iloc[:4]  # Just the first 4 metrics (accuracy, precision, recall, f1)\n",
    "\n",
    "# Convert to numeric for plotting\n",
    "metrics['Title-Only'] = metrics['Title-Only'].astype(float)\n",
    "metrics['Full-Text'] = metrics['Full-Text'].astype(float)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(metrics))\n",
    "\n",
    "plt.bar(index, metrics['Title-Only'], bar_width, label='Title-Only')\n",
    "plt.bar(index + bar_width, metrics['Full-Text'], bar_width, label='Full-Text')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('RoBERTa Performance Comparison: Title-Only vs Full-Text')\n",
    "plt.xticks(index + bar_width / 2, metrics['Metric'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('roberta_performance_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83f267",
   "metadata": {},
   "source": [
    "This comparative analysis addresses several key questions:\n",
    "1. Does RoBERTa's more sophisticated architecture provide significant benefits over simpler models?\n",
    "2. Is the increase in computational requirements justified by improved performance?\n",
    "3. How does the performance gap between title-only and full-text approaches compare to other models?\n",
    "4. Does RoBERTa's additional complexity translate to better handling of limited context?\n",
    "\n",
    "## 10. Conclusion and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model\n",
    "gc.collect()\n",
    "print(f\"Final memory usage: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864241a",
   "metadata": {},
   "source": [
    "The table and visualization above provide a clear comparison between using only titles versus full text for fake news detection with RoBERTa. Key findings include:\n",
    "\n",
    "1. **Performance Characteristics**: \n",
    "   - RoBERTa, with its larger size and more sophisticated architecture, is expected to achieve higher accuracy than DistilBERT and TinyBERT, particularly on full-text articles\n",
    "   - The performance gap between title-only and full-text approaches provides insight into how effectively RoBERTa leverages additional context\n",
    "\n",
    "2. **Resource Requirements**:\n",
    "   - RoBERTa demonstrates significantly higher memory usage and longer inference times compared to lighter models like TinyBERT and DistilBERT\n",
    "   - The specific resource metrics help determine whether RoBERTa is viable for different deployment scenarios\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - For high-stakes applications where maximum accuracy is critical and computational resources are abundant, RoBERTa may be the preferred choice\n",
    "   - For resource-constrained environments, the trade-off between RoBERTa's potentially higher accuracy and its increased computational demands must be carefully considered\n",
    "\n",
    "RoBERTa represents the upper end of the performance-efficiency spectrum in our comparative evaluation of transformer models for fake news detection. Its advanced architecture and additional pre-training make it a strong candidate for scenarios where accuracy is paramount, but its resource requirements may limit its applicability in edge or mobile environments.\n",
    "\n",
    "By directly comparing RoBERTa, DistilBERT, and TinyBERT across multiple metrics and evaluation scenarios, we gain comprehensive insights into the performance-efficiency trade-offs for fake news detection, enabling informed model selection based on specific deployment constraints and requirements."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
