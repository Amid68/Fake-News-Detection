{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b304e",
   "metadata": {},
   "source": [
    "# MobileBERT Evaluation on ISOT Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook evaluates a fine-tuned MobileBERT model on two distinct fake news detection scenarios:\n",
    "1. **Titles-only dataset**: Using only article headlines\n",
    "2. **Full-text dataset**: Using complete articles with both titles and text\n",
    "\n",
    "This dual evaluation approach provides insights into how well MobileBERT performs with limited context versus full article context. MobileBERT was specifically designed for mobile applications, potentially offering the best balance between accuracy and efficiency for resource-constrained environments compared to other models in our comparative study.\n",
    "\n",
    "## 1. Setting Up the Environment\n",
    "\n",
    "Let's start by importing all necessary libraries and setting up utility functions to monitor resource usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021fbba",
   "metadata": {},
   "source": [
    "These libraries provide the foundation for data manipulation, model evaluation, visualization, and resource monitoring - especially important for understanding MobileBERT's performance in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - using CPU for edge device testing\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054b5b2",
   "metadata": {},
   "source": [
    "I deliberately choose to use the CPU rather than GPU for this evaluation because:\n",
    "1. The primary target for MobileBERT deployment is mobile and edge devices that typically lack dedicated GPUs\n",
    "2. CPU performance metrics provide a more realistic assessment of real-world deployment scenarios\n",
    "3. This allows for direct comparison with other lightweight models in similar deployment conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "print(f\"Starting memory usage: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2007c",
   "metadata": {},
   "source": [
    "This utility function is particularly valuable for MobileBERT, as it helps quantify the model's memory efficiency - a primary consideration for its target deployment scenarios on mobile devices.\n",
    "\n",
    "## 2. Loading the Pre-trained Model\n",
    "\n",
    "Now I'll load the MobileBERT model that was previously fine-tuned on the ISOT dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2461bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained MobileBERT model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = \"../ml_models/mobilebert-fake-news-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "start_time = time.time()\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(model_path)\n",
    "model = MobileBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # Move to CPU\n",
    "load_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da3dbf",
   "metadata": {},
   "source": [
    "Model loading time and memory footprint are critical metrics for MobileBERT, as they directly impact the model's suitability for mobile and edge environments where startup time and memory constraints are significant concerns.\n",
    "\n",
    "## 3. Data Loading and Preparation\n",
    "\n",
    "Next, I'll load various data sources for our evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab222418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data sources\n",
    "print(\"\\nLoading all data sources...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c75fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load titles_only_real.csv (real news, titles only)\n",
    "try:\n",
    "    titles_only_real_df = pd.read_csv('../data/titles_only_real.csv')\n",
    "    # Ensure label is 1 for real news\n",
    "    titles_only_real_df['label'] = 1\n",
    "    print(f\"Loaded {len(titles_only_real_df)} real news titles from titles_only_real.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading titles_only_real.csv: {e}\")\n",
    "    titles_only_real_df = pd.DataFrame(columns=['title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d13dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load FakeNewsNet (fake news only)\n",
    "try:\n",
    "    fake_news_net_df = pd.read_csv('./datasets/simplified_FakeNewsNet.csv')\n",
    "    # Keep only fake news\n",
    "    fake_news_net_df = fake_news_net_df[fake_news_net_df['label'] == 0]\n",
    "    print(f\"Loaded {len(fake_news_net_df)} fake news articles from FakeNewsNet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FakeNewsNet: {e}\")\n",
    "    fake_news_net_df = pd.DataFrame(columns=['title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b775766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load fake_news_evaluation.csv (fake news with text)\n",
    "try:\n",
    "    fake_news_eval_df = pd.read_csv('./datasets/fake_news_evaluation.csv')\n",
    "    # Ensure label is 0 for fake news\n",
    "    fake_news_eval_df['label'] = 0\n",
    "    print(f\"Loaded {len(fake_news_eval_df)} fake news articles from fake_news_evaluation.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading fake_news_evaluation.csv: {e}\")\n",
    "    fake_news_eval_df = pd.DataFrame(columns=['title', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1773d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load manual_real.csv (real news with text)\n",
    "try:\n",
    "    manual_real_df = pd.read_csv('./datasets/manual_real.csv')\n",
    "    # Ensure label is 1 for real news\n",
    "    manual_real_df['label'] = 1\n",
    "    print(f\"Loaded {len(manual_real_df)} real news articles with text from manual_real.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading manual_real.csv: {e}\")\n",
    "    manual_real_df = pd.DataFrame(columns=['title', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322cec6",
   "metadata": {},
   "source": [
    "I'm using the same diverse set of data sources as in the other model evaluations to ensure fair comparisons across our entire model suite.\n",
    "\n",
    "## 4. Preparing Title-Only Dataset\n",
    "\n",
    "Now I'll prepare the title-only dataset for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ac9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create title-only dataset\n",
    "print(\"\\nPreparing title-only dataset...\")\n",
    "\n",
    "# Get the target size (number of real news titles) for balancing\n",
    "real_titles_count = len(titles_only_real_df)\n",
    "print(f\"Target size for balanced dataset: {real_titles_count} articles per class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fake news data (titles only)\n",
    "# 1. From FakeNewsNet\n",
    "if 'text' not in fake_news_net_df.columns:\n",
    "    fake_news_net_df['text'] = fake_news_net_df['title']\n",
    "else:\n",
    "    # Use only title as text\n",
    "    fake_news_net_df['text'] = fake_news_net_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. From fake_news_evaluation.csv\n",
    "fake_news_eval_titles_df = fake_news_eval_df.copy()\n",
    "fake_news_eval_titles_df['text'] = fake_news_eval_titles_df['title']  # Use only title, not full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df799d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all fake news sources (titles only)\n",
    "fake_news_title_only = pd.concat([fake_news_net_df[['text', 'label']], \n",
    "                                 fake_news_eval_titles_df[['text', 'label']]], \n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance fake news to match real news count\n",
    "# This allows us to handle growing real news dataset without manual adjustment\n",
    "if len(fake_news_title_only) > real_titles_count:\n",
    "    print(f\"Balancing fake news dataset: sampling {real_titles_count} articles from {len(fake_news_title_only)} total\")\n",
    "    # Sample randomly to match the real news count\n",
    "    fake_news_title_only = fake_news_title_only.sample(n=real_titles_count, random_state=42)\n",
    "else:\n",
    "    print(f\"Note: Not enough fake news articles ({len(fake_news_title_only)}) to match real news count ({real_titles_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare real news data (titles only)\n",
    "if 'text' not in titles_only_real_df.columns:\n",
    "    titles_only_real_df['text'] = titles_only_real_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab53f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fake and real news (titles only)\n",
    "title_only_dataset_df = pd.concat([fake_news_title_only, titles_only_real_df[['text', 'label']]], \n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to mix real and fake news\n",
    "title_only_dataset_df = title_only_dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared title-only dataset with {len(title_only_dataset_df)} articles\")\n",
    "print(f\"Class distribution: {title_only_dataset_df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "title_only_dataset = HFDataset.from_pandas(title_only_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53e5e4",
   "metadata": {},
   "source": [
    "Testing MobileBERT on headline-only data is particularly valuable because:\n",
    "1. The model's compact architecture was specifically designed for efficient processing on mobile devices\n",
    "2. Headline-only classification represents a common use case for mobile applications where processing speed is critical\n",
    "3. This evaluation helps determine if MobileBERT's optimizations maintain performance even with limited context\n",
    "\n",
    "## 5. Preparing Full-Text Dataset\n",
    "\n",
    "Similarly, I'll prepare the full-text dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full-text dataset\n",
    "print(\"\\nPreparing full-text dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76da3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fake news data (with full text)\n",
    "# Use fake_news_eval_df which already has text\n",
    "fake_news_full_text_df = fake_news_eval_df.copy()\n",
    "fake_news_full_text_df['text'] = fake_news_full_text_df['title'] + \" \" + fake_news_full_text_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare real news data (with full text)\n",
    "manual_real_text_df = manual_real_df.copy()\n",
    "manual_real_text_df['text'] = manual_real_text_df['title'] + \" \" + manual_real_text_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the datasets if needed\n",
    "fake_count = len(fake_news_full_text_df)\n",
    "real_count = len(manual_real_text_df)\n",
    "target_count = min(fake_count, real_count)\n",
    "\n",
    "print(f\"Full-text dataset - Fake: {fake_count}, Real: {real_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc088b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the datasets if needed\n",
    "if fake_count > real_count:\n",
    "    print(f\"Balancing full-text dataset: sampling {real_count} fake articles from {fake_count}\")\n",
    "    fake_news_full_text_df = fake_news_full_text_df.sample(n=real_count, random_state=42)\n",
    "elif real_count > fake_count:\n",
    "    print(f\"Balancing full-text dataset: sampling {fake_count} real articles from {real_count}\")\n",
    "    manual_real_text_df = manual_real_text_df.sample(n=fake_count, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fake and real news (with full text)\n",
    "full_text_dataset_df = pd.concat([fake_news_full_text_df[['text', 'label']], \n",
    "                                manual_real_text_df[['text', 'label']]], \n",
    "                                ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36994b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to mix real and fake news\n",
    "full_text_dataset_df = full_text_dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6717d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared full-text dataset with {len(full_text_dataset_df)} articles\")\n",
    "print(f\"Class distribution: {full_text_dataset_df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "full_text_dataset = HFDataset.from_pandas(full_text_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea3bf0",
   "metadata": {},
   "source": [
    "The full-text evaluation is important for understanding:\n",
    "1. How well MobileBERT handles longer sequences despite its streamlined architecture\n",
    "2. Whether the model's optimizations compromise its ability to capture long-range dependencies\n",
    "3. If the more comprehensive context provides substantial performance improvements that might justify the additional processing time\n",
    "\n",
    "## 6. Evaluation Utility Functions\n",
    "\n",
    "Next, I'll define utility functions for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_dataset(dataset):\n",
    "    \"\"\"Tokenize a dataset using the MobileBERT tokenizer\"\"\"\n",
    "    print(f\"Tokenizing dataset with {len(dataset)} examples...\")\n",
    "    tokenize_start_time = time.time()\n",
    "    \n",
    "    # Define tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    # Clean dataset to handle edge cases\n",
    "    def clean_dataset(example):\n",
    "        example['text'] = str(example['text']) if example['text'] is not None else \"\"\n",
    "        return example\n",
    "    \n",
    "    # Clean and tokenize\n",
    "    cleaned_dataset = dataset.map(clean_dataset)\n",
    "    tokenized_dataset = cleaned_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    tokenize_time = time.time() - tokenize_start_time\n",
    "    print(f\"Dataset tokenized in {tokenize_time:.2f} seconds\")\n",
    "    print(f\"Memory usage after tokenization: {get_memory_usage():.2f} MB\")\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a80d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 1: Setup\n",
    "def evaluate_model_setup(tokenized_dataset, dataset_name):\n",
    "    \"\"\"Evaluate the model on a tokenized dataset and return metrics and resource usage\"\"\"\n",
    "    print(f\"\\nEvaluating model on {dataset_name} dataset...\")\n",
    "    \n",
    "    # Reset all counters and lists\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_inference_time = 0\n",
    "    sample_count = 0\n",
    "    inference_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, \n",
    "        batch_size=16,  # Appropriate batch size for CPU\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting evaluation on {len(tokenized_dataset)} examples\")\n",
    "    \n",
    "    return eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daafa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 2: Inference loop\n",
    "def run_evaluation_loop(eval_dataloader, all_preds, all_labels, total_inference_time, \n",
    "                       sample_count, inference_times, memory_usages):\n",
    "    \"\"\"Run the evaluation loop on the provided dataloader\"\"\"\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(eval_dataloader):\n",
    "            # Track batch progress\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Processing batch {batch_idx}/{len(eval_dataloader)}\")\n",
    "            \n",
    "            # Extract batch data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Record batch size\n",
    "            current_batch_size = input_ids.size(0)\n",
    "            sample_count += current_batch_size\n",
    "            \n",
    "            # Memory tracking\n",
    "            memory_usages.append(get_memory_usage())\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            batch_inference_time = time.time() - start_time\n",
    "            inference_times.append(batch_inference_time)\n",
    "            total_inference_time += batch_inference_time\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.softmax(logits, dim=-1)\n",
    "            predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted_labels)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(f\"Evaluation complete. Total predictions: {len(all_preds)}, Total labels: {len(all_labels)}\")\n",
    "    \n",
    "    return all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039892a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 3: Metrics calculation\n",
    "def calculate_metrics(all_preds, all_labels, total_inference_time, sample_count, \n",
    "                     inference_times, memory_usages, dataset_name):\n",
    "    \"\"\"Calculate performance metrics from evaluation results\"\"\"\n",
    "    \n",
    "    if len(all_preds) == len(all_labels):\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"\\nEvaluation Results for {dataset_name} dataset:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = np.zeros((2, 2), dtype=int)\n",
    "        for true_label, pred_label in zip(all_labels, all_preds):\n",
    "            cm[true_label, pred_label] += 1\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix for {dataset_name} dataset:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Resource consumption analysis\n",
    "        print(f\"\\nResource Consumption Analysis for {dataset_name} dataset:\")\n",
    "        print(f\"Total evaluation time: {total_inference_time:.2f} seconds\")\n",
    "        print(f\"Average inference time per batch: {np.mean(inference_times):.4f} seconds\")\n",
    "        print(f\"Average inference time per sample: {total_inference_time/sample_count*1000:.2f} ms\")\n",
    "        print(f\"Peak memory usage: {max(memory_usages):.2f} MB\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'inference_time_per_sample': total_inference_time/sample_count*1000,\n",
    "            'peak_memory': max(memory_usages),\n",
    "        }\n",
    "    else:\n",
    "        print(\"ERROR: Cannot calculate metrics - prediction and label counts don't match\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbacd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function - Part 4: Visualization\n",
    "def visualize_results(metrics_dict, all_labels, all_preds, inference_times, memory_usages, dataset_name):\n",
    "    \"\"\"Create visualizations of evaluation results\"\"\"\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = metrics_dict['confusion_matrix']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(f'mobilebert_confusion_matrix_{dataset_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot resource usage\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inference_times)\n",
    "    plt.title(f'Inference Time per Batch (CPU) - {dataset_name}')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(memory_usages, label='System Memory')\n",
    "    plt.title(f'Memory Usage During Evaluation (CPU) - {dataset_name}')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Memory (MB)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'mobilebert_resource_usage_{dataset_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(f\"\\nDetailed Classification Report for {dataset_name}:\")\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Fake News', 'Real News'])\n",
    "    print(report)\n",
    "    \n",
    "    metrics_dict['classification_report'] = report\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined evaluation function\n",
    "def evaluate_model(tokenized_dataset, dataset_name):\n",
    "    \"\"\"Complete evaluation pipeline\"\"\"\n",
    "    # Setup\n",
    "    eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages = evaluate_model_setup(tokenized_dataset, dataset_name)\n",
    "    \n",
    "    # Run inference\n",
    "    all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages = run_evaluation_loop(\n",
    "        eval_dataloader, all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_dict = calculate_metrics(\n",
    "        all_preds, all_labels, total_inference_time, sample_count, inference_times, memory_usages, dataset_name\n",
    "    )\n",
    "    \n",
    "    if metrics_dict:\n",
    "        # Visualize results\n",
    "        metrics_dict = visualize_results(\n",
    "            metrics_dict, all_labels, all_preds, inference_times, memory_usages, dataset_name\n",
    "        )\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bcdfd",
   "metadata": {},
   "source": [
    "These evaluation functions are structured to provide comprehensive insights into MobileBERT's performance and resource usage. The detailed tracking of memory consumption and inference time is particularly important for MobileBERT's target deployment scenarios on resource-constrained devices.\n",
    "\n",
    "## 7. Evaluating Title-Only Dataset\n",
    "\n",
    "Now I'll evaluate the model on the title-only dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the title-only dataset\n",
    "title_only_tokenized = tokenize_dataset(title_only_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on title-only dataset\n",
    "title_only_results = evaluate_model(title_only_tokenized, \"Title-Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bf158",
   "metadata": {},
   "source": [
    "For MobileBERT, the title-only evaluation is particularly relevant for mobile applications where:\n",
    "1. Processing speed is a critical consideration for user experience\n",
    "2. Battery life and resource consumption need to be minimized\n",
    "3. Users may want quick preliminary assessments before deciding to read full articles\n",
    "\n",
    "## 8. Evaluating Full-Text Dataset\n",
    "\n",
    "Next, I'll evaluate the model on the full-text dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea087cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the full-text dataset\n",
    "full_text_tokenized = tokenize_dataset(full_text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on full-text dataset\n",
    "full_text_results = evaluate_model(full_text_tokenized, \"Full-Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20822fa8",
   "metadata": {},
   "source": [
    "The full-text evaluation helps determine:\n",
    "1. Whether MobileBERT's efficiency optimizations maintain performance for longer text\n",
    "2. If the model's bottleneck architecture adequately captures the complex linguistic patterns in full articles\n",
    "3. How the performance-efficiency trade-off compares to both smaller models like TinyBERT and larger models like DistilBERT and RoBERTa\n",
    "\n",
    "## 9. Comparing Results Between Datasets\n",
    "\n",
    "Finally, I'll create a comparative analysis of both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if title_only_results and full_text_results:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Inference Time (ms/sample)', 'Peak Memory (MB)'],\n",
    "        'Title-Only': [\n",
    "            title_only_results['accuracy'],\n",
    "            title_only_results['precision'],\n",
    "            title_only_results['recall'],\n",
    "            title_only_results['f1'],\n",
    "            title_only_results['inference_time_per_sample'],\n",
    "            title_only_results['peak_memory']\n",
    "        ],\n",
    "        'Full-Text': [\n",
    "            full_text_results['accuracy'],\n",
    "            full_text_results['precision'],\n",
    "            full_text_results['recall'],\n",
    "            full_text_results['f1'],\n",
    "            full_text_results['inference_time_per_sample'],\n",
    "            full_text_results['peak_memory']\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and display comparison table\n",
    "comparison_df['Title-Only'] = comparison_df['Title-Only'].apply(\n",
    "    lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) and x < 100 else f\"{x:.2f}\")\n",
    "comparison_df['Full-Text'] = comparison_df['Full-Text'].apply(\n",
    "    lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) and x < 100 else f\"{x:.2f}\")\n",
    "\n",
    "print(\"Performance Comparison Between Datasets:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of metrics comparison\n",
    "metrics = comparison_df.iloc[:4]  # Just the first 4 metrics (accuracy, precision, recall, f1)\n",
    "\n",
    "# Convert to numeric for plotting\n",
    "metrics['Title-Only'] = metrics['Title-Only'].astype(float)\n",
    "metrics['Full-Text'] = metrics['Full-Text'].astype(float)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(metrics))\n",
    "\n",
    "plt.bar(index, metrics['Title-Only'], bar_width, label='Title-Only')\n",
    "plt.bar(index + bar_width, metrics['Full-Text'], bar_width, label='Full-Text')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('MobileBERT Performance Comparison: Title-Only vs Full-Text')\n",
    "plt.xticks(index + bar_width / 2, metrics['Metric'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('mobilebert_performance_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7f078",
   "metadata": {},
   "source": [
    "This comparative analysis is particularly valuable for mobile applications, where developers must carefully balance performance with resource consumption. The results will help determine whether the additional context from full articles justifies the increased processing time and memory usage on mobile devices.\n",
    "\n",
    "## 10. Conclusion and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model\n",
    "gc.collect()\n",
    "print(f\"Final memory usage: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ea08a",
   "metadata": {},
   "source": [
    "The table and visualization above provide a clear comparison between using only titles versus full text for fake news detection with MobileBERT. Key findings include:\n",
    "\n",
    "1. **Performance Characteristics**: \n",
    "   - MobileBERT demonstrates how effectively a model optimized for mobile devices can perform on fake news detection tasks\n",
    "   - The performance gap between title-only and full-text approaches helps quantify the value of additional context within the constraints of mobile deployment\n",
    "\n",
    "2. **Resource Efficiency**:\n",
    "   - The memory footprint and inference time metrics directly relate to MobileBERT's primary design goals of mobile optimization\n",
    "   - These metrics help determine whether MobileBERT achieves its intended efficiency targets while maintaining competitive accuracy\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - For mobile applications where battery life and user experience are critical, the title-only approach may provide the best balance of accuracy and efficiency\n",
    "   - For more critical applications where maximum accuracy is required, the full-text approach with MobileBERT may offer a good compromise between performance and resource consumption\n",
    "\n",
    "When compared to TinyBERT, DistilBERT, and RoBERTa, these results provide valuable insights into which model offers the best balance of performance and efficiency for different deployment scenarios. MobileBERT's performance relative to its resource consumption will determine whether its mobile-specific optimizations make it the preferred choice for resource-constrained environments compared to more general model compression approaches."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
