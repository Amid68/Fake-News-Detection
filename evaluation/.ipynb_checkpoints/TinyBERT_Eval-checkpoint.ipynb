{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b124e131",
   "metadata": {},
   "source": [
    "# TinyBERT Evaluation on ISOT Dataset\n",
    "\n",
    "In this notebook, I'll evaluate my fine-tuned TinyBERT model on the same evaluation dataset. My goal is to understand how well this smaller model performs compared to DistilBERT and analyze its resource consumption, especially for CPU-based edge deployment on my laptop. TinyBERT is an even more compact model than DistilBERT, which could make it more suitable for resource-constrained environments.\n",
    "\n",
    "## 1. Setting Up My Environment\n",
    "\n",
    "First, I'll import all necessary libraries and set up utility functions to monitor resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - using CPU for edge device testing\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada630f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2aa99",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing ISOT Evaluation Dataset\n",
    "\n",
    "Now I'll load the same evaluation dataset that I used for DistilBERT to enable direct comparison between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage before loading dataset\n",
    "print(f\"Memory usage before loading dataset: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real and fake news datasets\n",
    "real_news_df = pd.read_csv('./datasets/manual_real.csv')\n",
    "fake_news_df = pd.read_csv('./datasets/fake_news_evaluation.csv')\n",
    "print(f\"Loaded {len(real_news_df)} real news articles and {len(fake_news_df)} fake news articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the real news data\n",
    "real_news_df['text'] = real_news_df['title'] + \" \" + real_news_df['text'].fillna('')\n",
    "real_news_df['label'] = 1  # 1 for real news\n",
    "real_news_clean = real_news_df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the fake news data\n",
    "fake_news_df['text'] = fake_news_df['title'] + \" \" + fake_news_df['text'].fillna('')\n",
    "fake_news_df['label'] = 0  # 0 for fake news\n",
    "fake_news_clean = fake_news_df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad55410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "combined_eval = pd.concat([real_news_clean, fake_news_clean], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to mix real and fake news\n",
    "combined_eval = combined_eval.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared evaluation dataset with {len(combined_eval)} articles\")\n",
    "print(f\"Class distribution: {combined_eval['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace dataset format\n",
    "combined_eval = HFDataset.from_pandas(combined_eval)\n",
    "print(f\"Memory usage after loading dataset: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04adec2",
   "metadata": {},
   "source": [
    "## 3. Loading My Pre-trained Model\n",
    "\n",
    "I'll now load the TinyBERT model that I previously fine-tuned. For edge deployment, I'm particularly interested in the model's loading time and memory footprint on CPU, which should be lower than DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be736049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained TinyBERT model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = \"../ml_models/tinybert-fake-news-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # This will be CPU\n",
    "load_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea59089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d22ff5",
   "metadata": {},
   "source": [
    "## 4. Tokenizing the Dataset\n",
    "\n",
    "Before I can run the model on my data, I need to tokenize it using the same tokenizer that was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "tokenize_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf40c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d15345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "tokenized_dataset = combined_eval.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d27fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_time = time.time() - tokenize_start_time\n",
    "print(f\"Dataset tokenized in {tokenize_time:.2f} seconds\")\n",
    "print(f\"Memory usage after tokenization: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30672f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset format check\n",
    "print(\"\\nDataset format check:\")\n",
    "print(f\"Dataset features: {tokenized_dataset.features}\")\n",
    "print(f\"First example keys: {tokenized_dataset[0].keys()}\")\n",
    "\n",
    "# Check that all examples have labels\n",
    "labels_count = sum(1 for example in tokenized_dataset if 'label' in example)\n",
    "print(f\"Examples with labels: {labels_count} out of {len(tokenized_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed71565",
   "metadata": {},
   "source": [
    "## 5. Running Model Evaluation\n",
    "\n",
    "Now comes the main part - evaluating my TinyBERT model's performance on the evaluation dataset. Since I'm targeting edge devices, I'll pay special attention to inference speed and memory usage on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "\n",
    "# Reset all counters and lists\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "total_inference_time = 0\n",
    "sample_count = 0\n",
    "inference_times = []\n",
    "memory_usages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh DataLoader with shuffle=False to ensure deterministic order\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset, \n",
    "    batch_size=16,  # Smaller batch size for CPU\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec97d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting evaluation on {len(tokenized_dataset)} examples\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(eval_dataloader):\n",
    "        # Track batch progress\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"Processing batch {batch_idx}/{len(eval_dataloader)}\")\n",
    "        \n",
    "        # Extract batch data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Record batch size\n",
    "        current_batch_size = input_ids.size(0)\n",
    "        sample_count += current_batch_size\n",
    "        \n",
    "        # Memory tracking\n",
    "        memory_usages.append(get_memory_usage())\n",
    "        \n",
    "        # Time the inference\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_inference_time = time.time() - start_time\n",
    "        inference_times.append(batch_inference_time)\n",
    "        total_inference_time += batch_inference_time\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.softmax(logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_preds.extend(predicted_labels)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Sanity check\n",
    "        if len(all_preds) != len(all_labels):\n",
    "            print(f\"WARNING: After batch {batch_idx}, preds={len(all_preds)} but labels={len(all_labels)}\")\n",
    "\n",
    "# Verify final counts match\n",
    "print(f\"Evaluation complete. Total predictions: {len(all_preds)}, Total labels: {len(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c745d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics if counts match\n",
    "if len(all_preds) == len(all_labels):\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "else:\n",
    "    print(\"ERROR: Cannot calculate metrics - prediction and label counts don't match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3421717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = np.zeros((2, 2), dtype=int)\n",
    "for true_label, pred_label in zip(all_labels, all_preds):\n",
    "    cm[true_label, pred_label] += 1\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46db07",
   "metadata": {},
   "source": [
    "## 6. Analyzing Resource Consumption\n",
    "\n",
    "Since I'm targeting edge devices, I'll focus on CPU-specific metrics like memory usage and inference time to determine if TinyBERT is more suitable than DistilBERT for edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156bbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource consumption analysis\n",
    "print(\"\\nResource Consumption Analysis for Edge Deployment:\")\n",
    "print(f\"Total evaluation time: {total_inference_time:.2f} seconds\")\n",
    "print(f\"Average inference time per batch: {np.mean(inference_times):.4f} seconds\")\n",
    "print(f\"Average inference time per sample: {total_inference_time/sample_count*1000:.2f} ms\")\n",
    "print(f\"Peak memory usage: {max(memory_usages):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resource usage\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(inference_times)\n",
    "plt.title('Inference Time per Batch (CPU)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ac63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(memory_usages, label='System Memory')\n",
    "plt.title('Memory Usage During Evaluation (CPU)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig('tinybert_resource_usage_cpu.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9943",
   "metadata": {},
   "source": [
    "## 7. Detailed Classification Analysis\n",
    "\n",
    "Finally, I'll generate a detailed classification report and visualize the confusion matrix to better understand where my TinyBERT model performs well and where it struggles on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('tinybert_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a54cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12aa7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, I've evaluated my TinyBERT model on the ISOT evaluation dataset specifically focusing on CPU performance for edge deployment. \n",
    "\n",
    "TinyBERT is a more compact version of BERT with fewer parameters than DistilBERT, which should lead to faster inference and lower memory requirements. This makes it potentially more suitable for resource-constrained edge devices.\n",
    "\n",
    "The metrics I've gathered are crucial for determining if this model could run effectively on edge devices like my laptop. For edge deployment, I'm particularly interested in:\n",
    "\n",
    "1. Memory footprint - How much RAM does the model require compared to DistilBERT?\n",
    "2. Inference speed - Is it faster than DistilBERT for real-time applications?\n",
    "3. Model loading time - Is the startup time better than DistilBERT?\n",
    "4. Classification performance - Does the smaller model maintain comparable accuracy?\n",
    "\n",
    "The resource consumption analysis gives me a clear picture of what kind of hardware requirements I would need for deploying this model on edge devices. By comparing these results with my DistilBERT evaluation, I can determine which model offers the best balance of performance and efficiency for edge deployment scenarios."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
