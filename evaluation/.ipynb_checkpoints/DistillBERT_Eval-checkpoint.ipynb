{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe869e3",
   "metadata": {},
   "source": [
    "# DistilBERT Evaluation on FakeNewsNet Dataset\n",
    "\n",
    "In this notebook, I'll evaluate my fine-tuned DistilBERT model on the FakeNewsNet dataset. My goal is to understand how well the model performs on this new dataset and analyze its resource consumption, especially for CPU-based edge deployment on my laptop.\n",
    "\n",
    "## 1. Setting Up My Environment\n",
    "\n",
    "First, I'll import all necessary libraries and set up utility functions to monitor resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ecfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e527b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - using CPU for edge device testing\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a87042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0c1da",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing FakeNewsNet Data\n",
    "\n",
    "Now I'll load the FakeNewsNet dataset. I'm keeping this simple by only using the essential columns: title, text, and the label. I'll combine the title and text just like I did when training on the ISOT dataset to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17009815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage before loading dataset\n",
    "print(f\"Memory usage before loading dataset: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b747ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FakeNewsNet dataset (update path as needed)\n",
    "fakenewsnet_df = pd.read_csv('path_to_fakenewsnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only essential columns\n",
    "fakenewsnet_clean = fakenewsnet_df[['title', 'text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28679143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text\n",
    "fakenewsnet_clean['text'] = fakenewsnet_clean['title'] + \" \" + fakenewsnet_clean['text'].fillna('')\n",
    "\n",
    "# Handle missing values and drop the now redundant title column\n",
    "fakenewsnet_clean = fakenewsnet_clean.dropna(subset=['text'])\n",
    "fakenewsnet_clean = fakenewsnet_clean[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10be371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace dataset format\n",
    "fakenewsnet_dataset = HFDataset.from_pandas(fakenewsnet_clean)\n",
    "\n",
    "print(f\"Dataset prepared with {len(fakenewsnet_dataset)} examples\")\n",
    "print(f\"Memory usage after loading dataset: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5febf",
   "metadata": {},
   "source": [
    "## 3. Loading My Pre-trained Model\n",
    "\n",
    "I'll now load the DistilBERT model that I previously fine-tuned on the ISOT dataset. For edge deployment, I'm particularly interested in the model's loading time and memory footprint on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe28868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained DistilBERT model\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = \"../ml_models/distilbert-fake-news-detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)  # This will be CPU\n",
    "load_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c54003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Memory usage after loading model: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d48e4",
   "metadata": {},
   "source": [
    "## 4. Tokenizing the Dataset\n",
    "\n",
    "Before I can run the model on my data, I need to tokenize it using the same tokenizer that was used during training. This step converts the text into the numerical format that the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4428bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "\n",
    "tokenize_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad634271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "tokenized_dataset = fakenewsnet_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_time = time.time() - tokenize_start_time\n",
    "print(f\"Dataset tokenized in {tokenize_time:.2f} seconds\")\n",
    "print(f\"Memory usage after tokenization: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd2086",
   "metadata": {},
   "source": [
    "## 5. Running Model Evaluation\n",
    "\n",
    "Now comes the main part - evaluating my model's performance on the FakeNewsNet dataset. Since I'm targeting edge devices, I'll pay special attention to inference speed and memory usage on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "\n",
    "# Set batch size for evaluation - smaller for CPU\n",
    "batch_size = 16  # Using smaller batch size for CPU\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "total_inference_time = 0\n",
    "sample_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555cb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batched processing\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track memory and time metrics\n",
    "inference_times = []\n",
    "memory_usages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        batch_size = len(batch['input_ids'])\n",
    "        sample_count += batch_size\n",
    "        \n",
    "        # Move batch to device (CPU)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        labels = batch.pop('label').to(device) if 'label' in batch else None\n",
    "        \n",
    "        # Track memory before inference\n",
    "        memory_usages.append(get_memory_usage())\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        outputs = model(**batch)\n",
    "        batch_inference_time = time.time() - start_time\n",
    "        inference_times.append(batch_inference_time)\n",
    "        \n",
    "        total_inference_time += batch_inference_time\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_labels = predictions.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(pred_labels)\n",
    "        if labels is not None:\n",
    "            all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7672947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a892e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = np.zeros((2, 2), dtype=int)\n",
    "for true_label, pred_label in zip(all_labels, all_preds):\n",
    "    cm[true_label, pred_label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d531a7",
   "metadata": {},
   "source": [
    "## 6. Analyzing Resource Consumption\n",
    "\n",
    "Since I'm targeting edge devices, I'll focus on CPU-specific metrics like memory usage and inference time to determine if this model is suitable for edge deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ddbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource consumption analysis\n",
    "print(\"\\nResource Consumption Analysis for Edge Deployment:\")\n",
    "print(f\"Total evaluation time: {total_inference_time:.2f} seconds\")\n",
    "print(f\"Average inference time per batch: {np.mean(inference_times):.4f} seconds\")\n",
    "print(f\"Average inference time per sample: {total_inference_time/sample_count*1000:.2f} ms\")\n",
    "print(f\"Peak memory usage: {max(memory_usages):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resource usage\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(inference_times)\n",
    "plt.title('Inference Time per Batch (CPU)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf64676",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(memory_usages, label='System Memory')\n",
    "plt.title('Memory Usage During Evaluation (CPU)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33262453",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig('distilbert_resource_usage_cpu.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd2ed3",
   "metadata": {},
   "source": [
    "## 7. Detailed Classification Analysis\n",
    "\n",
    "Finally, I'll generate a detailed classification report and visualize the confusion matrix to better understand where my model performs well and where it struggles on this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce204b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('distilbert_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca82fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715334c5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, I've evaluated my DistilBERT model on the FakeNewsNet dataset specifically focusing on CPU performance for edge deployment. \n",
    "\n",
    "The metrics I've gathered are crucial for determining if this model could run effectively on resource-constrained edge devices like my laptop. For edge deployment, I'm particularly interested in:\n",
    "\n",
    "1. Memory footprint - How much RAM does the model require?\n",
    "2. Inference speed - Is it fast enough for real-time applications?\n",
    "3. Model loading time - Is the startup time acceptable for edge applications?\n",
    "\n",
    "Based on these results, I can determine if further optimization techniques like quantization, pruning, or knowledge distillation might be necessary to make the model more suitable for edge deployment. For truly resource-constrained environments, I might even consider alternatives like TinyBERT or mobile-optimized models.\n",
    "\n",
    "This cross-dataset evaluation also helps me understand how well my model generalizes to new sources of fake news beyond what it was trained on, which is essential for real-world applications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
