{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3063c4e1",
   "metadata": {},
   "source": [
    "# DistilBERT External Dataset Evaluation for Fake News Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, I'm going to test my fine-tuned DistilBERT model on completely new datasets to see how well it actually works in the real world. DistilBERT is basically a smaller, faster version of BERT that keeps about 95% of the performance while being 40% smaller. The Hugging Face team created it using a technique called knowledge distillation, which makes it perfect for situations where I don't have tons of computing power.\n",
    "\n",
    "I want to find out several things:\n",
    "\n",
    "1. How well does my model work on external datasets with real news and AI-generated fake news?\n",
    "2. What are the practical costs of running this model, like memory usage and speed?\n",
    "3. What kinds of articles does my model get wrong, and why?\n",
    "\n",
    "The main question I'm trying to answer is whether my trained model can actually handle real-world content that's different from what it saw during training.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "First, I'll import all the libraries I need for this evaluation. These will handle everything from data processing to creating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32875e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and transformers\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8459451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88105db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved memory measurement function\n",
    "def measure_peak_memory_usage(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure peak memory usage during function execution\n",
    "    \n",
    "    Args:\n",
    "        func: Function to measure\n",
    "        *args, **kwargs: Arguments to pass to the function\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (function result, peak memory usage in MB)\n",
    "    \"\"\"\n",
    "    # Reset garbage collection and force collection before starting\n",
    "    gc.collect()\n",
    "    \n",
    "    # Start tracking\n",
    "    process = psutil.Process()\n",
    "    start_memory = process.memory_info().rss / (1024 * 1024)\n",
    "    peak_memory = start_memory\n",
    "    \n",
    "    # Define a memory tracking function\n",
    "    def track_peak_memory():\n",
    "        nonlocal peak_memory\n",
    "        current = process.memory_info().rss / (1024 * 1024)\n",
    "        peak_memory = max(peak_memory, current)\n",
    "    \n",
    "    # Set up a timer to periodically check memory\n",
    "    import threading\n",
    "    stop_tracking = False\n",
    "    \n",
    "    def memory_tracker():\n",
    "        while not stop_tracking:\n",
    "            track_peak_memory()\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    # Start tracking thread\n",
    "    tracking_thread = threading.Thread(target=memory_tracker)\n",
    "    tracking_thread.daemon = True\n",
    "    tracking_thread.start()\n",
    "    \n",
    "    # Run the function\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "    finally:\n",
    "        # Stop tracking\n",
    "        stop_tracking = True\n",
    "        tracking_thread.join(timeout=1.0)\n",
    "    \n",
    "    # Calculate memory used\n",
    "    memory_used = peak_memory - start_memory\n",
    "    \n",
    "    return result, memory_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings and set visualization style\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set consistent visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Force CPU usage to simulate edge device performance\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device} (simulating edge device performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051bda7",
   "metadata": {},
   "source": [
    "## Loading External Datasets\n",
    "\n",
    "Now I'll load my external test datasets. These contain news articles that my model has never seen before, which will give me a realistic picture of how it performs in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc61b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external datasets\n",
    "real_df = pd.read_csv('../datasets/manual_real.csv')\n",
    "fake_df = pd.read_csv('../datasets/fake_claude.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ebb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process real news data\n",
    "if 'title' in real_df.columns and 'content' in real_df.columns:\n",
    "    real_df['combined_text'] = real_df['title'] + \" \" + real_df['content']\n",
    "elif 'text' in real_df.columns:\n",
    "    real_df['combined_text'] = real_df['text']\n",
    "real_df['label'] = 0  # Real news\n",
    "\n",
    "# Process fake news data\n",
    "if 'title' in fake_df.columns and 'content' in fake_df.columns:\n",
    "    fake_df['combined_text'] = fake_df['title'] + \" \" + fake_df['content']\n",
    "elif 'text' in fake_df.columns:\n",
    "    fake_df['combined_text'] = fake_df['text']\n",
    "fake_df['label'] = 1  # Fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine external datasets\n",
    "external_df = pd.concat(\n",
    "    [real_df[['combined_text', 'label']], fake_df[['combined_text', 'label']]],\n",
    "    ignore_index=True\n",
    ")\n",
    "X_external = external_df['combined_text']\n",
    "y_external = external_df['label']\n",
    "\n",
    "print(f\"External dataset: {len(external_df)} articles ({len(real_df)} real, {len(fake_df)} fake)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a2b24",
   "metadata": {},
   "source": [
    "## Loading and Measuring DistilBERT\n",
    "\n",
    "Next, I'll load my trained model and check how much computer resources it actually uses. This is important because if I want to deploy this thing in the real world, I need to know what it costs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d25a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up before loading\n",
    "gc.collect()\n",
    "\n",
    "# Measure memory before model loading\n",
    "memory_before = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "# Load the DistilBERT model and tokenizer\n",
    "model_path = '../../ml_models/distilbert_welfake_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory after model loading\n",
    "memory_after = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
    "model_memory = memory_after - memory_before\n",
    "\n",
    "# Calculate model size from parameters\n",
    "param_size = sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"DistilBERT model loaded successfully\")\n",
    "print(f\"Number of parameters: {num_params:,}\")\n",
    "print(f\"Model size: {param_size:.2f} MB\")\n",
    "print(f\"Memory increase after loading: {model_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387198f",
   "metadata": {},
   "source": [
    "## Preparing Data for Evaluation\n",
    "\n",
    "Before I can test my model, I need to convert my text data into the format that the transformer expects. This involves tokenizing all the text and setting up data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1299ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(texts, labels, tokenizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tokenize text data and create DataLoader for model input\n",
    "    \n",
    "    Args:\n",
    "        texts: List or Series of text samples\n",
    "        labels: List or Series of labels\n",
    "        tokenizer: The tokenizer to use\n",
    "        batch_size: Batch size for DataLoader\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader with tokenized inputs and labels\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,  # Standard for BERT models\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(\n",
    "        encodings['input_ids'],\n",
    "        encodings['attention_mask'],\n",
    "        torch.tensor(labels.values if hasattr(labels, 'values') else labels)\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d40490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare external dataset\n",
    "external_loader = prepare_data(X_external, y_external, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3cd4b",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "I'll create a function that tests my model and tracks both how accurate it is and how much computer resources it uses. This gives me the full picture of what it would cost to run this in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate model and measure performance metrics and resource usage\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        dataloader: DataLoader with test data\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with performance metrics and resource usage\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Define the prediction function to measure\n",
    "    def make_predictions():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        predict_time = time.time() - start_time\n",
    "        return all_preds, all_labels, predict_time\n",
    "    \n",
    "    # Run predictions with memory measurement\n",
    "    (all_preds, all_labels, predict_time), memory_used = measure_peak_memory_usage(make_predictions)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nDistilBERT Evaluation on {dataset_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Prediction time: {predict_time:.2f} seconds for {len(all_labels)} samples\")\n",
    "    print(f\"Average prediction time: {predict_time/len(all_labels)*1000:.2f} ms per sample\")\n",
    "    print(f\"Peak memory usage during inference: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Return results for visualization\n",
    "    return {\n",
    "        'y_pred': all_preds,\n",
    "        'y_true': all_labels,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predict_time': predict_time,\n",
    "        'samples': len(all_labels),\n",
    "        'memory_used': memory_used\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f41f15",
   "metadata": {},
   "source": [
    "## Performance on External Datasets\n",
    "\n",
    "Now I'll actually test my model on the external datasets. This is where I find out if my model learned general patterns about fake news, or if it just memorized the specific training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098da788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on external datasets\n",
    "external_results = evaluate_model(model, external_loader, \"External Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7939e4",
   "metadata": {},
   "source": [
    "### Confusion Matrix for External Data\n",
    "\n",
    "I'll create a confusion matrix to see exactly where my model is making mistakes. This visualization shows me the patterns in how my model gets confused between real and fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Create and visualize confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real News', 'Fake News'],\n",
    "                yticklabels=['Real News', 'Fake News'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error rates\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp/(fp+tn)\n",
    "    fnr = fn/(fn+tp)\n",
    "    print(f\"False Positive Rate: {fpr:.4f} ({fp} real news articles misclassified as fake)\")\n",
    "    print(f\"False Negative Rate: {fnr:.4f} ({fn} fake news articles misclassified as real)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for External Datasets\n",
    "plot_confusion_matrix(\n",
    "    external_results['y_true'], \n",
    "    external_results['y_pred'], \n",
    "    \"DistilBERT Confusion Matrix on External Datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6de77e",
   "metadata": {},
   "source": [
    "### What the Results Revealed\n",
    "\n",
    "When I ran the confusion matrix analysis, it showed something really interesting about how my model behaves with new data. The pattern revealed important lessons about what happens when models encounter content that's different from their training data.\n",
    "\n",
    "The confusion matrix showed that my model had developed a strong bias toward classifying articles as \"real\" when it wasn't sure. This became especially obvious when looking at the false positive and false negative rates - my model was much better at spotting real news than fake news in these external datasets.\n",
    "\n",
    "Looking at how my model handled real news articles, it was almost perfect. It correctly identified nearly every authentic news article in the external dataset. This showed me that my model had actually learned some solid patterns about what makes legitimate journalism look legitimate - things like proper sourcing, coherent structure, and consistent writing style.\n",
    "\n",
    "But when it came to fake news, that's where things got interesting. My model kept classifying fake articles as real, even when they were obviously fabricated. This happened because the AI-generated fake news in my external dataset was much more sophisticated than the examples in my training data. These new fake articles used professional language, included specific details, and followed proper news article structure - they were basically much harder to detect than the training examples.\n",
    "\n",
    "This taught me something important about machine learning in general. Models learn patterns from their training data, but when they encounter new types of content that follow different patterns, they struggle. It's like studying for a test using only multiple choice questions, then finding out the actual test has essay questions - the knowledge is related but the format is completely different.\n",
    "\n",
    "## Looking at Specific Mistakes\n",
    "\n",
    "I want to dig deeper into the specific articles my model got wrong. Understanding exactly which articles confused my model will help me figure out what went wrong and how I might fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(X_text, y_true, y_pred, dataset_name, n_examples=3):\n",
    "    \"\"\"\n",
    "    Display examples of misclassified articles\n",
    "    \n",
    "    Args:\n",
    "        X_text: Text data\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        dataset_name: Name of the dataset\n",
    "        n_examples: Number of examples to display\n",
    "    \"\"\"\n",
    "    errors = np.where(y_true != y_pred)[0]\n",
    "    \n",
    "    if len(errors) == 0:\n",
    "        print(f\"No errors found on {dataset_name}!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nDistilBERT misclassified {len(errors)} out of {len(y_true)} articles on {dataset_name} ({len(errors)/len(y_true):.2%})\")\n",
    "    print(f\"Showing {min(n_examples, len(errors))} examples:\")\n",
    "    \n",
    "    # Select random errors to display\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    display_indices = np.random.choice(errors, size=min(n_examples, len(errors)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(display_indices):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text snippet: {X_text.iloc[idx][:200]}...\")  # First 200 chars\n",
    "        print(f\"True label: {'Real' if y_true[idx] == 0 else 'Fake'}\")\n",
    "        print(f\"Predicted: {'Real' if y_pred[idx] == 0 else 'Fake'}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf211f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors on External datasets\n",
    "analyze_errors(\n",
    "    X_external, \n",
    "    external_results['y_true'], \n",
    "    external_results['y_pred'], \n",
    "    \"External Datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204de2a",
   "metadata": {},
   "source": [
    "### What the Error Analysis Revealed\n",
    "\n",
    "When I examined the specific articles my model got wrong, I discovered some really interesting patterns. The fake news articles that fooled my model weren't your typical obviously fake stories. Instead, they were sophisticated pieces that used many of the same techniques as real journalism.\n",
    "\n",
    "These tricky fake articles had several things in common. They used authoritative language that sounded professional and credible. They included specific details and numbers that made them seem well-researched. They even had proper quote attribution and followed the structure you'd expect from a real news article. Basically, they were much more subtle than the obvious fake news my model had trained on.\n",
    "\n",
    "This taught me that AI-generated fake news has evolved a lot. It's no longer just about obvious lies or weird grammar. Modern fake news can be incredibly convincing because it mimics the surface features of real journalism very well. My model had learned to spot the old-style fake news but wasn't prepared for this new, more sophisticated approach.\n",
    "\n",
    "## Testing Performance on Edge Devices\n",
    "\n",
    "For my model to be useful in the real world, I need to understand how it performs when running on limited hardware. I'll test different batch sizes to see how I can optimize the trade-off between speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze batch processing efficiency\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "results = []\n",
    "\n",
    "# Create sample input\n",
    "sample_text = [\"This is a sample news article for testing inference speed.\"] * 32\n",
    "sample_encodings = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57697394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "for batch_size in batch_sizes:\n",
    "    # Prepare input batch\n",
    "    input_ids = sample_encodings['input_ids'][:batch_size].to(device)\n",
    "    attention_mask = sample_encodings['attention_mask'][:batch_size].to(device)\n",
    "    \n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(5):  # 5 runs per batch size\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            end = time.time()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_time = np.mean(times)\n",
    "    per_sample = avg_time / batch_size * 1000  # ms\n",
    "    \n",
    "    results.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Total Time (ms)': avg_time * 1000,\n",
    "        'Time per Sample (ms)': per_sample\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show batch efficiency results\n",
    "batch_df = pd.DataFrame(results)\n",
    "print(\"\\nBatch Processing Efficiency on CPU:\")\n",
    "print(batch_df.round(2))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_df['Batch Size'], batch_df['Time per Sample (ms)'], marker='o', linewidth=2)\n",
    "plt.title('Inference Time per Sample vs Batch Size')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time per Sample (ms)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f5a69",
   "metadata": {},
   "source": [
    "### What I Discovered About Batch Processing\n",
    "\n",
    "When I analyzed the batch processing results, I found a really interesting pattern that taught me fundamental lessons about computational efficiency. The graph I created showed a U-shaped curve that perfectly illustrated how computer resources work.\n",
    "\n",
    "At small batch sizes, I noticed that processing each individual article took a relatively long time. This happened because there are fixed costs every time the computer has to start up the model, kind of like how it takes time to start your car engine regardless of whether you're driving one block or ten miles. When I only processed one article at a time, that startup cost got spread across just one article, making it expensive per article.\n",
    "\n",
    "As I increased the batch size toward the middle range, something interesting happened. The time per article dropped significantly because the computer could process multiple articles simultaneously. Think of it like carpooling - the cost of the trip gets shared among more passengers, making it cheaper per person. The CPU could take advantage of its parallel processing capabilities and spread those fixed startup costs across more work.\n",
    "\n",
    "But then, beyond a certain point, the efficiency started decreasing again. This taught me about the limits of computer resources. When I tried to process too many articles at once, the system started struggling with memory constraints and scheduling issues. It's like trying to fit too many people in a car - eventually you reach a point where adding more passengers makes the trip slower and more uncomfortable for everyone.\n",
    "\n",
    "## Memory Usage Analysis for Different Sequence Lengths\n",
    "\n",
    "Next, I'll test how sequence length affects memory usage. Since transformers have to pay attention to every word in relation to every other word, longer sequences can get expensive fast in terms of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory usage for different sequence lengths\n",
    "seq_lengths = [64, 128, 256, 512]\n",
    "memory_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved memory measurement for sequence lengths\n",
    "for seq_len in seq_lengths:\n",
    "    # Create sample input with specific sequence length\n",
    "    sample_text = [\"This is a test\"] * 8  # Use batch size of 8\n",
    "    sample_encodings = tokenizer(\n",
    "        sample_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=seq_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = sample_encodings['input_ids'].to(device)\n",
    "    attention_mask = sample_encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # Measure memory usage with our improved function\n",
    "    def run_inference():\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Clean up and make measurements more reliable\n",
    "    gc.collect()\n",
    "    _, memory_used = measure_peak_memory_usage(run_inference)\n",
    "    \n",
    "    memory_results.append({\n",
    "        'Sequence Length': seq_len,\n",
    "        'Memory Used (MB)': memory_used\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e714e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show memory usage results\n",
    "memory_df = pd.DataFrame(memory_results)\n",
    "print(\"\\nMemory Usage for Different Sequence Lengths:\")\n",
    "print(memory_df)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(memory_df['Sequence Length'], memory_df['Memory Used (MB)'], marker='o', linewidth=2)\n",
    "plt.title('Memory Usage vs Sequence Length')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Memory Used (MB)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d45fb5",
   "metadata": {},
   "source": [
    "### What I Learned About Memory and Sequence Length\n",
    "\n",
    "When I ran the memory analysis, I discovered something important about how transformer models work under the hood. The relationship between sequence length and memory usage taught me about the fundamental complexity of the attention mechanism.\n",
    "\n",
    "The reason memory usage can increase dramatically with longer sequences comes down to how attention works. In a transformer, every single word has to \"pay attention\" to every other word in the sequence. So if I have a sequence with N words, the model has to compute N squared attention relationships. This means that doubling the sequence length can potentially quadruple the memory requirements, not just double them.\n",
    "\n",
    "Looking at my results, I could observe this scaling behavior in action. The memory requirements varied as I increased sequence length, showing how the quadratic nature of attention computations affects resource usage. This pattern taught me why transformer models can become so resource-hungry with long documents.\n",
    "\n",
    "From a practical standpoint, this analysis showed me that I could potentially save significant memory by using shorter sequences without probably losing much accuracy. Most of the important signals for detecting fake news likely appear in the first few hundred words of an article anyway, so truncating longer articles might be a smart trade-off between performance and efficiency.\n",
    "\n",
    "## Summary and What I Learned\n",
    "\n",
    "Based on all the testing I did, I learned some valuable lessons that go way beyond just fake news detection. This whole evaluation process taught me fundamental principles about how transformer models behave when you take them from the lab into the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of results focusing on external dataset performance\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Model Parameters',\n",
    "        'Model Size (MB)',\n",
    "        'Memory Footprint (MB)',\n",
    "        'Accuracy',\n",
    "        'Precision', \n",
    "        'Recall',\n",
    "        'F1 Score',\n",
    "        'Inference Time (ms/sample)',\n",
    "        'False Positive Rate',\n",
    "        'False Negative Rate',\n",
    "        'Optimal Batch Size',\n",
    "        'Memory at Optimal Batch'\n",
    "    ],\n",
    "    'External Dataset Results': [\n",
    "        f\"{num_params:,}\",\n",
    "        f\"{param_size:.2f}\",\n",
    "        f\"{model_memory:.2f}\",\n",
    "        f\"{external_results['accuracy']:.4f}\",\n",
    "        f\"{external_results['precision']:.4f}\",\n",
    "        f\"{external_results['recall']:.4f}\",\n",
    "        f\"{external_results['f1']:.4f}\",\n",
    "        f\"{external_results['predict_time']/external_results['samples']*1000:.2f}\",\n",
    "        \"From confusion matrix\",\n",
    "        \"From confusion matrix\",\n",
    "        \"16 samples\",\n",
    "        \"From batch analysis\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"DistilBERT External Dataset Performance Summary:\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329e39f",
   "metadata": {},
   "source": [
    "### The Big Picture Lessons\n",
    "\n",
    "When I completed this evaluation, the results taught me several important principles about working with large language models in production. Understanding these patterns helped me build better intuition for what happens when you move from research to real-world applications.\n",
    "\n",
    "The trade-off between efficiency and performance became really clear through this testing. DistilBERT showed me that you can get substantial computational savings while keeping most of the original model's capabilities. The knowledge distillation process demonstrated how you can transfer what a large model learned into a smaller, more practical version that can actually run on normal hardware.\n",
    "\n",
    "The generalization challenge was probably the most important lesson I learned. Even though my model performed really well on test data from the same distribution as my training data, it struggled when faced with truly different external content. This taught me why you need to test models on diverse datasets to understand how they'll really behave in the wild. Good performance in the lab doesn't automatically translate to good performance in the real world.\n",
    "\n",
    "The resource optimization insights gave me practical guidelines for actually deploying models. I learned that computational efficiency isn't just about model size - the way you configure your deployment, like choosing the right batch size and sequence length, can dramatically change how your model performs in production.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This whole evaluation process taught me how to systematically test both what my model can do and where it falls short when moving from the controlled training environment to messy real-world scenarios. The systematic approach I used here gave me valuable lessons that apply way beyond fake news detection to any kind of model deployment.\n",
    "\n",
    "Understanding my model's strengths through systematic testing showed me how DistilBERT successfully balanced efficiency with performance. My model achieved this balance through knowledge distillation, which taught me how you can compress complex models while keeping most of their capabilities. This principle works across many areas of machine learning, showing how powerful models can be made accessible even when you don't have massive computing resources.\n",
    "\n",
    "Learning from the generalization challenges was probably the most eye-opening part of this analysis. My external dataset evaluation revealed the big difference between statistical performance and practical usefulness. While my model showed strong technical capabilities in controlled testing, its real-world performance patterns taught me about the challenges of building systems that work reliably across diverse content types. This illustrated why testing across multiple datasets is absolutely essential for understanding how a model will actually behave when users start throwing real data at it.\n",
    "\n",
    "The practical deployment considerations gave me concrete guidelines for making production decisions. Understanding how batch size affects speed, how sequence length impacts memory usage, and how different optimization strategies trade off against each other helped me build intuition for deploying transformer models effectively in real systems. These insights go far beyond the technical specs and into the practical realities of making machine learning work in production.\n",
    "\n",
    "My evaluation revealed both the promise and the limitations of distilled transformer models for this type of application. The results suggested that while DistilBERT provides a solid foundation for fake news detection systems, successful deployment would need additional considerations like ensemble approaches, continuous learning systems, or hybrid architectures that can adapt as fake news generation techniques become more sophisticated.\n",
    "\n",
    "This systematic evaluation approach pointed me toward several important areas for future work. These include developing training strategies that improve generalization across diverse content types, exploring ensemble methods that combine multiple detection approaches, and investigating how models can be designed to maintain performance as the tactics for generating fake news continue to evolve.\n",
    "\n",
    "Understanding these patterns helped me build the analytical skills needed to evaluate and deploy machine learning models effectively across many different applications. The systematic approach I demonstrated here provides a template for rigorous model assessment that balances technical performance metrics with practical deployment realities.\n",
    "\n",
    "## Model Cleanup\n",
    "\n",
    "Finally, I'll clean up the memory by releasing the model resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up models to free memory\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Model resources released\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
