{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751abb66",
   "metadata": {},
   "source": [
    "# ISOT Fake News Dataset - Feature Engineering and Model Preparation\n",
    "\n",
    "Building on the findings from our exploratory data analysis, this notebook focuses on enhanced data cleaning, feature engineering, and preparing the dataset for model training. My goal is to develop a robust approach that helps models learn legitimate differences between real and fake news rather than dataset-specific artifacts.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, I'll import the necessary libraries and load our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.decomposition import NMF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e767fff",
   "metadata": {},
   "source": [
    "Now I'll load the datasets and the basic cleaned versions from the first notebook (if available), or recreate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58212e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load previously saved datasets or load raw data\n",
    "try:\n",
    "    # Try to load datasets with basic cleaning already applied\n",
    "    true_news = pd.read_csv('true_news_basic_cleaned.csv')\n",
    "    fake_news = pd.read_csv('fake_news_basic_cleaned.csv')\n",
    "    print(\"Loaded previously cleaned datasets\")\n",
    "except:\n",
    "    # If not available, load raw data and perform basic cleaning\n",
    "    print(\"Loading raw datasets\")\n",
    "    true_news = pd.read_csv('True.csv')\n",
    "    fake_news = pd.read_csv('Fake.csv')\n",
    "    \n",
    "    # Basic cleaning function\n",
    "    def clean_text(text, patterns_to_remove=None):\n",
    "        \"\"\"\n",
    "        Clean text by removing specified patterns\n",
    "        \n",
    "        Args:\n",
    "            text: Text to clean\n",
    "            patterns_to_remove: List of regex patterns to remove\n",
    "        \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        cleaned_text = text\n",
    "        \n",
    "        if patterns_to_remove:\n",
    "            for pattern in patterns_to_remove:\n",
    "                cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        return cleaned_text\n",
    "\n",
    "    # Apply basic cleaning\n",
    "    true_news['cleaned_text'] = true_news['text'].apply(lambda x: clean_text(x, [r'\\(Reuters\\)']))\n",
    "    fake_news['cleaned_text'] = fake_news['text'].apply(lambda x: clean_text(x, []))\n",
    "    \n",
    "    # Save basic cleaned versions for future use\n",
    "    true_news.to_csv('true_news_basic_cleaned.csv', index=False)\n",
    "    fake_news.to_csv('fake_news_basic_cleaned.csv', index=False)\n",
    "\n",
    "# Add text length as a feature if not present\n",
    "if 'text_length' not in true_news.columns:\n",
    "    true_news['text_length'] = true_news['text'].apply(lambda x: len(str(x)))\n",
    "    fake_news['text_length'] = fake_news['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Add labels if not present\n",
    "if 'label' not in true_news.columns:\n",
    "    true_news['label'] = 'Real'\n",
    "    fake_news['label'] = 'Fake'\n",
    "\n",
    "# Display basic info about the datasets\n",
    "print(\"True News Dataset Shape:\", true_news.shape)\n",
    "print(\"Fake News Dataset Shape:\", fake_news.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519584a0",
   "metadata": {},
   "source": [
    "## 2. Enhanced Data Cleaning\n",
    "\n",
    "Based on our exploratory analysis, I'll create an enhanced cleaning function that removes identified biases while preserving legitimate signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced cleaning function\n",
    "def enhanced_clean_text(text, is_true_news=True):\n",
    "    \"\"\"\n",
    "    Enhanced cleaning to remove bias-inducing patterns while preserving legitimate signals\n",
    "    \n",
    "    Args:\n",
    "        text: Text to clean\n",
    "        is_true_news: Whether the text is from true news (affects which patterns are removed)\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned_text = text\n",
    "    \n",
    "    if is_true_news:\n",
    "        # For true news, remove Reuters tag but preserve location\n",
    "        cleaned_text = re.sub(r'(\\b[A-Z]+(?:\\s[A-Z]+)*)\\s*\\(Reuters\\)', r'\\1', cleaned_text)\n",
    "        \n",
    "        # Remove other potentially biasing source markers specific to true news\n",
    "        for source in ['(SPD)', '(FDP)', '(AfD)', '(CDU)', '(SDF)', '(KRG)', '(NAFTA)', '(PKK)']:\n",
    "            cleaned_text = re.sub(re.escape(source), '', cleaned_text)\n",
    "    else:\n",
    "        # For fake news, remove patterns like (ACR) that are specific to fake news\n",
    "        for source in ['(ACR)', '(s)', '(id)', '(a)', '(R)', '(D)']:\n",
    "            cleaned_text = re.sub(re.escape(source), '', cleaned_text)\n",
    "            \n",
    "        # Remove links that are common in fake news\n",
    "        cleaned_text = re.sub(r'https?://\\S+', '', cleaned_text)\n",
    "        \n",
    "        # Remove specific phrases highly associated with fake news sources\n",
    "        fake_phrases = [\n",
    "            'Tune in to the Alternate Current Radio',\n",
    "            '21st Century Wire',\n",
    "            'Featured Image'\n",
    "        ]\n",
    "        for phrase in fake_phrases:\n",
    "            cleaned_text = cleaned_text.replace(phrase, '')\n",
    "    \n",
    "    # Common cleaning for both types\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply enhanced cleaning\n",
    "true_news['enhanced_cleaned_text'] = true_news['text'].apply(lambda x: enhanced_clean_text(x, is_true_news=True))\n",
    "fake_news['enhanced_cleaned_text'] = fake_news['text'].apply(lambda x: enhanced_clean_text(x, is_true_news=False))\n",
    "\n",
    "# Verify enhanced cleaning worked\n",
    "print(\"Sample of enhanced cleaned true news:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nOriginal text beginning: {true_news['text'].iloc[i][:100]}\")\n",
    "    print(f\"Basic cleaned text: {true_news['cleaned_text'].iloc[i][:100]}\")\n",
    "    print(f\"Enhanced cleaned text: {true_news['enhanced_cleaned_text'].iloc[i][:100]}\")\n",
    "\n",
    "print(\"\\nSample of enhanced cleaned fake news:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nOriginal text beginning: {fake_news['text'].iloc[i][:100]}\")\n",
    "    print(f\"Basic cleaned text: {fake_news['cleaned_text'].iloc[i][:100]}\")\n",
    "    print(f\"Enhanced cleaned text: {fake_news['enhanced_cleaned_text'].iloc[i][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211af6f0",
   "metadata": {},
   "source": [
    "I'm using this enhanced cleaning approach because:\n",
    "\n",
    "1. It removes high-bias markers like \"(Reuters)\" that could lead to overfitting\n",
    "2. It preserves legitimate signals like location datelines that reflect journalistic conventions\n",
    "3. It balances cleaning across both datasets to ensure similar types of artifacts are removed from each\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "Now I'll develop features that capture legitimate stylistic and content differences between real and fake news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf461f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to engineer features\n",
    "def engineer_features(df, text_column):\n",
    "    \"\"\"\n",
    "    Engineer features for classification\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the text data\n",
    "        text_column: Name of the column containing cleaned text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional features\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Text length features\n",
    "    result_df['text_length'] = result_df[text_column].apply(len)\n",
    "    result_df['word_count'] = result_df[text_column].apply(lambda x: len(str(x).split()))\n",
    "    result_df['avg_word_length'] = result_df[text_column].apply(\n",
    "        lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    "    )\n",
    "    result_df['sentence_count'] = result_df[text_column].apply(lambda x: len(re.findall(r'[.!?]+', str(x))) + 1)\n",
    "    result_df['avg_sentence_length'] = result_df['word_count'] / result_df['sentence_count']\n",
    "    \n",
    "    # Citation features\n",
    "    result_df['said_count'] = result_df[text_column].apply(lambda x: str(x).lower().count(' said '))\n",
    "    result_df['told_count'] = result_df[text_column].apply(lambda x: str(x).lower().count(' told '))\n",
    "    result_df['according_to_count'] = result_df[text_column].apply(lambda x: str(x).lower().count('according to'))\n",
    "    result_df['quote_count'] = result_df[text_column].apply(lambda x: str(x).count('\"'))\n",
    "    \n",
    "    # Normalize citation counts by text length\n",
    "    result_df['said_per_1000_words'] = result_df['said_count'] * 1000 / result_df['word_count']\n",
    "    result_df['quotes_per_1000_words'] = result_df['quote_count'] * 1000 / result_df['word_count']\n",
    "    \n",
    "    # Location features\n",
    "    result_df['has_location'] = result_df[text_column].apply(\n",
    "        lambda x: bool(re.match(r'^[A-Z]+(?:\\s[A-Z]+)*\\s+-', str(x)))\n",
    "    )\n",
    "    \n",
    "    # Emotional language features\n",
    "    emotional_words = ['believe', 'think', 'feel', 'opinion', 'incredible', 'amazing', 'terrible', 'horrible']\n",
    "    result_df['emotional_word_count'] = result_df[text_column].apply(\n",
    "        lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in emotional_words)\n",
    "    )\n",
    "    result_df['emotional_per_1000_words'] = result_df['emotional_word_count'] * 1000 / result_df['word_count']\n",
    "    \n",
    "    # Question and exclamation features\n",
    "    result_df['question_count'] = result_df[text_column].apply(lambda x: str(x).count('?'))\n",
    "    result_df['exclamation_count'] = result_df[text_column].apply(lambda x: str(x).count('!'))\n",
    "    result_df['question_exclamation_per_1000_words'] = (result_df['question_count'] + result_df['exclamation_count']) * 1000 / result_df['word_count']\n",
    "    \n",
    "    # Pronoun usage (indicates formality/informality)\n",
    "    first_person = ['i', 'we', 'our', 'us', 'my']\n",
    "    second_person = ['you', 'your', 'yours']\n",
    "    \n",
    "    result_df['first_person_count'] = result_df[text_column].apply(\n",
    "        lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in first_person)\n",
    "    )\n",
    "    result_df['second_person_count'] = result_df[text_column].apply(\n",
    "        lambda x: sum(str(x).lower().count(' ' + word + ' ') for word in second_person)\n",
    "    )\n",
    "    \n",
    "    result_df['first_person_per_1000_words'] = result_df['first_person_count'] * 1000 / result_df['word_count']\n",
    "    result_df['second_person_per_1000_words'] = result_df['second_person_count'] * 1000 / result_df['word_count']\n",
    "    \n",
    "    # Policy coverage features\n",
    "    policy_areas = {\n",
    "        'economy': ['economy', 'economic', 'tax', 'budget', 'deficit', 'gdp', 'inflation', 'unemployment', 'jobs', 'trade'],\n",
    "        'healthcare': ['healthcare', 'health', 'obamacare', 'insurance', 'hospital', 'medical', 'medicare', 'medicaid'],\n",
    "        'immigration': ['immigration', 'immigrant', 'border', 'refugee', 'asylum', 'visa', 'deportation'],\n",
    "        'foreign_policy': ['foreign', 'diplomatic', 'embassy', 'sanctions', 'treaty', 'international', 'relations'],\n",
    "        'environment': ['environment', 'climate', 'pollution', 'emissions', 'epa', 'warming', 'renewable', 'carbon']\n",
    "    }\n",
    "    \n",
    "    for area, terms in policy_areas.items():\n",
    "        result_df[f'{area}_count'] = result_df[text_column].apply(\n",
    "            lambda x: sum(str(x).lower().count(' ' + term + ' ') for term in terms)\n",
    "        )\n",
    "        result_df[f'{area}_per_1000_words'] = result_df[f'{area}_count'] * 1000 / result_df['word_count']\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply feature engineering to both datasets\n",
    "true_news_features = engineer_features(true_news, 'enhanced_cleaned_text')\n",
    "fake_news_features = engineer_features(fake_news, 'enhanced_cleaned_text')\n",
    "\n",
    "# Get a list of all engineered features\n",
    "feature_columns = [col for col in true_news_features.columns \n",
    "                  if col not in ['title', 'text', 'subject', 'date', 'cleaned_text', 'enhanced_cleaned_text']]\n",
    "\n",
    "# Display summary statistics for the engineered features\n",
    "print(\"\\nFeature statistics for True News:\")\n",
    "print(true_news_features[feature_columns].describe().transpose()[['mean', 'std']])\n",
    "\n",
    "print(\"\\nFeature statistics for Fake News:\")\n",
    "print(fake_news_features[feature_columns].describe().transpose()[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7b99a",
   "metadata": {},
   "source": [
    "Let's visualize the differences in key features between real and fake news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important features for visualization\n",
    "key_features = [\n",
    "    'said_per_1000_words', \n",
    "    'quotes_per_1000_words', \n",
    "    'emotional_per_1000_words',\n",
    "    'question_exclamation_per_1000_words',\n",
    "    'first_person_per_1000_words',\n",
    "    'second_person_per_1000_words'\n",
    "]\n",
    "\n",
    "# Create a combined dataset for visualization\n",
    "combined_features = pd.concat([\n",
    "    true_news_features[key_features + ['label']],\n",
    "    fake_news_features[key_features + ['label']]\n",
    "])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(key_features):\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('key_features_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16b808",
   "metadata": {},
   "source": [
    "Let's calculate the feature importance to identify the most discriminative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate feature importance using logistic regression\n",
    "def calculate_feature_importance(X, y):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using logistic regression\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target labels\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with feature importances\n",
    "    \"\"\"\n",
    "    # Initialize logistic regression model\n",
    "    model = LogisticRegressionCV(cv=5, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.coef_[0]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': np.abs(importances)\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Prepare data for feature importance calculation\n",
    "# Convert labels to numerical values\n",
    "true_news_features['numeric_label'] = 1\n",
    "fake_news_features['numeric_label'] = 0\n",
    "\n",
    "# Combine datasets\n",
    "combined_features = pd.concat([\n",
    "    true_news_features[feature_columns + ['numeric_label']],\n",
    "    fake_news_features[feature_columns + ['numeric_label']]\n",
    "])\n",
    "\n",
    "# Remove any non-numeric features\n",
    "numeric_features = [f for f in feature_columns if combined_features[f].dtype in [np.int64, np.float64]]\n",
    "\n",
    "# Calculate feature importance\n",
    "X = combined_features[numeric_features]\n",
    "y = combined_features['numeric_label']\n",
    "\n",
    "feature_importance = calculate_feature_importance(X, y)\n",
    "\n",
    "# Display top 15 most important features\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee1fb8",
   "metadata": {},
   "source": [
    "## 4. Text Vectorization with TF-IDF\n",
    "\n",
    "Let's create text features using TF-IDF vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create TF-IDF vectors\n",
    "def create_tfidf_vectors(true_texts, fake_texts, max_features=5000):\n",
    "    \"\"\"\n",
    "    Create TF-IDF vectors for text classification\n",
    "    \n",
    "    Args:\n",
    "        true_texts: Series of true news texts\n",
    "        fake_texts: Series of fake news texts\n",
    "        max_features: Maximum number of features to keep\n",
    "        \n",
    "    Returns:\n",
    "        TF-IDF vectorizer and vectors for both datasets\n",
    "    \"\"\"\n",
    "    # Combine texts for fitting the vectorizer\n",
    "    all_texts = pd.concat([true_texts, fake_texts])\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Fit and transform all texts\n",
    "    all_vectors = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Split vectors back into true and fake\n",
    "    true_vectors = all_vectors[:len(true_texts)]\n",
    "    fake_vectors = all_vectors[len(true_texts):]\n",
    "    \n",
    "    return vectorizer, true_vectors, fake_vectors\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "vectorizer, true_vectors, fake_vectors = create_tfidf_vectors(\n",
    "    true_news_features['enhanced_cleaned_text'],\n",
    "    fake_news_features['enhanced_cleaned_text']\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF vectors created with {true_vectors.shape[1]} features\")\n",
    "print(f\"True news vectors shape: {true_vectors.shape}\")\n",
    "print(f\"Fake news vectors shape: {fake_vectors.shape}\")\n",
    "\n",
    "# Get the top features (words/phrases) by their IDF scores\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "feature_idf = sorted(zip(feature_names, idf_scores), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nTop 10 most common terms (lowest IDF):\")\n",
    "for feature, score in feature_idf[:10]:\n",
    "    print(f\"- {feature}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 most rare terms (highest IDF):\")\n",
    "for feature, score in feature_idf[-10:]:\n",
    "    print(f\"- {feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64c215",
   "metadata": {},
   "source": [
    "## 5. Baseline Model Evaluation\n",
    "\n",
    "Let's evaluate a simple baseline model using our engineered features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataset with all features\n",
    "true_news_final = true_news_features[['title', 'enhanced_cleaned_text'] + feature_columns].copy()\n",
    "true_news_final['label'] = 1  # 1 for real news\n",
    "\n",
    "fake_news_final = fake_news_features[['title', 'enhanced_cleaned_text'] + feature_columns].copy()\n",
    "fake_news_final['label'] = 0  # 0 for fake news\n",
    "\n",
    "# Combine datasets\n",
    "combined_final = pd.concat([true_news_final, fake_news_final], axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "combined_final = combined_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select numeric features for model evaluation\n",
    "numeric_features = [f for f in feature_columns if combined_final[f].dtype in [np.int64, np.float64]]\n",
    "\n",
    "# Define X and y\n",
    "X = combined_final[numeric_features]\n",
    "y = combined_final['label']\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and evaluate a baseline logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Baseline Model Evaluation (Engineered Features Only)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f0309",
   "metadata": {},
   "source": [
    "Let's also evaluate a model that combines TF-IDF features with our engineered features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7263ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse TF-IDF matrices to DataFrames for easier handling\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine true and fake vectors\n",
    "all_vectors = hstack([true_vectors, fake_vectors])\n",
    "\n",
    "# Create train/test split with TF-IDF vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create labels array\n",
    "labels = np.concatenate([np.ones(true_vectors.shape[0]), np.zeros(fake_vectors.shape[0])])\n",
    "\n",
    "# Split TF-IDF vectors and labels\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(\n",
    "    all_vectors, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Initialize model for TF-IDF only\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "tfidf_model.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "# Make predictions\n",
    "y_tfidf_pred = tfidf_model.predict(X_tfidf_test)\n",
    "\n",
    "# Evaluate TF-IDF only model\n",
    "print(\"TF-IDF Only Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_score(y_tfidf_test, y_tfidf_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_tfidf_test, y_tfidf_pred))\n",
    "\n",
    "# Now let's create a model that combines both types of features\n",
    "# This is a bit more complex since we need to align the samples\n",
    "# In a real implementation, you would use a FeatureUnion or a custom transformer\n",
    "# For this demonstration, we'll use a simpler approach\n",
    "print(\"\\nNext step would be to create a combined model with both TF-IDF and engineered features\")\n",
    "print(\"This would typically be implemented using sklearn's Pipeline and FeatureUnion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ff361",
   "metadata": {},
   "source": [
    "## 6. Preparing Final Datasets for Model Fine-tuning\n",
    "\n",
    "Now I'll prepare the final datasets for model fine-tuning, including train/validation/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2028d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset with enhanced cleaning\n",
    "# Adding the label column (1 for true, 0 for fake)\n",
    "true_news_final = true_news[['title', 'enhanced_cleaned_text']].copy()\n",
    "true_news_final['label'] = 1  # 1 for real news\n",
    "\n",
    "fake_news_final = fake_news[['title', 'enhanced_cleaned_text']].copy()\n",
    "fake_news_final['label'] = 0  # 0 for fake news\n",
    "\n",
    "# Combine datasets\n",
    "combined_final = pd.concat([true_news_final, fake_news_final], axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "combined_final = combined_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check class balance\n",
    "print(\"Class distribution:\")\n",
    "print(combined_final['label'].value_counts())\n",
    "print(f\"Class balance: {combined_final['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Save the final dataset\n",
    "combined_final.to_csv('fake_news_preprocessed.csv', index=False)\n",
    "print(\"Final dataset saved as 'fake_news_preprocessed.csv'\")\n",
    "\n",
    "# Create train, validation, and test splits\n",
    "train_df, temp_df = train_test_split(combined_final, test_size=0.3, random_state=42, stratify=combined_final['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Training set: {len(train_df)} samples ({len(train_df)/len(combined_final)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(combined_final)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(combined_final)*100:.1f}%)\")\n",
    "\n",
    "# Save the splits\n",
    "train_df.to_csv('train_fake_news.csv', index=False)\n",
    "val_df.to_csv('val_fake_news.csv', index=False)\n",
    "test_df.to_csv('test_fake_news.csv', index=False)\n",
    "print(\"Train, validation, and test sets saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a64f5a",
   "metadata": {},
   "source": [
    "## 7. Recommendations for Model Fine-tuning\n",
    "\n",
    "Based on our analysis and feature engineering, here are recommendations for model fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of recommendations\n",
    "recommendations = [\n",
    "    \"Start with simple baseline models (Logistic Regression, Naive Bayes) to establish benchmarks\",\n",
    "    \"Use both engineered features and text-based features (TF-IDF or embeddings) for best performance\",\n",
    "    \"Consider transformer-based models like BERT or RoBERTa for advanced text representation\",\n",
    "    \"Monitor for signs of the model learning unwanted patterns or shortcuts\",\n",
    "    \"Perform cross-validation to ensure robust performance evaluation\",\n",
    "    \"Analyze misclassifications to identify remaining biases or patterns\",\n",
    "    \"Consider ensemble methods that combine multiple models\",\n",
    "    \"Test on external datasets to ensure generalization beyond the ISOT dataset\",\n",
    "    \"Use stratified sampling throughout to maintain class balance\",\n",
    "    \"Implement regularization to prevent overfitting to dataset-specific patterns\"\n",
    "]\n",
    "\n",
    "print(\"Recommendations for Model Fine-tuning:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae56c31",
   "metadata": {},
   "source": [
    "## 8. Suggested Model Training Pipeline\n",
    "\n",
    "Here's a suggested pipeline for training a fake news detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline of a comprehensive model training pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# This is pseudocode to outline the pipeline structure\n",
    "'''\n",
    "# 1. Define preprocessing for text and numeric features\n",
    "text_preprocessing = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)))\n",
    "])\n",
    "\n",
    "numeric_preprocessing = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Combine preprocessing steps with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_preprocessing, 'enhanced_cleaned_text'),\n",
    "    ('numeric', numeric_preprocessing, numeric_features)\n",
    "])\n",
    "\n",
    "# 3. Create full pipeline with model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4. Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'preprocessor__text__tfidf__max_features': [3000, 5000, 10000],\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20]\n",
    "}\n",
    "\n",
    "# 5. Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 6. Grid search for best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "'''\n",
    "\n",
    "print(\"The code above outlines a comprehensive model training pipeline\")\n",
    "print(\"It demonstrates how to combine text features and engineered features\")\n",
    "print(\"And how to perform hyperparameter optimization with cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b1fd",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "In this notebook, I've performed the following key tasks:\n",
    "\n",
    "1. Enhanced data cleaning to remove dataset-specific patterns while preserving legitimate signals\n",
    "2. Engineered a comprehensive set of features capturing stylistic, structural, and content differences\n",
    "3. Created text representations using TF-IDF vectorization\n",
    "4. Evaluated baseline models to establish benchmarks\n",
    "5. Prepared finalized datasets for model training with proper train/validation/test splits\n",
    "6. Provided recommendations for model fine-tuning\n",
    "\n",
    "The analysis suggests that both engineered features and text-based features are valuable for distinguishing between real and fake news. The Reuters pattern in true news has been addressed, and we've identified multiple legitimate signals that can help models learn substantive differences.\n",
    "\n",
    "Next steps would include:\n",
    "1. Training models using the prepared datasets\n",
    "2. Experimenting with more sophisticated text representation techniques\n",
    "3. Fine-tuning transformer-based models like BERT\n",
    "4. Evaluating models on external datasets\n",
    "5. Analyzing error cases to further refine the approach\n",
    "\n",
    "By following these steps, we can develop models that effectively distinguish between real and fake news based on substantive characteristics rather than dataset-specific artifacts.\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ad5c5",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive approach to preparing the ISOT Fake News Dataset for model fine-tuning, addressing the biases we identified in our exploratory analysis and creating features that capture legitimate differences between real and fake news."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
